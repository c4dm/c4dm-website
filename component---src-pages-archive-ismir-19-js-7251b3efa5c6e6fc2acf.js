"use strict";(self.webpackChunkc4dm_website=self.webpackChunkc4dm_website||[]).push([[662],{8590:function(e,t,a){var l=a(7294);t.Z=e=>{let{text:t,backgroundColor:a,textColor:n,className:r}=e;return l.createElement("div",{className:"parallelogram has-background-"+a+" "+r},l.createElement("h1",{className:"is-size-2-desktop is-size-3-tablet is-size-4-mobile has-text-centered has-text-weight-bold has-text-"+n},t))}},5642:function(e,t,a){a.r(t),a.d(t,{Head:function(){return m},default:function(){return s}});var l=a(7294),n=a(8678),r=a(8590),o=a.p+"static/bad_comparison-f5e737c5928c4b890d36e3b49a84239f.png",c=a.p+"static/good_comparison-dea340d06fec3596105eeb73687a54e0.png";var s=e=>{let{pageContext:t}=e;const{breadcrumb:{crumbs:a}}=t;return l.createElement(n.Z,{name:"Blending Acoustic and Language Model Predictions for Automatic Music Transcription",crumbs:a},l.createElement("section",{className:"section"},l.createElement("div",{className:"column is-one-third"},l.createElement(r.Z,{text:"ISMIR 2019 Paper",backgroundColor:"primary",textColor:"white"})),l.createElement("div",{className:"lowerPadding"}),l.createElement("div",{className:"content"},l.createElement("h1",null,"Blending Acoustic and Language Model Predictions for Automatic Music Transcription"),l.createElement("h2",null,"Adrien Ycart, Andrew McLeod, Emmanouil Benetos and Kazuyoshi Yoshii - ",l.createElement("i",null,"ISMIR 2019")),l.createElement("h3",null,"Presentation"),l.createElement("p",null,"On this page can be found the supplementary material for:",l.createElement("br",null),'Adrien Ycart, Andrew McLeod, Emmanouil Benetos and Kazuyoshi Yoshii. "Blending Acoustic and Language Model Predictions for Automatic Music Transcription"',l.createElement("i",null,"20th International Society for Music Information Retrieval Conference (ISMIR), "),"November 2019, Delft, Netherlands."),l.createElement("h3",null,"Code"),l.createElement("p",null,"The code to reproduce the experiments presented in the paper can be found at this address:",l.createElement("a",{href:"https://github.com/adrienycart/MLM_decoding"}," https://github.com/adrienycart/MLM_decoding ")),l.createElement("p",null,"The splits we used to train the acoustic model and language model are available here:",l.createElement("ul",null,l.createElement("li",null,l.createElement("a",{href:"http://c4dm.eecs.qmul.ac.uk/datasets/ycart/ismir19/split_MAPS.zip"}," Acoustic model split ")),l.createElement("li",null,l.createElement("a",{href:"http://c4dm.eecs.qmul.ac.uk/datasets/ycart/ismir19/split_PM.zip"}," Language model split ")))),l.createElement("h3",null,"Examples of results"),l.createElement("p",null,"Here, we propose selected examples to illustrate our model's performance: one where our model is successful, one where it fails. In both cases, we display a figure comparing:",l.createElement("ul",null,l.createElement("li",null,"The ground truth"),l.createElement("li",null,"The transcription obtained by thresholding the posteriogram at 0.5"),l.createElement("li",null,"The transcription obtained with HMM smoothing"),l.createElement("li",null,"The transcription obtained with our system in PM+S configuration"),l.createElement("li",null,"The posteriogram"),l.createElement("li",null,"The predictions made by the language model"))),l.createElement("p",null,'All comparisons are shown using a 16th-note timestep, hence the varying number of frames in both examples. The images can be displayed full size by using right-clicking the image and selecting "Open image in new tab".'),l.createElement("p",null,"We also propose the ground truth, thresholded posteriogram, HMM-smoothed posteriogram and PM+S transcriptions converted to MIDI format (",l.createElement("a",{href:"./ycart/ismir19/ismir19_examples_MIDI.zip"}," download all MIDI files here"),")."),l.createElement("h4",null,"Successful example"),l.createElement("img",{src:c,alt:"Good Comparison",width:"2000",className:"center"}),l.createElement("p",null,"Here, our model manages to successfully detect the short notes around frame 250, while the other two baseline models do not. This is mostly due to our language model: the MLM predictions show that it is able to recognise that pattern of short, repeated notes."),l.createElement("table",null,l.createElement("tbody",null,l.createElement("tr",null,l.createElement("td",null,"Ground truth:"),l.createElement("td",null,l.createElement("audio",{controls:!0},l.createElement("source",{src:"http://c4dm.eecs.qmul.ac.uk/datasets/ycart/ismir19/good_example_ground_truth.mp3",type:"audio/mpeg"}),"Your browser does not support the audio element."))),l.createElement("tr",null,l.createElement("td",null,"Baseline Kelz:"),l.createElement("td",null,l.createElement("audio",{controls:!0},l.createElement("source",{src:"http://c4dm.eecs.qmul.ac.uk/datasets/ycart/ismir19/good_example_baseline_quant.mp3",type:"audio/mpeg"}),"Your browser does not support the audio element."))),l.createElement("tr",null,l.createElement("td",null,"HMM:"),l.createElement("td",null,l.createElement("audio",{controls:!0},l.createElement("source",{src:"http://c4dm.eecs.qmul.ac.uk/datasets/ycart/ismir19/good_example_hmm_quant.mp3",type:"audio/mpeg"}),"Your browser does not support the audio element."))),l.createElement("tr",null,l.createElement("td",null,"PM+S:"),l.createElement("td",null,l.createElement("audio",{controls:!0},l.createElement("source",{src:"http://c4dm.eecs.qmul.ac.uk/datasets/ycart/ismir19/good_example_PM+S_quant.mp3",type:"audio/mpeg"}),"Your browser does not support the audio element."))))),l.createElement("h4",null,"Unsuccessful example"),l.createElement("img",{src:o,alt:"Bad Comparison",width:"2000",className:"center"}),l.createElement("p",null,"Here, our model does not improve much compared to the other two baseline models. If anything, it over-fragments notes, (e.g. around frame 80), and even adds some false positives (e.g. around frame 160). We can see that the MLM predictions are very blurry, failing to predict any note with confidence."),l.createElement("table",null,l.createElement("tbody",null,l.createElement("tr",null,l.createElement("td",null,"Ground truth:"),l.createElement("td",null,l.createElement("audio",{controls:!0},l.createElement("source",{src:"http://c4dm.eecs.qmul.ac.uk/datasets/ycart/ismir19/bad_example_ground_truth.mp3",type:"audio/mpeg"}),"Your browser does not support the audio element."))),l.createElement("tr",null,l.createElement("td",null,"Baseline Kelz:"),l.createElement("td",null,l.createElement("audio",{controls:!0},l.createElement("source",{src:"http://c4dm.eecs.qmul.ac.uk/datasets/ycart/ismir19/bad_example_baseline_quant.mp3",type:"audio/mpeg"}),"Your browser does not support the audio element."))),l.createElement("tr",null,l.createElement("td",null,"HMM:"),l.createElement("td",null,l.createElement("audio",{controls:!0},l.createElement("source",{src:"http://c4dm.eecs.qmul.ac.uk/datasets/ycart/ismir19/bad_example_hmm_quant.mp3",type:"audio/mpeg"}),"Your browser does not support the audio element."))),l.createElement("tr",null,l.createElement("td",null,"PM+S:"),l.createElement("td",null,l.createElement("audio",{controls:!0},l.createElement("source",{src:"http://c4dm.eecs.qmul.ac.uk/datasets/ycart/ismir19/bad_example_PM+S_quant.mp3",type:"audio/mpeg"}),"Your browser does not support the audio element."))))))))};const m=()=>l.createElement("title",null,"Blending Acoustic and Language Model Predictions for Automatic Music Transcription")}}]);
//# sourceMappingURL=component---src-pages-archive-ismir-19-js-7251b3efa5c6e6fc2acf.js.map
---
    title: "Proposed PhD Research Topics"
---

The Centre for Digital Music welcomes PhD research in any of our general areas of interest, broadly covering the field of music and audio technology, including informatics, retrieval, signal analysis and music understanding.

In addition, we are advertising PhD research in the following topic areas;

- [****Automatic transcription from audio to common music notation****](#automatic-transcription-from-audio-to-common-music-notation)
- [****Intelligent Mixing of Live Multichannel Sound****](#intelligent-mixing-of-live-multichannel-sound)
- [_Recommended skills_](#recommended-skills)
- [****Development of Interchannel Dependent Audio Effects****](#development-of-interchannel-dependent-audio-effects)
- [_Recommended skills_](#recommended-skills-1)
- [****Intelligent Instrument Recognition****](#intelligent-instrument-recognition)

### ****Automatic transcription from audio to common music notation****

_Contact:_ [Dr. Josh Reiss](http://www.eecs.qmul.ac.uk/~josh/index.htm), [josh.reiss@elec.qmul.ac.uk](mailto:josh.reiss@elec.qmul.ac.uk)

Automatic music transcription systems attempt to automatically extract a representation for the musical content of an audio signal. Ideally, the transcription will capture the musical essence of a performance, and can be played back by a performer. However, most transcription systems attempt to transcribe from audio to MIDI, that is, at best they represent only note pitch, onset, and duration times. Yet common music notation describes music in terms of a score which contains information on rhythm, key, measures, bars and so on.

Recent advances in audio signal processing have shown that these features may, to some degree, be also automatically extracted. Thus the goal of this research is to take current research one step further, to go beyond MIDI representation and towards a transcription of an audio signal in common music notation. For this, the researcher will explore the current state of the art and build upon existing work by others at the Centre for Digital Music and elsewhere. Knowledge of programming, signal processing and music theory are required, though missing skills may be acquired during the internship.

_Recommended skills_

*   Knowledge of audio signal processing
*   Programming skills
*   Knowledge of music theory

### ****Intelligent Mixing of Live Multichannel Sound****

_Contact:_ [Dr. Josh Reiss](http://www.eecs.qmul.ac.uk/~josh/index.htm), [josh.reiss@elec.qmul.ac.uk](mailto:josh.reiss@elec.qmul.ac.uk)

This PhD topic aims to intelligently generate an automatic sound mix out of an unknown set of multi-channel inputs. The input channels can be analysed to determine preferred settings for gain, equalization, compression, reverb, and so on. The research explores the possibility of reproducing the mixing decisions of a skilled audio engineer with minimal or no human interaction. This research has application to live music concerts, remote mixing, recording and postproduction as well as live mixing for interactive scenes.

Currently automated mixers are capable of saving a timeline of static mix scenes, which can be loaded for later use. But they lack the ability to adapt to a different room or to a different set of inputs. In other words, they lack the ability to automatically taking mixing decisions.

The justification of this research is the need of non-expert audio operators and musicians to be able to achieve a quality mix with minimal effort. Currently, mixing is a task which requires great skills, practice and can be sometime tedious. For the professional mixing engineer this kind of tool will reduce sound check time and will prove useful in multiple music group and festivals where changing from one group to another should be done really quickly. Large audio productions tend to have hundreds of channels, thus being able to group some of those channels into an automatic mode will ease the mixing task of an audio engineer. There is also the possibility of applying this technology to remote mixing applications where latency is too large to be able to interact with all aspects of the mix

This research topic builds on ;revious, successful work by researchers within the Centre for Digital Music, but is broad enough in scope that it could be taken in new and exciting directions.

### _Recommended skills_

*   Knowledge of audio signal processing
*   Programming skills
*   Knowledge of music and/or sound engineering

### ****Development of Interchannel Dependent Audio Effects****

_Contact:_ [Dr. Josh Reiss](http://www.eecs.qmul.ac.uk/~josh/index.htm), [josh.reiss@elec.qmul.ac.uk](mailto:josh.reiss@elec.qmul.ac.uk)

Most digital audio effects, whether implemented as plug-ins for mixers and audio editors or implemented as offline audio signal processing techniques, typically take a single channel as input and produce a single channel as output. The exceptions to this are fairly simple, such as ducking (which modifies one channel based on the level of another) and stereo effects (which produce two output channels).

The goal of this research is to develop MIMO (Multiple Input, Multiple Output) audio effects. These can be used to create different versions of a multichannel recording which are tailored to different listeners, or to modify channels based on the content in many other channels. Applications include live sound, where a customized mix may be fed back to each performer.

Current audio editors do not offer the ability to create plug-ins which may analyse or modify the multi-channel content. Thus this work will also involve either submitting modifications to an open source audio editor, or creating your own with the required functionality.

### _Recommended skills_

*   Knowledge of audio signal processing
*   Programming skills
*   Knowledge of music and/or sound engineering

### ****Intelligent Instrument Recognition****

_Contact:_ [Dr. Josh Reiss](http://www.eecs.qmul.ac.uk/~josh/index.htm), [josh.reiss@elec.qmul.ac.uk](mailto:josh.reiss@elec.qmul.ac.uk)

Musical Instrument Identification is one of the more well-known tasks in musical signal processing. There are standard procedures and techniques for this, yet the classification rates are often very poor. The techniques are often focused on single instrument sounds, and fail when applied on testbeds with notably different qualities than the training data. Furthermore, they are rarely adapted to the task of Musical Instrument Segmentation, and thus cannot be easily used to, for instance, identify guitar solos in popular recordings.

Researchers at the Centre for Digital Music have developed more sophisticated instrument identification techniques that focus on the spectral content produced by each instrument. These have yielded exceptionally high classification rates on standard testbeds. The goal of this research would be to assess and implement these techniques, and then to adapt them to the task of Instrument Segmentation and Labelling, with an emphasis on diverse testbeds. The planned outcome of this research is a clear advancement in the state of the art of the performance and usability of instrument recognition techniques.

Prerequisites for this are programming skills and an understanding of musical signal analysis and processing. This project will also require frequent interaction with other researchers

_Recommended skills_

*   Knowledge of audio signal processing and machine learning techniques
*   Programming skills
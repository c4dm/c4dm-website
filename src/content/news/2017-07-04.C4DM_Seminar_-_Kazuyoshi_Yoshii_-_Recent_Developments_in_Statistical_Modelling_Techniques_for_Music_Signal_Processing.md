---
        title: "C4DM Seminar: Kazuyoshi Yoshii - Recent Developments in Statistical Modelling Techniques for Music Signal Processing"
        author: "Admin"
        date: "2017-07-04"
        image: "placeholder.png"
        tags: ["seminars", "events"]
---
=
<p>For external participants: Please join our mailing list to receive announcements of future C4DM seminars: <a href="http://c4dm.eecs.qmul.ac.uk/seminars.html">http://c4dm.eecs.qmul.ac.uk/seminars.html</a></p>


<span style="font-size: 130%;">Date and Time</span></br>
Tuesday, 4th July 2017, at 4:00pm

<span style="font-size: 130%;">Place</span></br>
Graduate Centre, Room GC222, Queen Mary University of London, Mile End Road, London E1 4NS. Information on how to access the school can be found at <a href="http://www.eecs.qmul.ac.uk/contact-us/">http://www.eecs.qmul.ac.uk/contact-us/</a>.

<span style="font-size: 130%;">Speaker</span></br>
Kazuyoshi Yoshii

<span style="font-size: 130%;">Title</span></br>
Recent Developments in Statistical Modelling Techniques for Music Signal Processing

<span style="font-size: 130%;">Abstract</span></br>
What discriminates "music" signal processing from general audio signal processing? A lot of studies have been conducted for analyzing music audio signals, but most of them just tried to improve signal processing techniques that aim to convert audio signals into discrete symbols (e.g., chords, pitches, and musical notes). To deal with music data, it is important to carefully consider both signal (continuous) and symbolic (discrete) aspects of music. In this talk, I introduce our several attempts to build probabilistic generative models of music signals based on latent hierarchical structures underlying those signals by integrating an acoustic/signal model describing how signals are generated from symbols with a language model describing how those symbols are arranged according to musical grammars. Both models can be simultaneously learned observed signal data in an unsupervised manner. This approach is effective to improve the performance of automatic music transcription and opens up a new research direction towards next-generation data-driven computational musicology.

<span style="font-size: 130%;">Bio</span></br>
Kazuyoshi Yoshii is a Senior Lecturer at Speech and Audio Processing Group, Department of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Japan and is concurrently the Team Leader of Sound Scene Understanding Team at RIKEN Center for Advanced Intelligence Project (AIP), Japan. He received a PhD in Informatics at Kyoto University in 2008. His research interests include statistical music analysis, microphone array processing, Bayesian modeling, and machine learning.

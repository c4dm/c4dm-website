---
title: "C4DM Seminar: Manvi Agarwal"
author: "admin"
date: 2024-12-03
image: "./placeholder.png"
tags: ["seminars", "events"]
---

### C4DM Seminar: Manvi Agarwal: Fast Structure-informed Positional Encoding for Music Generation
-----------------

#### QMUL, School of Electronic Engineering and Computer Science

#### Centre for Digital Music Seminar Series

**Seminar by:**   
   Manvi Agarwal (Télécom Paris)

**Date/time:  Tuesday, 3rd December 2024, 2pm**

**Location: Room G2, Engineering Building, Mile End Campus, QMUL, E1 4NS **
Zoom: https://qmul-ac-uk.zoom.us/j/2387202947


<b>Title</b>: Fast Structure-informed Positional Encoding for Music Generation
-----------------

<b>Abstract</b>:  Music generated by deep learning methods often suffers from a lack of coherence and long-term organization. Over the years, several solutions have been proposed to help generative music architectures capture multi-scale hierarchical structure, which is a distinctive feature of music signals. The focus of my talk is the use of musically-relevant structural information to improve music Transformers. Specifically, I will present structure-informed positional encoding as a way to achieve superior music generation performance with low resource requirements. I will put forward two perspectives - an empirical approach exploring different designs for incorporating structural information in positional encoding and a theoretical approach using kernel approximations for improving the generative performance and computational complexity of such designs. In this way, I hope to underline the strengths of well-designed priors in dealing with some of the challenges facing music generation systems.

<b>Bio</b>: Manvi Agarwal is doing her PhD under the supervision of Dr. Changhong Wang and Prof. Gaël Richard in the ADASP (Audio Data Analysis and Signal Processing) group, Télécom Paris, Institut Polytechnique de Paris, France, supported by the ERC-funded Hi-Audio project. Her PhD research looks at how inductive biases can be introduced into Transformers to improve their modelling capabilities on music data. Broadly, she is interested in how sequence-based learning works and how our understanding of this process in different deep learning architectures can help us make these architectures perform better, especially in low-resource settings.

{"componentChunkName":"component---src-templates-news-post-js","path":"/news/2016-11-25.C4DM-Seminar_-_Tian_Cheng_and_Siddharth_Sigtia_(Video_Available)/","result":{"data":{"markdownRemark":{"html":"<p>For external participants: Please join our <a href=\"/seminars.html\">mailing list</a> to receive announcements of future C4DM seminars.</p>\n<p><span style=\"font-size: 130%;\">Date and Time</span></br>\nFriday, 25th Nov 2016, at 3:00pm</p>\n<p><span style=\"font-size: 130%;\">Place</span></br>\nRoom 3.25, Electronic Engineering building, Queen Mary University of London, Mile End Road, London E1 4NS. Information on how to access the school can be found at <a href=\"http://www.eecs.qmul.ac.uk/contact-us/\">here</a>.</p>\n<hr>\n<p><span style=\"font-size: 130%;\">Speaker</span></br>\nTian Cheng</p>\n<p><span style=\"font-size: 130%;\">Title</span></br>\nExploiting Piano Acoustics in Automatic Transcription</p>\n<p><span style=\"font-size: 130%;\">Video</span></br></p>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/65ixINbglHU?rel=0\" frameborder=\"0\" allowfullscreen></iframe>\n<p><span style=\"font-size: 130%;\">Abstract</span></br>\nIn this talk we exploit piano acoustics to automatically transcribe piano recordings into a symbolic representation: the pitch and timing of each detected note. The talk mainly consists of two parts. Firstly, we investigate the decay of individual piano partials based on the theoretical analysis of coupled piano strings to model the decay patterns of piano in real-world recordings. In the second part, we propose an attack/decay model, that takes into account the time-varying timbre and decaying energy of piano sounds. The system divides a piano note into percussive attack and harmonic decay stages, and separately models the two parts using two sets of templates and amplitude envelopes. The two parts are coupled by the note activations. We simplify the decay envelope by an exponentially decaying function. We demonstrate the utility of the proposed system in piano music transcription. Results show that explicitly modelling piano acoustical features, especially temporal features, can improve the transcription performance.</p>\n<p><span style=\"font-size: 130%;\">Bio</span></br>\nTian Cheng recently completed a Ph.D. at Centre for Digital Music in Queen Mary of London University, supervised by Simon Dixon and Matthias Mauch. Her Ph.D. research topic is automatic transcription of piano music using acoustical cues. In 2012, she received a Master's degree in Huazhong University of Science and Technology, China.</p>\n<hr>\n<p><span style=\"font-size: 130%;\">Speaker</span></br>\nSiddharth Sigtia</p>\n<p><span style=\"font-size: 130%;\">Title</span></br>\nNeural Networks for Analysing Music and Environmental Audio</p>\n<p><span style=\"font-size: 130%;\">Video</span></br></p>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/frRJriBpDqI?rel=0\" frameborder=\"0\" allowfullscreen></iframe>\n<p><span style=\"font-size: 130%;\">Abstract</span></br>\nWe consider the analysis of music and environmental audio recordings with neural networks. Recently, neural networks have been shown to be an effective family of models for speech recognition, computer vision, natural language processing and a number of other statistical modelling problems. The composite layer-wise structure of neural networks allows for flexible model design, where prior knowledge about the domain of application can be used to inform the design and architecture of the neural network models. Additionally, it has been shown that when trained on large quantities of data, neural networks can be directly applied to low-level features to learn mappings to high level concepts like phonemes in speech and object classes in computer vision. In this work we investigate whether neural network models can be usefully applied to processing music and environmental audio.</p>\n<p><span style=\"font-size: 130%;\">Bio</span></br>\nSiddharth Sigtia is currently a researcher at the Siri Speech team at Apple where he investigates neural networks for acoustic modelling for speech recognition. He finished his PhD in Electronics Engineering at the Centre for Digital Music (C4DM) at Queen Mary University of London, where he was supervised by Simon Dixon. Previously, he received a Master's degree in Physics and a Bachelor's degree in Electronics Engineering from BITS, Pilani, India.</p>","frontmatter":{"title":"C4DM Seminar: Tian Cheng and Siddharth Sigtia","date":"Friday, 25 November 2016","author":"Admin","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/ba986/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/f4be1/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/b444b/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/ba986/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/9b21f/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/9ff6b/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/f2559/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":800}}}}}},"pageContext":{"slug":"/news/2016-11-25.C4DM-Seminar_-_Tian_Cheng_and_Siddharth_Sigtia_(Video_Available)","breadcrumb":{"location":"/news/2016-11-25.C4DM-Seminar_-_Tian_Cheng_and_Siddharth_Sigtia_(Video_Available)/","crumbs":[{"pathname":"/","crumbLabel":"Home"},{"pathname":"/news","crumbLabel":"news"},{"pathname":"/news/2016-11-25.C4DM-Seminar_-_Tian_Cheng_and_Siddharth_Sigtia_(Video_Available)","crumbLabel":"2016-11-25.C4DM-Seminar_-_Tian_Cheng_and_Siddharth_Sigtia_(Video_Available)"}]}}},"staticQueryHashes":["537410583"],"slicesMap":{}}
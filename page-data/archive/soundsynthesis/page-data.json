{"componentChunkName":"component---src-templates-archive-post-js","path":"/archive/soundsynthesis/","result":{"data":{"markdownRemark":{"html":"<p>Sound synthesis is the generation of sounds using algorithms, whether implemented in analogue or digital forms. It is an important application for cinema, multimedia, games and sound installations. It fits within the wider context of sound design, which is the discipline of acquiring, creating and manipulating sounds to achieve a desired effect or mood. Sound synthesis research within the Centre for Digital Music crosses several themes, including <a href=\"http://c4dm.eecs.qmul.ac.uk/audioengineering.html\">Audio Engineering</a> and <a href=\"http://www.eecs.qmul.ac.uk/~andrewm/\">Augmented Instruments</a>. We seek to uncover new synthesis techniques, as well as enhance existing approaches and adapt them to new applications. With a strong emphasis on performance, expression and evaluation, much of our research is focused on real world applications, empowering users and bringing sound synthesis to the forefront of sound design in the creative industries.</p>\n<p>Some of our current and recent research projects include;</p>\n<ul>\n<li>RTSFX (PI, Dr. Reiss, Innovate UK, 2015-16) - This project is concerned with developing and assessing a cloud-based real-time sound effects service, providing a streamlined sound synthesis-based workflow for sound designers.</li>\n<li>Digital Foley Artistry (PI, Dr. McPherson, Queen Mary Innovation, 2015) - The Digital Foley project creates a rapid prototyping environment for game audio, deploying procedural audio models to embedded hardware where they can be performed using physical sensors and capturing expressive parameter trajectories from these performances.</li>\n<li>Physically informed procedural audio (Researcher: Rod Selfridge, 2014 - ) – This research is concerned with developing realistic, controllable, real-time procedural audio techniques for synthesizing sound textures (e.g. wind and rain) using physical models.</li>\n<li>Improved sound synthesis through perceptual evaluation (Researcher: Dave Moffat, 2014 - ) – This research seeks to advance the state of the art in sound synthesis evaluation, in order to identify the performance of synthesis techniques, both using objective and subjective measures, thus leading to deeper insights into the advantages and disadvantages of the different approaches, and guiding work towards improved techniques.</li>\n</ul>\n<p>Some relevant, recent publications describing our work include;</p>\n<ul>\n<li>D. Moffat and J. D. Reiss. <a href=\"https://dl.acm.org/citation.cfm?id=3165287\">Perceptual Evaluation of Synthesized Sound Effects</a>. ACM Transactions on Applied Perception (TAP) 15, 2, Article 13 (April 2018).</li>\n<li>R. Selfridge, D. Moffat and J. D. Reiss. <a href=\"http://www.mdpi.com/2076-3417/7/11/1177\">Sound Synthesis of Objects Swinging through Air Using Physical Models</a>. Applied Sciences 7 (11), 2017.</li>\n<li>D. Moffat, D. Ronan and J. D. Reiss, <a href=\"https://dafx17.eca.ed.ac.uk/papers/DAFx17_paper_26.pdf\">Unsupervised Taxonomy of Sound Effects</a>, 20th International Conference on Digital Audio Effects (DAFx-17), Edinburgh UK, September 2017.</li>\n<li>R. Selfridge, D. Moffat and J. D. Reiss. <a href=\"http://davemoffat.com/wp/wp-content/uploads/2017/08/Propeller_AuthorsVersion.pdf\">Physically Derived Sound Synthesis Model of a Propeller</a>. ACM Audio Mostly Conference 2017, London UK, July 2017.</li>\n<li>R. Selfridge, D. Moffat and J. D. Reiss. <a href=\"http://smc2017.aalto.fi/media/materials/proceedings/SMC17_p299.pdf\">Real-time Physical Model for Synthesis of Sword Swing Sound</a>, 14th International Conference on Sound and Music Computing, Helsinki, Finland, July 2017. (Won Best Paper Award)</li>\n<li>R. Selfridge, D. Moffat, J. D. Reiss and E. J. Avital. <a href=\"https://www.iiav.org/archives_icsv_last/2017_icsv24/content/papers/papers/full_paper_169_20170427190827242.pdf\">Real-time Physical Model of an Aeolian Harp</a>, 24th International Congress on Sound and Vibration, London, UK, July 2017.</li>\n<li>L. Mengual, D. Moffat and J. D. Reiss, <a href=\"http://www.eecs.qmul.ac.uk/~josh/documents/2016/mengual%20moffat%20reiss%20-%202016.pdf\">Modal Synthesis of Weapon Sounds</a>, AES 61st International Conference, London, UK, February 10-12, 2016.</li>\n<li>G. Durr, L. Peixoto, M. Souza, R. Tanoue and J. D. Reiss, <a href=\"http://www.eecs.qmul.ac.uk/~josh/documents/2015/Durr%20et%20al%20-%20AES56%20-%202015.pdf\">Implementation and evaluation of dynamic level of audio detail</a>, AES 56th International Conference, London, UK, February 11–13, 2015.</li>\n<li>C. Heinrichs, A. McPherson and A. Farnell. <a href=\"http://www.eecs.qmul.ac.uk/~andrewm/heinrichs-mcpherson-farnell-tns.pdf\">Human performance of computational sound models for immersive environments</a>. The New Soundtrack Journal, 2014.</li>\n<li>C. Heinrichs and A. McPherson. <a href=\"http://www.eecs.qmul.ac.uk/~andrewm/heinrichs-mcpherson-sive.pdf\">Mapping and interaction strategies for performing environmental sound. IEEE VR Workshop on Sonic Interactions for Virtual Environments</a>, Minneapolis, USA, 2014.</li>\n<li>C. Heinrichs and A. McPherson. <a href=\"http://www.smcnetwork.org/system/files/smc2012-176.pdf\">A hybrid keyboard-guitar interface using capacitive touch sensing and physical modelling</a>. Proc. Sound and Music Computing, Copenhagen, Denmark, 2012. <a href=\"https://vimeo.com/42470692\">Video Demo</a></li>\n<li>A. Primavera, F. Piazza and J. D. Reiss, <a href=\"http://www.eecs.qmul.ac.uk/~josh/documents/PrimaveraReiss-AudioMorphing.pdf\">Audio Morphing for Percussive Hybrid Sound Generation</a>, AES 45th Conference on Applications of Time-Frequency Processing in Audio, Helsinki, March 2012.</li>\n<li>A. Zacharakis and J. D. Reiss, <a href=\"http://www.eecs.qmul.ac.uk/~josh/documents/ZacharakisReiss-2011-AES130.pdf\">An additive synthesis technique for the independent modification of the auditory perceptions of brightness and warmth</a>, AES 130th Convention, May 2011.</li>\n<li>S. Hendry and J. D. Reiss, <a href=\"http://www.eecs.qmul.ac.uk/~josh/documents/HendryReiss-AES129.pdf\">Physical Modeling and Synthesis of Motor Noise for Replication of a Sound Effects Library</a>, 129th AES Convention, San Francisco, Nov. 4-7, 2010.</li>\n</ul>\n<p><a href=\"/study.html\">PhD Study</a> - interested in joining the team? We are currently accepting PhD applications.</p>\n<h2>Members</h2>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Project/interests/keywords</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/banarberker.html\">Berker Banar</a></td>\n<td>Towards Composing Contemporary Classical Music using Generative Deep Learning</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/comunitamarco.html\">Marco Comunità</a></td>\n<td>Machine learning applied to sound synthesis models</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/grafmax.html\">Max Graf</a></td>\n<td>PERFORM-AI (Provide Extended Realities for Musical Performance using AI)</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/hayesbenjaminjames.html\">Benjamin Hayes</a></td>\n<td>Perceptually motivated deep learning approaches to creative sound synthesis</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/luoyin-jyun.html\">Yin-Jyun Luo</a></td>\n<td>Industry-scale Machine Listening for Music and Audio Data</td>\n</tr>\n<tr>\n<td><a href=\"http://www.eecs.qmul.ac.uk/~andrewm\">Prof Andrew McPherson</a>  <br>Professor of Musical Interaction</td>\n<td>new interfaces for musical expression, augmented instruments, performance study, human-computer interaction, embedded hardware</td>\n</tr>\n<tr>\n<td><a href=\"https://trebolium.github.io/\">Brendan O'Connor</a></td>\n<td>Singing Voice Attribute Transformation</td>\n</tr>\n<tr>\n<td><a href=\"http://www.eecs.qmul.ac.uk/~josh/\">Prof. Joshua D Reiss</a>  <br>Professor of Audio Engineering</td>\n<td>sound engineering, intelligent audio production, sound synthesis, audio effects, automatic mixing</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/roweleanorroxannevictoria.html\">Eleanor Row</a></td>\n<td>Automatic micro-composition for professional/novice composers using generative models as creativity support tools</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/singhshubhr.html\">Shubhr Singh</a></td>\n<td>Audio Applications of Novel Mathematical Methods in Deep Learning</td>\n</tr>\n<tr>\n<td><a href=\"https://dsuedholt.github.io/\">David Südholt</a></td>\n<td>Machine Learning of Physical Models for Voice Synthesis</td>\n</tr>\n<tr>\n<td>Ningzhi Wang</td>\n<td>Generative Models For Music Audio Representation And Understanding</td>\n</tr>\n<tr>\n<td><a href=\"https://yoyololicon.github.io/\">Chin-Yun Yu</a></td>\n<td>Neural Audio Synthesis with Expressiveness Control</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li><a href=\"http://obiwannabe.co.uk/\">Dr. Andy Farnell</a> (collaborator) – guidance on physically inspired procedural audio approaches and research directions</li>\n</ul>","frontmatter":{"title":"Sound Synthesis Research in the Centre for Digital Music","image":null}}},"pageContext":{"slug":"/archive/soundsynthesis","breadcrumb":{"location":"/archive/soundsynthesis/","crumbs":[{"pathname":"/","crumbLabel":"Home"},{"pathname":"/archive","crumbLabel":"archive"},{"pathname":"/archive/soundsynthesis","crumbLabel":"soundsynthesis"}]}}},"staticQueryHashes":["537410583"],"slicesMap":{}}
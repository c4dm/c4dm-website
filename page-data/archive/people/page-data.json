{"componentChunkName":"component---src-templates-archive-post-js","path":"/archive/people/","result":{"data":{"markdownRemark":{"html":"<h2>Primary academic staff and fellows</h2>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Project/interests/keywords</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"http://www.eecs.qmul.ac.uk/people/view/4808/dr-mathieu-barthet\">Dr Mathieu Barthet</a>  <br>Senior Lecturer in Digital Media</td>\n<td>Music information research, Internet of musical things, Extended reality, New interfaces for musical expression, Semantic audio, Music perception (timbre, emotions), Audience-Performer interaction, Participatory art</td>\n</tr>\n<tr>\n<td><a href=\"http://www.eecs.qmul.ac.uk/people/view/4741/dr-emmanouil-benetos\">Dr Emmanouil Benetos</a>  <br>Reader in Machine Listening, Turing Fellow</td>\n<td>Machine listening, music information retrieval, computational sound scene analysis, machine learning for audio analysis, language models for music and audio, computational musicology</td>\n</tr>\n<tr>\n<td><a href=\"http://www.eecs.qmul.ac.uk/~simond/\">Prof. Simon Dixon</a>  <br>Professor of Computer Science, Deputy Director of C4DM, Director of the AIM CDT, Turing Fellow</td>\n<td>Music informatics, music signal processing, artificial intelligence, music cognition; extraction of musical content (e.g. rhythm, harmony, intonation) from audio signals: beat tracking, audio alignment, chord and note transcription, singing intonation; using signal processing approaches, probabilistic models, and deep learning.</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/~gyorgyf\">Dr George Fazekas</a>  <br>Senior Lecturer</td>\n<td>Semantic Audio, Music Information Retrieval, Semantic Web for Music, Machine Learning and Data Science, Music Emotion Recognition, Interactive music sytems (e.g. intellignet editing, audio production and performance systems)</td>\n</tr>\n<tr>\n<td><a href=\"https://aidanhogg.uk/\">Dr Aidan Hogg</a>  <br>Lecturer in Computer Science</td>\n<td>spatial and immersive audio, music signal processing, machine learning for audio, music information retrieval</td>\n</tr>\n<tr>\n<td><a href=\"http://www.eecs.qmul.ac.uk/~andrewm\">Prof Andrew McPherson</a>  <br>Professor of Musical Interaction</td>\n<td>new interfaces for musical expression, augmented instruments, performance study, human-computer interaction, embedded hardware</td>\n</tr>\n<tr>\n<td><a href=\"http://www.eecs.qmul.ac.uk/people/view/50775/johan-pauwels\">Dr Johan Pauwels</a>  <br>Lecturer in Audio Signal Processing</td>\n<td>automatic music labelling, music information retrieval, music signal processing, machine learning for audio, chord/key/structure (joint) estimation, instrument identification, multi-track/channel audio, music transcription, graphical models, big data science</td>\n</tr>\n<tr>\n<td><a href=\"http://www.eecs.qmul.ac.uk/~josh/\">Prof. Joshua D Reiss</a>  <br>Professor of Audio Engineering</td>\n<td>sound engineering, intelligent audio production, sound synthesis, audio effects, automatic mixing</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/saitischaralampos.html\">Dr Charalampos Saitis</a>  <br>Lecturer in Digital Music Processing, Turing Fellow</td>\n<td>Communication acoustics, crossmodal correspondences, sound synthesis, cognitive audio, musical haptics</td>\n</tr>\n<tr>\n<td><a href=\"http://www.eecs.qmul.ac.uk/people/view/3114/prof-mark-sandler\">Prof Mark Sandler</a>  <br>C4DM Director, Turing Fellow, Royal Society Wolfson Research Merit award holder</td>\n<td>Digital Signal Processing, Digital Audio, Music Informatics, Audio Features, Semantic Audio, Immersive Audio, Studio Science, Music Data Science, Music Linked Data.</td>\n</tr>\n<tr>\n<td><a href=\"http://www.eecs.qmul.ac.uk/people/view/3026/dr-tony-stockman\">Dr Tony Stockman</a>  <br>Senior Lecturer</td>\n<td>Interaction Design, auditory displays, Data Sonification, Collaborative Systems, Cross-modal Interaction, Assistive Technology, Accessibility</td>\n</tr>\n<tr>\n<td><a href=\"http://www.eecs.qmul.ac.uk/~linwang/\">Dr Lin Wang</a>  <br>Lecturer in Applied Data Science and Signal Processing</td>\n<td>signal processing; machine learning; robot perception</td>\n</tr>\n<tr>\n<td><a href=\"https://annaxambo.me/\">Dr Anna Xambó</a>  <br>Senior Lecturer in Sound and Music Computing</td>\n<td>new interfaces for musical expression, performance study, human-computer interaction, interaction design</td>\n</tr>\n</tbody>\n</table>\n<h2>Associate academic staff and fellows</h2>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Project/interests/keywords</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"http://www.eecs.qmul.ac.uk/%7Eph/\">Prof Pat Healey</a>  <br>Professor of Human Interaction, Turing Fellow</td>\n<td></td>\n</tr>\n<tr>\n<td><a href=\"https://www.marcus-pearce.com\">Dr Marcus Pearce</a>  <br>Senior Lecturer in Sound &#x26; Music Processing</td>\n<td>Music Cognition, Auditory Perception, Empirical Aesthetics, Statistical Learning, Probabilistic Modelling.</td>\n</tr>\n<tr>\n<td><a href=\"http://www.eecs.qmul.ac.uk/~mpurver/\">Prof Matthew Purver</a>  <br>Professor of Computational Linguistics, Turing Fellow</td>\n<td>computational linguistics including models of language and music</td>\n</tr>\n<tr>\n<td><a href=\"https://ai.vub.ac.be/team/geraint-wiggins/?utm_source=www.google.com&#x26;utm_medium=organic&#x26;utm_campaign=Google&#x26;referrer-analytics=1\">Prof Geraint Wiggins</a>  <br>Professor of Computational Creativity</td>\n<td>Computational Creativity, Artificial Intelligence, Music Cognition</td>\n</tr>\n</tbody>\n</table>\n<h2>Research support staff</h2>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Project/interests/keywords</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/bortalvaro.html\">Alvaro Bort</a>  <br>Research Programme Manager</td>\n<td>Projects: UKRI Centre for Doctoral Training in Artificial Intelligence and Music, New Frontiers in Music Information Processing (MIP-Frontiers)</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/winfieldjonathan.html\">Jonathan Winfield</a>  <br>Research Programme Manager</td>\n<td>Project: Centre for Doctoral Training in Media and Arts Technology</td>\n</tr>\n</tbody>\n</table>\n<h2>Postdoctoral research assistants</h2>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Project/interests/keywords</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Dr Jacob Harrison</td>\n<td>Bridging the gap: visually impaired and sighted music industry professionals working side by side</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/liuyuanyuan.html\">Dr Yuanyuan Liu</a></td>\n<td>Project: Digital Platforms for Craft in the UK and China</td>\n</tr>\n</tbody>\n</table>\n<h2>Research assistants</h2>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Project/interests/keywords</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/sungkyun-chang.html\">Sungkyun Chang</a></td>\n<td>Deep learning technologies for multi-instrument automatic music transcription</td>\n</tr>\n</tbody>\n</table>\n<h2>Research students</h2>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Project/interests/keywords</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/banarberker.html\">Berker Banar</a></td>\n<td>Towards Composing Contemporary Classical Music using Generative Deep Learning</td>\n</tr>\n<tr>\n<td>Adán Benito</td>\n<td>Beyond the fret: gesture analysis on fretted instruments and its applications to instrument augmentation</td>\n</tr>\n<tr>\n<td><a href=\"https://www.linkedin.com/in/adibh/\">Aditya Bhattacharjee</a></td>\n<td>Self-supervision in Audio Fingerprinting</td>\n</tr>\n<tr>\n<td>James Bolt</td>\n<td>Intelligent audio and music editing with deep learning</td>\n</tr>\n<tr>\n<td>Gary Bromham</td>\n<td>The role of nostalga in music production</td>\n</tr>\n<tr>\n<td>Carey Bunks</td>\n<td>Cover Song Identification</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/comunitamarco.html\">Marco Comunità</a></td>\n<td>Machine learning applied to sound synthesis models</td>\n</tr>\n<tr>\n<td>Ruby Crocker</td>\n<td>Continuous mood recognition in film music</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/edwardsandrewcharles.html\">Andrew (Drew) Edwards</a></td>\n<td>Deep Learning for Jazz Piano: Transcription + Generative Modeling</td>\n</tr>\n<tr>\n<td>Oluremi Falowo</td>\n<td>E-AIM - Embodied Cognition in Intelligent Musical Systems</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/fordcoreyjohn.html\">Corey Ford</a></td>\n<td>Artificial Intelligence for Supporting Musical Creativity and Engagement in Child-Computer Interaction</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/fosterdavid.html\">David Foster</a></td>\n<td>Modelling the Creative Process of Jazz Improvisation</td>\n</tr>\n<tr>\n<td>Nelly Garcia</td>\n<td>An investigation evaluating realism in sound design</td>\n</tr>\n<tr>\n<td><a href=\"https://www.researchgate.net/profile/Adam-Garrow\">Adam Andrew Garrow</a></td>\n<td>: Probabilistic learning of sequential structures in music cognition</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/Ighina\">Iacopo Ghinassi</a></td>\n<td>Semantic understanding of TV programme content and structure to enable automatic enhancement and adjustment</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/grafmax.html\">Max Graf</a></td>\n<td>PERFORM-AI (Provide Extended Realities for Musical Performance using AI)</td>\n</tr>\n<tr>\n<td><a href=\"https://mat.qmul.ac.uk/students/andrea-guidi\">Andrea Guidi</a></td>\n<td>Design for auditory imagery</td>\n</tr>\n<tr>\n<td><a href=\"https://mat.qmul.ac.uk/students/edward-hall\">Edward Hall</a></td>\n<td>Probabilistic modelling of thematic development and structural coherence in music</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/hamiltonmadelineann.html\">Madeline Hamilton</a></td>\n<td>Improving AI-generated Music with Pleasure Models</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/hayesbenjaminjames.html\">Benjamin Hayes</a></td>\n<td>Perceptually motivated deep learning approaches to creative sound synthesis</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/huangjiawen.html\">Jiawen Huang</a></td>\n<td>Lyrics Alignment For Polyphonic Music</td>\n</tr>\n<tr>\n<td><a href=\"https://ilias-audio.github.io/\">Ilias Ibnyahya</a></td>\n<td>Audio Effects design optimization</td>\n</tr>\n<tr>\n<td><a href=\"https://kappers.github.io/\">Thomas Kaplan</a></td>\n<td>Probabilistic modelling of rhythm perception and production</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/kheraharnicksingh.html\">Harnick Khera</a></td>\n<td>Informed source separation for multi-mic production</td>\n</tr>\n<tr>\n<td><a href=\"http://www.giacomolepri.com/\">Giacomo Lepri</a></td>\n<td>Exploring the role of culture and community in the design of new musical instruments</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/liyukun.html\">Yukun Li</a></td>\n<td>Computational Comparison Between Different Genres of Music in Terms of the Singing Voice</td>\n</tr>\n<tr>\n<td><a href=\"https://jinhualiang.github.io/\">Jinhua Liang</a></td>\n<td>AI for everyday sounds</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/liulele.html\">Lele Liu</a></td>\n<td>Automatic music transcription with end-to-end deep neural networks</td>\n</tr>\n<tr>\n<td><a href=\"https://cpvlordelo.github.io/\">Carlos Lordelo</a></td>\n<td>Instrument modelling to aid polyphonic transcription</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/luoyin-jyun.html\">Yin-Jyun Luo</a></td>\n<td>Industry-scale Machine Listening for Music and Audio Data</td>\n</tr>\n<tr>\n<td><a href=\"https://nicolaus625.github.io/\">Yinghao Ma</a></td>\n<td>Self-supervision in machine listening</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/mancoilaria.html\">Ilaria Manco</a></td>\n<td>Multimodal Deep Learning for Music Information Retrieval</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/marinelliluca.html\">Luca Marinelli</a></td>\n<td>Gender-coded sound: A multimodal data-driven analysis of gender encoding strategies in sound and music for advertising</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/martelloniandrea-1.html\">Andrea Martelloni</a></td>\n<td>Real-Time Gesture Classification on an Augmented Acoustic Guitar using Deep Learning to Improve Extended-Range and Percussive Solo Playing</td>\n</tr>\n<tr>\n<td>Tyler Howard McIntosh</td>\n<td>Expressive Performance Rendering for Music Generation Systems</td>\n</tr>\n<tr>\n<td><a href=\"https://christhetr.ee\">Christopher Mitcheltree</a></td>\n<td>Representation Learning for Audio Production Style and Modulations</td>\n</tr>\n<tr>\n<td><a href=\"https://ashleynoelhirst.co.uk/\">Ashley Noel-Hirst</a></td>\n<td>Latent Spaces for Human-AI music generation</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/nolascoines.html\">Inês Nolasco</a></td>\n<td>Towards an automatic acoustic identification of individuals in the wild</td>\n</tr>\n<tr>\n<td><a href=\"https://trebolium.github.io/\">Brendan O'Connor</a></td>\n<td>Singing Voice Attribute Transformation</td>\n</tr>\n<tr>\n<td><a href=\"https://sites.google.com/view/arjunpc4dm/home\">Arjun Pankajakshan</a></td>\n<td>Computational sound scene analysis</td>\n</tr>\n<tr>\n<td><a href=\"https://www.teresapelinski.com/\">Teresa Pelinski</a></td>\n<td>Sensor mesh as performance interface</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/marypilataki\">Mary Pilataki</a></td>\n<td>Deep Learning methods for Multi-Instrument Music Transcription</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/preniqivjosa.html\">Vjosa Preniqi</a></td>\n<td>Predicting demographics, personalities, and global values from digital media behaviours</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/rileyjohnxavier.html\">Xavier Riley</a></td>\n<td>Pitch tracking for music applications - beyond 99% accuracy</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/roweleanorroxannevictoria.html\">Eleanor Row</a></td>\n<td>Automatic micro-composition for professional/novice composers using generative models as creativity support tools</td>\n</tr>\n<tr>\n<td>Sebastián Ruiz</td>\n<td>Physiological Responses to Ensemble Interaction</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/sarkarsaurjya-1.html\">Saurjya Sarkar</a></td>\n<td>New perspectives in instrument-based audio source separation</td>\n</tr>\n<tr>\n<td><a href=\"https://otnemrasordep.github.io/\">Pedro Sarmento</a></td>\n<td>Guitar-Oriented Neural Music Generation in Symbolic Format</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/senvaitytedalia.html\">Dalia Senvaityte</a></td>\n<td>Audio Source Separation for Advanced Digital Audio Effects</td>\n</tr>\n<tr>\n<td><a href=\"https://comma.eecs.qmul.ac.uk/people/bleiz/\">Bleiz Del Sette</a></td>\n<td>The Sound of Care: researching the use of Deep Learning and Sonification for the daily support of people with Chronic Primary Pain</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/shatrielona-1.html\">Elona Shatri</a></td>\n<td>Optical music recognition using deep learning</td>\n</tr>\n<tr>\n<td><a href=\"https://jordieshier.com/\">Jordie Shier</a></td>\n<td>Real-time timbral mapping for synthesized percussive performance</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/singhshubhr.html\">Shubhr Singh</a></td>\n<td>Audio Applications of Novel Mathematical Methods in Deep Learning</td>\n</tr>\n<tr>\n<td><a href=\"https://www.christiansteinmetz.com/\">Christian Steinmetz</a></td>\n<td>End-to-end generative modeling of multitrack mixing with non-parallel data and adversarial networks</td>\n</tr>\n<tr>\n<td><a href=\"https://dsuedholt.github.io/\">David Südholt</a></td>\n<td>Machine Learning of Physical Models for Voice Synthesis</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/tangjingjing.html\">Jingjing Tang</a></td>\n<td>End-to-End System Design for Music Style Transfer with Neural Networks</td>\n</tr>\n<tr>\n<td>Louise Thorpe</td>\n<td>Using Signal-informed Source Separation (SISS) principles to improve instrument separation from legacy recordings</td>\n</tr>\n<tr>\n<td><a href=\"https://www.researchgate.net/profile/Antonella-Torrisi\">Antonella Torrisi</a></td>\n<td>Computational analysis of chick vocalisations: from categorisation to live feedback</td>\n</tr>\n<tr>\n<td>Maryam Torshizi</td>\n<td>Music emotion modelling using graph analysis</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/vahidicyrus.html\">Cyrus Vahidi</a></td>\n<td>Perceptual end to end learning for music understanding</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/vankasaisoumya.html\">Soumya Sai Vanka</a></td>\n<td>Music Production Style Transfer and Mix Similarity</td>\n</tr>\n<tr>\n<td><a href=\"https://www.linkedin.com/in/yannis-vasilakis-6bb9b11b1/\">Yannis (John) Vasilakis</a></td>\n<td>Active Learning for Interactive Music Transcription</td>\n</tr>\n<tr>\n<td>Ningzhi Wang</td>\n<td>Generative Models For Music Audio Representation And Understanding</td>\n</tr>\n<tr>\n<td>James Weaver</td>\n<td>Space and Intelligibility of Musical Performance</td>\n</tr>\n<tr>\n<td>Alexander Williams</td>\n<td>User-driven deep music generation in digital audio workstations</td>\n</tr>\n<tr>\n<td><a href=\"https://lwlsn.github.io\">Elizabeth Wilson</a></td>\n<td>Co-creative Algorithmic Composition Based on Models of Affective Response</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/winnardchristopherjames.html\">Chris Winnard</a></td>\n<td>Music Interestingness in the Brain</td>\n</tr>\n<tr>\n<td><a href=\"http://lewiswolstanholme.co.uk\">Lewis Wolstanholme</a></td>\n<td>Meta-Physical Modelling</td>\n</tr>\n<tr>\n<td>Chengye Wu</td>\n<td>Leveraging cross-sensory associations in communication</td>\n</tr>\n<tr>\n<td><a href=\"https://yoyololicon.github.io/\">Chin-Yun Yu</a></td>\n<td>Neural Audio Synthesis with Expressiveness Control</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/people/profiles/zhanghuan.html\">Huan Zhang</a></td>\n<td>Computational Modelling of Expressive Piano Performance</td>\n</tr>\n<tr>\n<td>Jincheng Zhang</td>\n<td>Emotion-specific Music Generation Using Deep Learning</td>\n</tr>\n<tr>\n<td><a href=\"http://eecs.qmul.ac.uk/profiles/zhangyixiao.html\">Yixiao Zhang</a></td>\n<td>Machine Learning Methods for Artificial Musicality</td>\n</tr>\n</tbody>\n</table>\n<h2>Visiting academics</h2>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Project/interests/keywords</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"http://www.eecs.qmul.ac.uk/profiles/bearhelen.html\">Dr Helen Bear</a>  <br>Honorary Lecturer</td>\n<td>Integrating sound and context recognition for acoustic scene analysis</td>\n</tr>\n<tr>\n<td><a href=\"http://www.matthiasmauch.net/\">Dr Matthias Mauch</a>  <br>Visiting Academic</td>\n<td>music transcription (chords, beats, drums, melody, ...), interactive music annotation, singing research, research in the evolution of musical styles</td>\n</tr>\n<tr>\n<td><a href=\"https://scholar.google.co.uk/citations?user=8izRvu4AAAAJ&#x26;hl=en\">Dr Veronica Morfi</a></td>\n<td>Machine transcription of wildlife bird sound scenes</td>\n</tr>\n<tr>\n<td><a href=\"https://iwk.mdw.ac.at/montserrat-pamies-vila/\">Dr Montserrat Pàmies-Vilà</a>  <br>University of Music and Performing Arts Vienna</td>\n<td>Timbre modelling for non-conventional cello techniques</td>\n</tr>\n</tbody>\n</table>\n<h2>Visitors</h2>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Project/interests/keywords</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://domenicostefani.com/\">Domenico Stefani</a>  <br>University of Trento, Italy</td>\n<td>Embedded machine learning for smart musical instruments</td>\n</tr>\n</tbody>\n</table>","frontmatter":{"title":"People","image":null}}},"pageContext":{"slug":"/archive/people","breadcrumb":{"location":"/archive/people/","crumbs":[{"pathname":"/","crumbLabel":"Home"},{"pathname":"/archive","crumbLabel":"archive"},{"pathname":"/archive/people","crumbLabel":"people"}]}}},"staticQueryHashes":["537410583"],"slicesMap":{}}
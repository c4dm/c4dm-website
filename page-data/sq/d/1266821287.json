{"data":{"people":{"nodes":[{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#e8f8f8","images":{"fallback":{"src":"/static/543ed3a102028b2ed1952a0a04a3d543/d558d/aidanhogg.jpg","srcSet":"/static/543ed3a102028b2ed1952a0a04a3d543/7fbe3/aidanhogg.jpg 142w,\n/static/543ed3a102028b2ed1952a0a04a3d543/46b1b/aidanhogg.jpg 284w,\n/static/543ed3a102028b2ed1952a0a04a3d543/d558d/aidanhogg.jpg 567w","sizes":"(min-width: 567px) 567px, 100vw"},"sources":[{"srcSet":"/static/543ed3a102028b2ed1952a0a04a3d543/bcdd4/aidanhogg.webp 142w,\n/static/543ed3a102028b2ed1952a0a04a3d543/5d551/aidanhogg.webp 284w,\n/static/543ed3a102028b2ed1952a0a04a3d543/b709e/aidanhogg.webp 567w","type":"image/webp","sizes":"(min-width: 567px) 567px, 100vw"}]},"width":567,"height":567}}},"name":"Dr Aidan Hogg","url":"https://aidanhogg.uk/","acadposition":"Lecturer in Computer Science","blurb":"spatial and immersive audio, music signal processing, machine learning for audio, music information retrieval","themes":["comma","mir"],"role":"Academic"},"id":"87088d07-80bf-5486-9d68-2bc54fb7d6e9"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#484858","images":{"fallback":{"src":"/static/3d59fe8293835010c7ad4f180e4cc954/dd515/annaxambo.jpg","srcSet":"/static/3d59fe8293835010c7ad4f180e4cc954/6ac16/annaxambo.jpg 50w,\n/static/3d59fe8293835010c7ad4f180e4cc954/e07e1/annaxambo.jpg 100w,\n/static/3d59fe8293835010c7ad4f180e4cc954/dd515/annaxambo.jpg 200w","sizes":"(min-width: 200px) 200px, 100vw"},"sources":[{"srcSet":"/static/3d59fe8293835010c7ad4f180e4cc954/dbc4a/annaxambo.webp 50w,\n/static/3d59fe8293835010c7ad4f180e4cc954/d8057/annaxambo.webp 100w,\n/static/3d59fe8293835010c7ad4f180e4cc954/2e34e/annaxambo.webp 200w","type":"image/webp","sizes":"(min-width: 200px) 200px, 100vw"}]},"width":200,"height":200}}},"name":"Dr Anna Xamb√≥","url":"https://annaxambo.me/","acadposition":"Senior Lecturer in Sound and Music Computing","blurb":"new interfaces for musical expression, performance study, human-computer interaction, interaction design","themes":["augmi","mir","isam"],"role":"Academic"},"id":"1f3913f4-5507-56c4-a92a-0782c7b095bb"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#685858","images":{"fallback":{"src":"/static/75b0458ab899e559446e8ecba1f3fb46/30f07/charissaitis.jpg","srcSet":"/static/75b0458ab899e559446e8ecba1f3fb46/41624/charissaitis.jpg 160w,\n/static/75b0458ab899e559446e8ecba1f3fb46/1b894/charissaitis.jpg 320w,\n/static/75b0458ab899e559446e8ecba1f3fb46/30f07/charissaitis.jpg 640w","sizes":"(min-width: 640px) 640px, 100vw"},"sources":[{"srcSet":"/static/75b0458ab899e559446e8ecba1f3fb46/60b4d/charissaitis.webp 160w,\n/static/75b0458ab899e559446e8ecba1f3fb46/5e011/charissaitis.webp 320w,\n/static/75b0458ab899e559446e8ecba1f3fb46/90d07/charissaitis.webp 640w","type":"image/webp","sizes":"(min-width: 640px) 640px, 100vw"}]},"width":640,"height":640}}},"name":"Dr Charalampos Saitis","url":"http://eecs.qmul.ac.uk/profiles/saitischaralampos.html","acadposition":"Lecturer in Digital Music Processing","blurb":"Communication acoustics, crossmodal correspondences, sound synthesis, cognitive audio, musical haptics","themes":["comma","mcog"],"role":"Academic"},"id":"05041b7b-ff55-5565-8f24-a036490ca8e5"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/8106f707dd10b39434bd1c08f1f22f6d/30f07/emmanouilbenetos.jpg","srcSet":"/static/8106f707dd10b39434bd1c08f1f22f6d/41624/emmanouilbenetos.jpg 160w,\n/static/8106f707dd10b39434bd1c08f1f22f6d/1b894/emmanouilbenetos.jpg 320w,\n/static/8106f707dd10b39434bd1c08f1f22f6d/30f07/emmanouilbenetos.jpg 640w","sizes":"(min-width: 640px) 640px, 100vw"},"sources":[{"srcSet":"/static/8106f707dd10b39434bd1c08f1f22f6d/60b4d/emmanouilbenetos.webp 160w,\n/static/8106f707dd10b39434bd1c08f1f22f6d/5e011/emmanouilbenetos.webp 320w,\n/static/8106f707dd10b39434bd1c08f1f22f6d/90d07/emmanouilbenetos.webp 640w","type":"image/webp","sizes":"(min-width: 640px) 640px, 100vw"}]},"width":640,"height":640}}},"name":"Dr Emmanouil Benetos","url":"http://www.eecs.qmul.ac.uk/people/view/4741/dr-emmanouil-benetos","acadposition":"Reader in Machine Listening, Turing Fellow","blurb":"Machine listening, music information retrieval, computational sound scene analysis, machine learning for audio analysis, language models for music and audio, computational musicology","themes":["mir","mlist"],"role":"Academic"},"id":"ef8d4761-ec9b-58f0-bfa5-26159c60c38e"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#a8b8e8","images":{"fallback":{"src":"/static/eaf04fbbdffdb3aca0caed16cfdf1102/7900e/gyorgyfazekas.jpg","srcSet":"/static/eaf04fbbdffdb3aca0caed16cfdf1102/f713e/gyorgyfazekas.jpg 55w,\n/static/eaf04fbbdffdb3aca0caed16cfdf1102/e2afd/gyorgyfazekas.jpg 110w,\n/static/eaf04fbbdffdb3aca0caed16cfdf1102/7900e/gyorgyfazekas.jpg 219w","sizes":"(min-width: 219px) 219px, 100vw"},"sources":[{"srcSet":"/static/eaf04fbbdffdb3aca0caed16cfdf1102/938d3/gyorgyfazekas.webp 55w,\n/static/eaf04fbbdffdb3aca0caed16cfdf1102/8c6ff/gyorgyfazekas.webp 110w,\n/static/eaf04fbbdffdb3aca0caed16cfdf1102/3f57d/gyorgyfazekas.webp 219w","type":"image/webp","sizes":"(min-width: 219px) 219px, 100vw"}]},"width":219,"height":219}}},"name":"Dr George Fazekas","url":"http://eecs.qmul.ac.uk/~gyorgyf","acadposition":"Senior Lecturer","blurb":"Semantic Audio, Music Information Retrieval, Semantic Web for Music, Machine Learning and Data Science, Music Emotion Recognition, Interactive music sytems (e.g. intellignet editing, audio production and performance systems)","themes":["mir"],"role":"Academic"},"id":"9aa31664-07ac-5194-a7fe-146cab80fddb"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#c8a878","images":{"fallback":{"src":"/static/28c5745e61741b22792aa6d4a135584a/68974/iranroman.jpg","srcSet":"/static/28c5745e61741b22792aa6d4a135584a/d4a57/iranroman.jpg 64w,\n/static/28c5745e61741b22792aa6d4a135584a/19e71/iranroman.jpg 128w,\n/static/28c5745e61741b22792aa6d4a135584a/68974/iranroman.jpg 256w","sizes":"(min-width: 256px) 256px, 100vw"},"sources":[{"srcSet":"/static/28c5745e61741b22792aa6d4a135584a/8257c/iranroman.webp 64w,\n/static/28c5745e61741b22792aa6d4a135584a/6766a/iranroman.webp 128w,\n/static/28c5745e61741b22792aa6d4a135584a/22bfc/iranroman.webp 256w","type":"image/webp","sizes":"(min-width: 256px) 256px, 100vw"}]},"width":256,"height":256}}},"name":"Dr Iran Roman","url":"https://iranroman.github.io/","acadposition":"Lecturer in Sound and Music Computing","blurb":"theoretical neuroscience, machine perception, artificial intelligence","themes":["comma","mir","mlist"],"role":"Academic"},"id":"be6e7a39-4760-57e6-a607-f988d7a322da"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8a898","images":{"fallback":{"src":"/static/5c4ebc68a7723fc56115ed3fe749ff7a/96deb/johanpauwels.jpg","srcSet":"/static/5c4ebc68a7723fc56115ed3fe749ff7a/c81d1/johanpauwels.jpg 38w,\n/static/5c4ebc68a7723fc56115ed3fe749ff7a/91a6d/johanpauwels.jpg 75w,\n/static/5c4ebc68a7723fc56115ed3fe749ff7a/96deb/johanpauwels.jpg 150w","sizes":"(min-width: 150px) 150px, 100vw"},"sources":[{"srcSet":"/static/5c4ebc68a7723fc56115ed3fe749ff7a/0852d/johanpauwels.webp 38w,\n/static/5c4ebc68a7723fc56115ed3fe749ff7a/18188/johanpauwels.webp 75w,\n/static/5c4ebc68a7723fc56115ed3fe749ff7a/c65bc/johanpauwels.webp 150w","type":"image/webp","sizes":"(min-width: 150px) 150px, 100vw"}]},"width":150,"height":150}}},"name":"Dr Johan Pauwels","url":"http://www.eecs.qmul.ac.uk/people/view/50775/johan-pauwels","acadposition":"Lecturer in Audio Signal Processing","blurb":"automatic music labelling, music information retrieval, music signal processing, machine learning for audio, chord/key/structure (joint) estimation, instrument identification, multi-track/channel audio, music transcription, graphical models, big data science","themes":["mir","mlist"],"role":"Academic"},"id":"b3e3ac4a-cd06-5d69-8ac9-f47d73ef5c0b"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#3898d8","images":{"fallback":{"src":"/static/7712519d3052de12533c72fb8d0992e0/73bb6/linwang.jpg","srcSet":"/static/7712519d3052de12533c72fb8d0992e0/3c559/linwang.jpg 30w,\n/static/7712519d3052de12533c72fb8d0992e0/93848/linwang.jpg 60w,\n/static/7712519d3052de12533c72fb8d0992e0/73bb6/linwang.jpg 120w","sizes":"(min-width: 120px) 120px, 100vw"},"sources":[{"srcSet":"/static/7712519d3052de12533c72fb8d0992e0/bde72/linwang.webp 30w,\n/static/7712519d3052de12533c72fb8d0992e0/927d1/linwang.webp 60w,\n/static/7712519d3052de12533c72fb8d0992e0/507b0/linwang.webp 120w","type":"image/webp","sizes":"(min-width: 120px) 120px, 100vw"}]},"width":120,"height":120}}},"name":"Dr Lin Wang","url":"http://www.eecs.qmul.ac.uk/~linwang/","acadposition":"Lecturer in Applied Data Science and Signal Processing","blurb":"signal processing; machine learning; robot perception","themes":["mlist","mir"],"role":"Academic"},"id":"aaea13f6-1fd2-57d5-b574-a5cc0cee002d"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/661d84eba8a4676c5210510bd4b2c4b6/93106/mathieubarthet.webp","srcSet":"/static/661d84eba8a4676c5210510bd4b2c4b6/c23ec/mathieubarthet.webp 123w,\n/static/661d84eba8a4676c5210510bd4b2c4b6/02ea5/mathieubarthet.webp 246w,\n/static/661d84eba8a4676c5210510bd4b2c4b6/93106/mathieubarthet.webp 492w","sizes":"(min-width: 492px) 492px, 100vw"},"sources":[]},"width":492,"height":492}}},"name":"Dr Mathieu Barthet","url":"http://www.eecs.qmul.ac.uk/people/view/4808/dr-mathieu-barthet","acadposition":"Senior Lecturer in Digital Media","blurb":"Music information research, Internet of musical things, Extended reality, New interfaces for musical expression, Semantic audio, Music perception (timbre, emotions), Audience-Performer interaction, Participatory art","themes":["mir","augmi","isam","mupae"],"role":"Academic"},"id":"d74e8c2b-56f9-50cf-a43b-8b0191803bff"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/5e2239e574c579219804778dc3392423/e07e1/tonystockman.jpg","srcSet":"/static/5e2239e574c579219804778dc3392423/74ef0/tonystockman.jpg 25w,\n/static/5e2239e574c579219804778dc3392423/6ac16/tonystockman.jpg 50w,\n/static/5e2239e574c579219804778dc3392423/e07e1/tonystockman.jpg 100w","sizes":"(min-width: 100px) 100px, 100vw"},"sources":[{"srcSet":"/static/5e2239e574c579219804778dc3392423/2fa99/tonystockman.webp 25w,\n/static/5e2239e574c579219804778dc3392423/dbc4a/tonystockman.webp 50w,\n/static/5e2239e574c579219804778dc3392423/d8057/tonystockman.webp 100w","type":"image/webp","sizes":"(min-width: 100px) 100px, 100vw"}]},"width":100,"height":100}}},"name":"Dr Tony Stockman","url":"http://www.eecs.qmul.ac.uk/people/view/3026/dr-tony-stockman","acadposition":"Senior Lecturer","blurb":"Interaction Design, auditory displays, Data Sonification, Collaborative Systems, Cross-modal Interaction, Assistive Technology, Accessibility","themes":["isam"],"role":"Academic"},"id":"f6a0206a-60ef-5eb9-b2f5-ac4807ebb164"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#8898c8","images":{"fallback":{"src":"/static/bf8d7356ac6d442350ba1cb9bcc35445/baaed/andrewmcpherson.jpg","srcSet":"/static/bf8d7356ac6d442350ba1cb9bcc35445/dd515/andrewmcpherson.jpg 200w,\n/static/bf8d7356ac6d442350ba1cb9bcc35445/47930/andrewmcpherson.jpg 400w,\n/static/bf8d7356ac6d442350ba1cb9bcc35445/baaed/andrewmcpherson.jpg 800w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[{"srcSet":"/static/bf8d7356ac6d442350ba1cb9bcc35445/2e34e/andrewmcpherson.webp 200w,\n/static/bf8d7356ac6d442350ba1cb9bcc35445/416c3/andrewmcpherson.webp 400w,\n/static/bf8d7356ac6d442350ba1cb9bcc35445/c1587/andrewmcpherson.webp 800w","type":"image/webp","sizes":"(min-width: 800px) 800px, 100vw"}]},"width":800,"height":800}}},"name":"Prof Andrew McPherson","url":"http://www.eecs.qmul.ac.uk/~andrewm","acadposition":"Professor of Musical Interaction","blurb":"new interfaces for musical expression, augmented instruments, performance study, human-computer interaction, embedded hardware","themes":["augmi","soundsynthesis"],"role":"Academic"},"id":"5dd23ff3-3112-5b88-8b4e-f6493bdcdbac"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#d8c8c8","images":{"fallback":{"src":"/static/22312dadef02a15635234aa9c85e6a6b/6f467/marksandler.jpg","srcSet":"/static/22312dadef02a15635234aa9c85e6a6b/99d04/marksandler.jpg 247w,\n/static/22312dadef02a15635234aa9c85e6a6b/72fdf/marksandler.jpg 494w,\n/static/22312dadef02a15635234aa9c85e6a6b/6f467/marksandler.jpg 988w","sizes":"(min-width: 988px) 988px, 100vw"},"sources":[{"srcSet":"/static/22312dadef02a15635234aa9c85e6a6b/f19fb/marksandler.webp 247w,\n/static/22312dadef02a15635234aa9c85e6a6b/dab72/marksandler.webp 494w,\n/static/22312dadef02a15635234aa9c85e6a6b/3c8a3/marksandler.webp 988w","type":"image/webp","sizes":"(min-width: 988px) 988px, 100vw"}]},"width":988,"height":988}}},"name":"Prof Mark Sandler ","url":"http://www.eecs.qmul.ac.uk/people/view/3114/prof-mark-sandler","acadposition":"C4DM Director","blurb":"Digital Signal Processing, Digital Audio, Music Informatics, Audio Features, Semantic Audio, Immersive Audio, Studio Science, Music Data Science, Music Linked Data.","themes":["mir"],"role":"Academic"},"id":"3fffc866-8036-535d-8b12-673653a1e811"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8d8f8","images":{"fallback":{"src":"/static/e3f990a7d5022b64b25e5c46e4a6d5bf/47930/joshuadreiss.jpg","srcSet":"/static/e3f990a7d5022b64b25e5c46e4a6d5bf/e07e1/joshuadreiss.jpg 100w,\n/static/e3f990a7d5022b64b25e5c46e4a6d5bf/dd515/joshuadreiss.jpg 200w,\n/static/e3f990a7d5022b64b25e5c46e4a6d5bf/47930/joshuadreiss.jpg 400w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/e3f990a7d5022b64b25e5c46e4a6d5bf/d8057/joshuadreiss.webp 100w,\n/static/e3f990a7d5022b64b25e5c46e4a6d5bf/2e34e/joshuadreiss.webp 200w,\n/static/e3f990a7d5022b64b25e5c46e4a6d5bf/416c3/joshuadreiss.webp 400w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":400}}},"name":"Prof. Joshua D Reiss","url":"http://www.eecs.qmul.ac.uk/~josh/","acadposition":"Professor of Audio Engineering","blurb":"sound engineering, intelligent audio production, sound synthesis, audio effects, automatic mixing","themes":["audioeng","soundsynthesis"],"role":"Academic"},"id":"ca2575a9-c402-50c9-a660-eea9648428a6"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/91f7a353a15c6e68c71f84ff92066743/ffdfb/simondixon.png","srcSet":"/static/91f7a353a15c6e68c71f84ff92066743/5bd84/simondixon.png 92w,\n/static/91f7a353a15c6e68c71f84ff92066743/f9b49/simondixon.png 184w,\n/static/91f7a353a15c6e68c71f84ff92066743/ffdfb/simondixon.png 367w","sizes":"(min-width: 367px) 367px, 100vw"},"sources":[{"srcSet":"/static/91f7a353a15c6e68c71f84ff92066743/483b8/simondixon.webp 92w,\n/static/91f7a353a15c6e68c71f84ff92066743/42e3f/simondixon.webp 184w,\n/static/91f7a353a15c6e68c71f84ff92066743/46fdb/simondixon.webp 367w","type":"image/webp","sizes":"(min-width: 367px) 367px, 100vw"}]},"width":367,"height":367}}},"name":"Prof. Simon Dixon","url":"http://www.eecs.qmul.ac.uk/~simond/","acadposition":"Professor of Computer Science, Deputy Director of C4DM, Director of the AIM CDT","blurb":"Music informatics, music signal processing, artificial intelligence, music cognition; extraction of musical content (e.g. rhythm, harmony, intonation) from audio signals: beat tracking, audio alignment, chord and note transcription, singing intonation; using signal processing approaches, probabilistic models, and deep learning.","themes":["mir"],"role":"Academic"},"id":"06196503-f058-5081-ab1e-21e9b406e714"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/41bb8cd95ee27c26627d3667c74c073f/0c4eb/marcuspearce.jpg","srcSet":"/static/41bb8cd95ee27c26627d3667c74c073f/7b799/marcuspearce.jpg 46w,\n/static/41bb8cd95ee27c26627d3667c74c073f/66e87/marcuspearce.jpg 92w,\n/static/41bb8cd95ee27c26627d3667c74c073f/0c4eb/marcuspearce.jpg 183w","sizes":"(min-width: 183px) 183px, 100vw"},"sources":[{"srcSet":"/static/41bb8cd95ee27c26627d3667c74c073f/1a9ee/marcuspearce.webp 46w,\n/static/41bb8cd95ee27c26627d3667c74c073f/483b8/marcuspearce.webp 92w,\n/static/41bb8cd95ee27c26627d3667c74c073f/04870/marcuspearce.webp 183w","type":"image/webp","sizes":"(min-width: 183px) 183px, 100vw"}]},"width":183,"height":183}}},"name":"Dr Marcus Pearce","url":"https://www.marcus-pearce.com","acadposition":"Reader in Cognitive Science","blurb":"Music Cognition, Auditory Perception, Empirical Aesthetics, Statistical Learning, Probabilistic Modelling.","themes":["mcog"],"role":"Academic Associate"},"id":"ceebf4f8-06c5-5a36-9420-47c585bd8247"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#c8c8c8","images":{"fallback":{"src":"/static/ac1b43fa43492b09e0c056f15501f65b/7683a/geraintwiggins.webp","srcSet":"/static/ac1b43fa43492b09e0c056f15501f65b/9a807/geraintwiggins.webp 36w,\n/static/ac1b43fa43492b09e0c056f15501f65b/de323/geraintwiggins.webp 72w,\n/static/ac1b43fa43492b09e0c056f15501f65b/7683a/geraintwiggins.webp 143w","sizes":"(min-width: 143px) 143px, 100vw"},"sources":[]},"width":143,"height":143}}},"name":"Prof Geraint Wiggins ","url":"https://ai.vub.ac.be/team/geraint-wiggins/?utm_source=www.google.com&utm_medium=organic&utm_campaign=Google&referrer-analytics=1","acadposition":"Professor of Computational Creativity","blurb":"Computational Creativity, Artificial Intelligence, Music Cognition","themes":["mcog"],"role":"Academic Associate"},"id":"4234442f-d4f9-593d-9fd5-5a76d6d71124"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/563078348bcf9aea43c31c540bb26f8b/5aead/matthewpurver.png","srcSet":"/static/563078348bcf9aea43c31c540bb26f8b/e9fba/matthewpurver.png 50w,\n/static/563078348bcf9aea43c31c540bb26f8b/15e42/matthewpurver.png 100w,\n/static/563078348bcf9aea43c31c540bb26f8b/5aead/matthewpurver.png 200w","sizes":"(min-width: 200px) 200px, 100vw"},"sources":[{"srcSet":"/static/563078348bcf9aea43c31c540bb26f8b/dbc4a/matthewpurver.webp 50w,\n/static/563078348bcf9aea43c31c540bb26f8b/d8057/matthewpurver.webp 100w,\n/static/563078348bcf9aea43c31c540bb26f8b/2e34e/matthewpurver.webp 200w","type":"image/webp","sizes":"(min-width: 200px) 200px, 100vw"}]},"width":200,"height":200}}},"name":"Prof Matthew Purver","url":"http://www.eecs.qmul.ac.uk/~mpurver/","acadposition":"Professor of Computational Linguistics","blurb":"computational linguistics including models of language and music","themes":[],"role":"Academic Associate"},"id":"8f91717c-d7c5-5427-892b-0c25ab0e8f83"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/ad2545b7c27da104d546cde302d1834c/e07e1/pathealey.jpg","srcSet":"/static/ad2545b7c27da104d546cde302d1834c/74ef0/pathealey.jpg 25w,\n/static/ad2545b7c27da104d546cde302d1834c/6ac16/pathealey.jpg 50w,\n/static/ad2545b7c27da104d546cde302d1834c/e07e1/pathealey.jpg 100w","sizes":"(min-width: 100px) 100px, 100vw"},"sources":[{"srcSet":"/static/ad2545b7c27da104d546cde302d1834c/2fa99/pathealey.webp 25w,\n/static/ad2545b7c27da104d546cde302d1834c/dbc4a/pathealey.webp 50w,\n/static/ad2545b7c27da104d546cde302d1834c/d8057/pathealey.webp 100w","type":"image/webp","sizes":"(min-width: 100px) 100px, 100vw"}]},"width":100,"height":100}}},"name":"Prof Pat Healey","url":"http://www.eecs.qmul.ac.uk/%7Eph/","acadposition":"Professor of Human Interaction","blurb":"human interaction, human communication","themes":["isam"],"role":"Academic Associate"},"id":"4416d8c8-6a83-56a1-bf38-acb52ed928f8"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#a8b8b8","images":{"fallback":{"src":"/static/a8d172ed32be9ea5a59da2cfc572ebff/59de6/adamgarrow.jpg","srcSet":"/static/a8d172ed32be9ea5a59da2cfc572ebff/36466/adamgarrow.jpg 425w,\n/static/a8d172ed32be9ea5a59da2cfc572ebff/fa3c6/adamgarrow.jpg 850w,\n/static/a8d172ed32be9ea5a59da2cfc572ebff/59de6/adamgarrow.jpg 1700w","sizes":"(min-width: 1700px) 1700px, 100vw"},"sources":[{"srcSet":"/static/a8d172ed32be9ea5a59da2cfc572ebff/0c2e5/adamgarrow.webp 425w,\n/static/a8d172ed32be9ea5a59da2cfc572ebff/68d6e/adamgarrow.webp 850w,\n/static/a8d172ed32be9ea5a59da2cfc572ebff/fcd8f/adamgarrow.webp 1700w","type":"image/webp","sizes":"(min-width: 1700px) 1700px, 100vw"}]},"width":1700,"height":1700}}},"name":"Adam Andrew Garrow","url":"https://www.researchgate.net/profile/Adam-Garrow","acadposition":"PhD Student","blurb":"Probabilistic learning of sequential structures in music cognition","themes":["mcog"],"role":"PhD"},"id":"a912c176-1a88-5049-a749-4fb123746635"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/static/04f8214162f75e402b3fbd11a6ffc348/30cdc/adamhe.png","srcSet":"/static/04f8214162f75e402b3fbd11a6ffc348/7458e/adamhe.png 75w,\n/static/04f8214162f75e402b3fbd11a6ffc348/de3a1/adamhe.png 150w,\n/static/04f8214162f75e402b3fbd11a6ffc348/30cdc/adamhe.png 300w","sizes":"(min-width: 300px) 300px, 100vw"},"sources":[{"srcSet":"/static/04f8214162f75e402b3fbd11a6ffc348/18188/adamhe.webp 75w,\n/static/04f8214162f75e402b3fbd11a6ffc348/c65bc/adamhe.webp 150w,\n/static/04f8214162f75e402b3fbd11a6ffc348/078c3/adamhe.webp 300w","type":"image/webp","sizes":"(min-width: 300px) 300px, 100vw"}]},"width":300,"height":300}}},"name":"Adam He","url":null,"acadposition":"PhD Student","blurb":"Neuro-evolved Heuristics for Meta-composition","themes":[],"role":"PhD"},"id":"4c9aa88b-7c33-532b-8eb7-02647eab7c73"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8d8e8","images":{"fallback":{"src":"/static/a0048693a658a807cb05a19252bb723c/02dff/adityabhattacharjee.jpg","srcSet":"/static/a0048693a658a807cb05a19252bb723c/0fdf4/adityabhattacharjee.jpg 300w,\n/static/a0048693a658a807cb05a19252bb723c/a89ca/adityabhattacharjee.jpg 600w,\n/static/a0048693a658a807cb05a19252bb723c/02dff/adityabhattacharjee.jpg 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/a0048693a658a807cb05a19252bb723c/078c3/adityabhattacharjee.webp 300w,\n/static/a0048693a658a807cb05a19252bb723c/6d09e/adityabhattacharjee.webp 600w,\n/static/a0048693a658a807cb05a19252bb723c/83805/adityabhattacharjee.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"name":"Aditya Bhattacharjee","url":"https://www.linkedin.com/in/adibh/","acadposition":"PhD Student","blurb":"Self-supervision in Audio Fingerprinting","themes":["mir","mlist"],"role":"PhD"},"id":"226ac21f-bb0b-56df-9975-41dc5d56d6ca"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8a898","images":{"fallback":{"src":"/static/c5a8ea27a522980b54f492f142a1326d/9eee9/adanbenito.jpg","srcSet":"/static/c5a8ea27a522980b54f492f142a1326d/bc669/adanbenito.jpg 151w,\n/static/c5a8ea27a522980b54f492f142a1326d/577e8/adanbenito.jpg 302w,\n/static/c5a8ea27a522980b54f492f142a1326d/9eee9/adanbenito.jpg 603w","sizes":"(min-width: 603px) 603px, 100vw"},"sources":[{"srcSet":"/static/c5a8ea27a522980b54f492f142a1326d/43f11/adanbenito.webp 151w,\n/static/c5a8ea27a522980b54f492f142a1326d/5a5e0/adanbenito.webp 302w,\n/static/c5a8ea27a522980b54f492f142a1326d/9e027/adanbenito.webp 603w","type":"image/webp","sizes":"(min-width: 603px) 603px, 100vw"}]},"width":603,"height":603}}},"name":"Ad√°n Benito","url":"","acadposition":"PhD Student","blurb":"Beyond the fret: gesture analysis on fretted instruments and its applications to instrument augmentation","themes":["augmi"],"role":"PhD"},"id":"b7db6ff1-b6a2-5313-897a-a6b32abcdff9"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Alexander Williams","url":"","acadposition":"PhD Student","blurb":"User-driven deep music generation in digital audio workstations","themes":["mir","audioeng"],"role":"PhD"},"id":"cc381a30-c5d2-5ff7-b60d-1f10d81dbf9b"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/static/cce06b1e41eda976989e159ecb5db09c/81ef7/andreamartelloni.jpg","srcSet":"/static/cce06b1e41eda976989e159ecb5db09c/96deb/andreamartelloni.jpg 150w,\n/static/cce06b1e41eda976989e159ecb5db09c/9a5ea/andreamartelloni.jpg 299w,\n/static/cce06b1e41eda976989e159ecb5db09c/81ef7/andreamartelloni.jpg 598w","sizes":"(min-width: 598px) 598px, 100vw"},"sources":[{"srcSet":"/static/cce06b1e41eda976989e159ecb5db09c/c65bc/andreamartelloni.webp 150w,\n/static/cce06b1e41eda976989e159ecb5db09c/47ed3/andreamartelloni.webp 299w,\n/static/cce06b1e41eda976989e159ecb5db09c/8c892/andreamartelloni.webp 598w","type":"image/webp","sizes":"(min-width: 598px) 598px, 100vw"}]},"width":598,"height":598}}},"name":"Andrea Martelloni","url":"http://eecs.qmul.ac.uk/profiles/martelloniandrea-1.html","acadposition":"PhD Student","blurb":"Real-Time Gesture Classification on an Augmented Acoustic Guitar using Deep Learning to Improve Extended-Range and Percussive Solo Playing","themes":["mir"],"role":"PhD"},"id":"d35868ef-7cda-5e98-aa09-bb356da9e5d3"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#989888","images":{"fallback":{"src":"/static/09388c6cc31edb4824a849da2f74754c/30f07/andrewedwards.jpg","srcSet":"/static/09388c6cc31edb4824a849da2f74754c/41624/andrewedwards.jpg 160w,\n/static/09388c6cc31edb4824a849da2f74754c/1b894/andrewedwards.jpg 320w,\n/static/09388c6cc31edb4824a849da2f74754c/30f07/andrewedwards.jpg 640w","sizes":"(min-width: 640px) 640px, 100vw"},"sources":[{"srcSet":"/static/09388c6cc31edb4824a849da2f74754c/60b4d/andrewedwards.webp 160w,\n/static/09388c6cc31edb4824a849da2f74754c/5e011/andrewedwards.webp 320w,\n/static/09388c6cc31edb4824a849da2f74754c/90d07/andrewedwards.webp 640w","type":"image/webp","sizes":"(min-width: 640px) 640px, 100vw"}]},"width":640,"height":640}}},"name":"Andrew (Drew) Edwards","url":"http://eecs.qmul.ac.uk/profiles/edwardsandrewcharles.html","acadposition":"PhD Student","blurb":"Deep Learning for Jazz Piano: Transcription + Generative Modeling","themes":["mir","mlist","mcog"],"role":"PhD"},"id":"738dd58b-17cc-53bb-b319-0186faf690cc"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Antonella Torrisi","url":"https://www.researchgate.net/profile/Antonella-Torrisi","acadposition":"PhD Student","blurb":"Computational analysis of chick vocalisations: from categorisation to live feedback","themes":["mlist"],"role":"PhD"},"id":"58ba2962-4189-5258-a6b6-36025195cada"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#685848","images":{"fallback":{"src":"/static/c0f574ea604c6ff9084a488c4bc7e4e5/0be83/ashleynoelhirst.png","srcSet":"/static/c0f574ea604c6ff9084a488c4bc7e4e5/5f035/ashleynoelhirst.png 320w,\n/static/c0f574ea604c6ff9084a488c4bc7e4e5/eadd3/ashleynoelhirst.png 640w,\n/static/c0f574ea604c6ff9084a488c4bc7e4e5/0be83/ashleynoelhirst.png 1280w","sizes":"(min-width: 1280px) 1280px, 100vw"},"sources":[{"srcSet":"/static/c0f574ea604c6ff9084a488c4bc7e4e5/5e011/ashleynoelhirst.webp 320w,\n/static/c0f574ea604c6ff9084a488c4bc7e4e5/90d07/ashleynoelhirst.webp 640w,\n/static/c0f574ea604c6ff9084a488c4bc7e4e5/9e21f/ashleynoelhirst.webp 1280w","type":"image/webp","sizes":"(min-width: 1280px) 1280px, 100vw"}]},"width":1280,"height":1280}}},"name":"Ashley Noel-Hirst","url":"https://ashleynoelhirst.co.uk/","acadposition":"PhD Student","blurb":"Latent Spaces for Human-AI music generation","themes":["mir","isam"],"role":"PhD"},"id":"9a6273ce-dc95-5d23-b5f9-7bcebfdc1772"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/static/122d04ac0e0cbe928b1e890f48016415/2e67f/benjaminhayes.jpg","srcSet":"/static/122d04ac0e0cbe928b1e890f48016415/2b6e2/benjaminhayes.jpg 73w,\n/static/122d04ac0e0cbe928b1e890f48016415/ca0c2/benjaminhayes.jpg 147w,\n/static/122d04ac0e0cbe928b1e890f48016415/2e67f/benjaminhayes.jpg 293w","sizes":"(min-width: 293px) 293px, 100vw"},"sources":[{"srcSet":"/static/122d04ac0e0cbe928b1e890f48016415/96ea5/benjaminhayes.webp 73w,\n/static/122d04ac0e0cbe928b1e890f48016415/93d5d/benjaminhayes.webp 147w,\n/static/122d04ac0e0cbe928b1e890f48016415/4cb57/benjaminhayes.webp 293w","type":"image/webp","sizes":"(min-width: 293px) 293px, 100vw"}]},"width":293,"height":293}}},"name":"Benjamin Hayes","url":"http://eecs.qmul.ac.uk/profiles/hayesbenjaminjames.html","acadposition":"PhD Student","blurb":"Perceptually motivated deep learning approaches to creative sound synthesis","themes":["soundsynthesis","mcog"],"role":"PhD"},"id":"7920f116-7143-57a4-b40b-9325b220e835"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#282828","images":{"fallback":{"src":"/static/92ff73f501424e00ed9ec016481170c9/7706b/berkerbanar.jpg","srcSet":"/static/92ff73f501424e00ed9ec016481170c9/96deb/berkerbanar.jpg 150w,\n/static/92ff73f501424e00ed9ec016481170c9/ed539/berkerbanar.jpg 301w,\n/static/92ff73f501424e00ed9ec016481170c9/7706b/berkerbanar.jpg 601w","sizes":"(min-width: 601px) 601px, 100vw"},"sources":[{"srcSet":"/static/92ff73f501424e00ed9ec016481170c9/c65bc/berkerbanar.webp 150w,\n/static/92ff73f501424e00ed9ec016481170c9/214a8/berkerbanar.webp 301w,\n/static/92ff73f501424e00ed9ec016481170c9/2b014/berkerbanar.webp 601w","type":"image/webp","sizes":"(min-width: 601px) 601px, 100vw"}]},"width":601,"height":601}}},"name":"Berker Banar","url":"http://eecs.qmul.ac.uk/profiles/banarberker.html","acadposition":"PhD Student","blurb":"Towards Composing Contemporary Classical Music using Generative Deep Learning","themes":["mir","soundsynthesis"],"role":"PhD"},"id":"3e2bbfc8-0f9b-5aac-ba55-b0f8ffc899b9"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Bleiz Del Sette","url":"https://comma.eecs.qmul.ac.uk/people/bleiz/","acadposition":"PhD Student","blurb":"The Sound of Care: researching the use of Deep Learning and Sonification for the daily support of people with Chronic Primary Pain","themes":["comma"],"role":"PhD"},"id":"3aef98b5-03b4-5cbc-8469-3c762163024d"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8b8a8","images":{"fallback":{"src":"/static/6309876a725bc3c552d55ad64dcd18fa/0fdf4/bradleyaldous.jpg","srcSet":"/static/6309876a725bc3c552d55ad64dcd18fa/91a6d/bradleyaldous.jpg 75w,\n/static/6309876a725bc3c552d55ad64dcd18fa/96deb/bradleyaldous.jpg 150w,\n/static/6309876a725bc3c552d55ad64dcd18fa/0fdf4/bradleyaldous.jpg 300w","sizes":"(min-width: 300px) 300px, 100vw"},"sources":[{"srcSet":"/static/6309876a725bc3c552d55ad64dcd18fa/18188/bradleyaldous.webp 75w,\n/static/6309876a725bc3c552d55ad64dcd18fa/c65bc/bradleyaldous.webp 150w,\n/static/6309876a725bc3c552d55ad64dcd18fa/078c3/bradleyaldous.webp 300w","type":"image/webp","sizes":"(min-width: 300px) 300px, 100vw"}]},"width":300,"height":300}}},"name":"Bradley Aldous","url":null,"acadposition":"PhD Student","blurb":"Advancing music generation via accelerated deep learning","themes":[],"role":"PhD"},"id":"dbde6f37-d874-5e6f-bddb-dec87f68de9f"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#a88878","images":{"fallback":{"src":"/static/e2494d25fff51e99ef73d11bf688ea88/be4dc/careybunks.png","srcSet":"/static/e2494d25fff51e99ef73d11bf688ea88/d8938/careybunks.png 159w,\n/static/e2494d25fff51e99ef73d11bf688ea88/2655f/careybunks.png 318w,\n/static/e2494d25fff51e99ef73d11bf688ea88/be4dc/careybunks.png 636w","sizes":"(min-width: 636px) 636px, 100vw"},"sources":[{"srcSet":"/static/e2494d25fff51e99ef73d11bf688ea88/0993c/careybunks.webp 159w,\n/static/e2494d25fff51e99ef73d11bf688ea88/4ce64/careybunks.webp 318w,\n/static/e2494d25fff51e99ef73d11bf688ea88/1852a/careybunks.webp 636w","type":"image/webp","sizes":"(min-width: 636px) 636px, 100vw"}]},"width":636,"height":636}}},"name":"Carey Bunks","url":"","acadposition":"PhD Student","blurb":"Cover Song Identification","themes":["mir"],"role":"PhD"},"id":"4f0a9d6f-a76c-5668-832e-de53c816bfd5"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#e8e8e8","images":{"fallback":{"src":"/static/4d90e16ad97b57932e1d33039b1d87cc/6d0f2/carlosdelavegamartin.jpg","srcSet":"/static/4d90e16ad97b57932e1d33039b1d87cc/f7afd/carlosdelavegamartin.jpg 496w,\n/static/4d90e16ad97b57932e1d33039b1d87cc/09746/carlosdelavegamartin.jpg 991w,\n/static/4d90e16ad97b57932e1d33039b1d87cc/6d0f2/carlosdelavegamartin.jpg 1982w","sizes":"(min-width: 1982px) 1982px, 100vw"},"sources":[{"srcSet":"/static/4d90e16ad97b57932e1d33039b1d87cc/45827/carlosdelavegamartin.webp 496w,\n/static/4d90e16ad97b57932e1d33039b1d87cc/81db5/carlosdelavegamartin.webp 991w,\n/static/4d90e16ad97b57932e1d33039b1d87cc/94e91/carlosdelavegamartin.webp 1982w","type":"image/webp","sizes":"(min-width: 1982px) 1982px, 100vw"}]},"width":1982,"height":1982}}},"name":"Carlos De La Vega Martin","url":"https://www.qmul.ac.uk/eecs/people/profiles/delavegamartincarlos.html","acadposition":"PhD Student","blurb":"Neural Drum Synthesis","themes":["soundsynthesis"],"role":"PhD"},"id":"000ae1c5-cd99-587f-b370-d863b89be317"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/83b515844814d95b7fc7d641a74fed63/9cf6c/chinyunyu.jpg","srcSet":"/static/83b515844814d95b7fc7d641a74fed63/2f83b/chinyunyu.jpg 351w,\n/static/83b515844814d95b7fc7d641a74fed63/b41ee/chinyunyu.jpg 702w,\n/static/83b515844814d95b7fc7d641a74fed63/9cf6c/chinyunyu.jpg 1403w","sizes":"(min-width: 1403px) 1403px, 100vw"},"sources":[{"srcSet":"/static/83b515844814d95b7fc7d641a74fed63/9bbac/chinyunyu.webp 351w,\n/static/83b515844814d95b7fc7d641a74fed63/ce9fc/chinyunyu.webp 702w,\n/static/83b515844814d95b7fc7d641a74fed63/f5914/chinyunyu.webp 1403w","type":"image/webp","sizes":"(min-width: 1403px) 1403px, 100vw"}]},"width":1403,"height":1403}}},"name":"Chin-Yun Yu","url":"https://yoyololicon.github.io/","acadposition":"PhD Student","blurb":"Neural Audio Synthesis with Expressiveness Control","themes":["audioeng","mir","soundsynthesis"],"role":"PhD"},"id":"34ef929b-de59-5c95-a554-597f98d80226"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#383838","images":{"fallback":{"src":"/static/ce96cd91ca642836e98f01277e51b31b/d846c/chriswinnard.jpg","srcSet":"/static/ce96cd91ca642836e98f01277e51b31b/db2b9/chriswinnard.jpg 135w,\n/static/ce96cd91ca642836e98f01277e51b31b/da9dc/chriswinnard.jpg 271w,\n/static/ce96cd91ca642836e98f01277e51b31b/d846c/chriswinnard.jpg 541w","sizes":"(min-width: 541px) 541px, 100vw"},"sources":[{"srcSet":"/static/ce96cd91ca642836e98f01277e51b31b/e01df/chriswinnard.webp 135w,\n/static/ce96cd91ca642836e98f01277e51b31b/723c9/chriswinnard.webp 271w,\n/static/ce96cd91ca642836e98f01277e51b31b/68c9a/chriswinnard.webp 541w","type":"image/webp","sizes":"(min-width: 541px) 541px, 100vw"}]},"width":541,"height":541}}},"name":"Chris Winnard","url":"http://eecs.qmul.ac.uk/profiles/winnardchristopherjames.html","acadposition":"PhD Student","blurb":"Music Interestingness in the Brain","themes":["mcog"],"role":"PhD"},"id":"130ff14f-3659-5f6c-b154-4bacc73ae9de"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/144c0a0c0f7932d557eb785c18a2c6ce/3c367/christiansteinmetz.jpg","srcSet":"/static/144c0a0c0f7932d557eb785c18a2c6ce/19e71/christiansteinmetz.jpg 128w,\n/static/144c0a0c0f7932d557eb785c18a2c6ce/68974/christiansteinmetz.jpg 256w,\n/static/144c0a0c0f7932d557eb785c18a2c6ce/3c367/christiansteinmetz.jpg 512w","sizes":"(min-width: 512px) 512px, 100vw"},"sources":[{"srcSet":"/static/144c0a0c0f7932d557eb785c18a2c6ce/6766a/christiansteinmetz.webp 128w,\n/static/144c0a0c0f7932d557eb785c18a2c6ce/22bfc/christiansteinmetz.webp 256w,\n/static/144c0a0c0f7932d557eb785c18a2c6ce/d689f/christiansteinmetz.webp 512w","type":"image/webp","sizes":"(min-width: 512px) 512px, 100vw"}]},"width":512,"height":512}}},"name":"Christian Steinmetz","url":"https://www.christiansteinmetz.com/","acadposition":"PhD Student","blurb":"End-to-end generative modeling of multitrack mixing with non-parallel data and adversarial networks","themes":["audioeng"],"role":"PhD"},"id":"4dc93e11-7859-5d54-80fd-7f2c652e4c3a"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#d8d8d8","images":{"fallback":{"src":"/static/48f61b38abf690cd1ce8981afef7140c/5c8f1/christophermitcheltree.jpg","srcSet":"/static/48f61b38abf690cd1ce8981afef7140c/be5ed/christophermitcheltree.jpg 500w,\n/static/48f61b38abf690cd1ce8981afef7140c/5a7c3/christophermitcheltree.jpg 1000w,\n/static/48f61b38abf690cd1ce8981afef7140c/5c8f1/christophermitcheltree.jpg 2000w","sizes":"(min-width: 2000px) 2000px, 100vw"},"sources":[{"srcSet":"/static/48f61b38abf690cd1ce8981afef7140c/5f169/christophermitcheltree.webp 500w,\n/static/48f61b38abf690cd1ce8981afef7140c/3cd29/christophermitcheltree.webp 1000w,\n/static/48f61b38abf690cd1ce8981afef7140c/62c39/christophermitcheltree.webp 2000w","type":"image/webp","sizes":"(min-width: 2000px) 2000px, 100vw"}]},"width":2000,"height":2000}}},"name":"Christopher Mitcheltree","url":"https://christhetr.ee","acadposition":"PhD Student","blurb":"Representation Learning for Audio Production Style and Modulations","themes":["mlist","audioeng","mir"],"role":"PhD"},"id":"c210c41b-f202-57d7-ab0c-14538a1c59bc"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#d8d8c8","images":{"fallback":{"src":"/static/fb343d9e48ae480abae8278412139e50/30cdc/christosplachouras.png","srcSet":"/static/fb343d9e48ae480abae8278412139e50/7458e/christosplachouras.png 75w,\n/static/fb343d9e48ae480abae8278412139e50/de3a1/christosplachouras.png 150w,\n/static/fb343d9e48ae480abae8278412139e50/30cdc/christosplachouras.png 300w","sizes":"(min-width: 300px) 300px, 100vw"},"sources":[{"srcSet":"/static/fb343d9e48ae480abae8278412139e50/18188/christosplachouras.webp 75w,\n/static/fb343d9e48ae480abae8278412139e50/c65bc/christosplachouras.webp 150w,\n/static/fb343d9e48ae480abae8278412139e50/078c3/christosplachouras.webp 300w","type":"image/webp","sizes":"(min-width: 300px) 300px, 100vw"}]},"width":300,"height":300}}},"name":"Christos Plachouras","url":null,"acadposition":"PhD Student","blurb":"Deep learning for low-resource music","themes":[],"role":"PhD"},"id":"6ae134f9-e2f8-5b4f-8927-caea3104a339"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#988878","images":{"fallback":{"src":"/static/81f0d5ca330795a7366c18a6c1d1a10a/47faa/cyrusvahidi.jpg","srcSet":"/static/81f0d5ca330795a7366c18a6c1d1a10a/fcf11/cyrusvahidi.jpg 306w,\n/static/81f0d5ca330795a7366c18a6c1d1a10a/04fa2/cyrusvahidi.jpg 612w,\n/static/81f0d5ca330795a7366c18a6c1d1a10a/47faa/cyrusvahidi.jpg 1224w","sizes":"(min-width: 1224px) 1224px, 100vw"},"sources":[{"srcSet":"/static/81f0d5ca330795a7366c18a6c1d1a10a/c51a2/cyrusvahidi.webp 306w,\n/static/81f0d5ca330795a7366c18a6c1d1a10a/c089a/cyrusvahidi.webp 612w,\n/static/81f0d5ca330795a7366c18a6c1d1a10a/23828/cyrusvahidi.webp 1224w","type":"image/webp","sizes":"(min-width: 1224px) 1224px, 100vw"}]},"width":1224,"height":1224}}},"name":"Cyrus Vahidi","url":"http://eecs.qmul.ac.uk/profiles/vahidicyrus.html","acadposition":"PhD Student","blurb":"Perceptual end to end learning for music understanding","themes":["mir"],"role":"PhD"},"id":"1e774e43-fee0-57f5-b2b5-632fe8762c2a"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8e8c8","images":{"fallback":{"src":"/static/c7d60cc8efa14a13b6dbba4bc7ccc137/a20e0/davefoster.jpg","srcSet":"/static/c7d60cc8efa14a13b6dbba4bc7ccc137/71c2b/davefoster.jpg 52w,\n/static/c7d60cc8efa14a13b6dbba4bc7ccc137/ac761/davefoster.jpg 104w,\n/static/c7d60cc8efa14a13b6dbba4bc7ccc137/a20e0/davefoster.jpg 208w","sizes":"(min-width: 208px) 208px, 100vw"},"sources":[{"srcSet":"/static/c7d60cc8efa14a13b6dbba4bc7ccc137/284ac/davefoster.webp 52w,\n/static/c7d60cc8efa14a13b6dbba4bc7ccc137/5ca4c/davefoster.webp 104w,\n/static/c7d60cc8efa14a13b6dbba4bc7ccc137/c95dc/davefoster.webp 208w","type":"image/webp","sizes":"(min-width: 208px) 208px, 100vw"}]},"width":208,"height":208}}},"name":"David Foster","url":"http://eecs.qmul.ac.uk/profiles/fosterdavid.html","acadposition":"PhD Student","blurb":"Modelling the Creative Process of Jazz Improvisation","themes":["mir"],"role":"PhD"},"id":"f855554f-b1f4-56be-8d59-801f8b9664fe"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#987878","images":{"fallback":{"src":"/static/1811de1c418db6a7f0c4a8727b254b42/6fd26/davidsudholt.png","srcSet":"/static/1811de1c418db6a7f0c4a8727b254b42/b5a32/davidsudholt.png 78w,\n/static/1811de1c418db6a7f0c4a8727b254b42/d8a72/davidsudholt.png 156w,\n/static/1811de1c418db6a7f0c4a8727b254b42/6fd26/davidsudholt.png 312w","sizes":"(min-width: 312px) 312px, 100vw"},"sources":[{"srcSet":"/static/1811de1c418db6a7f0c4a8727b254b42/7a63e/davidsudholt.webp 78w,\n/static/1811de1c418db6a7f0c4a8727b254b42/d1e3d/davidsudholt.webp 156w,\n/static/1811de1c418db6a7f0c4a8727b254b42/da295/davidsudholt.webp 312w","type":"image/webp","sizes":"(min-width: 312px) 312px, 100vw"}]},"width":312,"height":312}}},"name":"David S√ºdholt","url":"https://dsuedholt.github.io/","acadposition":"PhD Student","blurb":"Machine Learning of Physical Models for Voice Synthesis","themes":["soundsynthesis","audioeng"],"role":"PhD"},"id":"612a6483-8966-5db8-9744-f6fd1021a11a"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#8898b8","images":{"fallback":{"src":"/static/0c6f3e9bc7e76cfcf929beb58b989a9c/0fdf4/eceyurdakul.jpg","srcSet":"/static/0c6f3e9bc7e76cfcf929beb58b989a9c/91a6d/eceyurdakul.jpg 75w,\n/static/0c6f3e9bc7e76cfcf929beb58b989a9c/96deb/eceyurdakul.jpg 150w,\n/static/0c6f3e9bc7e76cfcf929beb58b989a9c/0fdf4/eceyurdakul.jpg 300w","sizes":"(min-width: 300px) 300px, 100vw"},"sources":[{"srcSet":"/static/0c6f3e9bc7e76cfcf929beb58b989a9c/18188/eceyurdakul.webp 75w,\n/static/0c6f3e9bc7e76cfcf929beb58b989a9c/c65bc/eceyurdakul.webp 150w,\n/static/0c6f3e9bc7e76cfcf929beb58b989a9c/078c3/eceyurdakul.webp 300w","type":"image/webp","sizes":"(min-width: 300px) 300px, 100vw"}]},"width":300,"height":300}}},"name":"Ece Yurdakul","url":null,"acadposition":"PhD Student","blurb":"Emotion-based Personalised Music Recommendation","themes":[],"role":"PhD"},"id":"0878de00-812b-5119-953d-9c39b08d715e"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#081808","images":{"fallback":{"src":"/static/38f5515ab7b3aa20eeef7b22fd3a5581/0e51d/eleanorrow.jpg","srcSet":"/static/38f5515ab7b3aa20eeef7b22fd3a5581/2c33f/eleanorrow.jpg 56w,\n/static/38f5515ab7b3aa20eeef7b22fd3a5581/f41fb/eleanorrow.jpg 112w,\n/static/38f5515ab7b3aa20eeef7b22fd3a5581/0e51d/eleanorrow.jpg 224w","sizes":"(min-width: 224px) 224px, 100vw"},"sources":[{"srcSet":"/static/38f5515ab7b3aa20eeef7b22fd3a5581/f8744/eleanorrow.webp 56w,\n/static/38f5515ab7b3aa20eeef7b22fd3a5581/65bf6/eleanorrow.webp 112w,\n/static/38f5515ab7b3aa20eeef7b22fd3a5581/f42a0/eleanorrow.webp 224w","type":"image/webp","sizes":"(min-width: 224px) 224px, 100vw"}]},"width":224,"height":224}}},"name":"Eleanor Row","url":"http://eecs.qmul.ac.uk/profiles/roweleanorroxannevictoria.html","acadposition":"PhD Student","blurb":"Automatic micro-composition for professional/novice composers using generative models as creativity support tools","themes":["soundsynthesis"],"role":"PhD"},"id":"2b006670-df63-567c-9c51-62d19c6b1372"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/87ab058ab4e8a08974fd4a9a22ef1bac/36a05/elonashatri.jpg","srcSet":"/static/87ab058ab4e8a08974fd4a9a22ef1bac/a46b0/elonashatri.jpg 205w,\n/static/87ab058ab4e8a08974fd4a9a22ef1bac/3e7a6/elonashatri.jpg 410w,\n/static/87ab058ab4e8a08974fd4a9a22ef1bac/36a05/elonashatri.jpg 819w","sizes":"(min-width: 819px) 819px, 100vw"},"sources":[{"srcSet":"/static/87ab058ab4e8a08974fd4a9a22ef1bac/24bbe/elonashatri.webp 205w,\n/static/87ab058ab4e8a08974fd4a9a22ef1bac/d883f/elonashatri.webp 410w,\n/static/87ab058ab4e8a08974fd4a9a22ef1bac/8f6df/elonashatri.webp 819w","type":"image/webp","sizes":"(min-width: 819px) 819px, 100vw"}]},"width":819,"height":819}}},"name":"Elona Shatri","url":"http://eecs.qmul.ac.uk/profiles/shatrielona-1.html","acadposition":"PhD Student","blurb":"Optical music recognition using deep learning","themes":["mir"],"role":"PhD"},"id":"7b043f0b-de2f-5155-9877-1dfff2b8c117"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#98c8f8","images":{"fallback":{"src":"/static/d82c484701588fef8165b84cef4ce782/1a361/faridayusuf.jpg","srcSet":"/static/d82c484701588fef8165b84cef4ce782/2c33f/faridayusuf.jpg 56w,\n/static/d82c484701588fef8165b84cef4ce782/fa873/faridayusuf.jpg 113w,\n/static/d82c484701588fef8165b84cef4ce782/1a361/faridayusuf.jpg 225w","sizes":"(min-width: 225px) 225px, 100vw"},"sources":[{"srcSet":"/static/d82c484701588fef8165b84cef4ce782/f8744/faridayusuf.webp 56w,\n/static/d82c484701588fef8165b84cef4ce782/26b1c/faridayusuf.webp 113w,\n/static/d82c484701588fef8165b84cef4ce782/252a0/faridayusuf.webp 225w","type":"image/webp","sizes":"(min-width: 225px) 225px, 100vw"}]},"width":225,"height":225}}},"name":"Farida Yusuf","url":null,"acadposition":"PhD Student","blurb":"Information-theoretic neural networks for online perception of auditory objects","themes":[],"role":"PhD"},"id":"16a7b4ba-2910-57d2-a5e9-3798d576ec2a"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#e8d8c8","images":{"fallback":{"src":"/static/45ee009513755f6c0be98eb6348c483d/8ef8d/francocaspe.jpg","srcSet":"/static/45ee009513755f6c0be98eb6348c483d/78372/francocaspe.jpg 242w,\n/static/45ee009513755f6c0be98eb6348c483d/2b82c/francocaspe.jpg 484w,\n/static/45ee009513755f6c0be98eb6348c483d/8ef8d/francocaspe.jpg 967w","sizes":"(min-width: 967px) 967px, 100vw"},"sources":[{"srcSet":"/static/45ee009513755f6c0be98eb6348c483d/ed101/francocaspe.webp 242w,\n/static/45ee009513755f6c0be98eb6348c483d/6e7b2/francocaspe.webp 484w,\n/static/45ee009513755f6c0be98eb6348c483d/c9c99/francocaspe.webp 967w","type":"image/webp","sizes":"(min-width: 967px) 967px, 100vw"}]},"width":967,"height":967}}},"name":"Franco Caspe","url":"https://www.qmul.ac.uk/eecs/people/profiles/caspefrancosantiago.html","acadposition":"PhD Student","blurb":"AI-assisted FM synthesis for sound design and control mapping","themes":[],"role":"PhD"},"id":"22e905bc-01d4-54c1-a0a9-c09bab9153e8"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Gary Bromham","url":"","acadposition":"PhD Student","blurb":"The role of nostalga in music production","themes":["mir","audioeng"],"role":"PhD"},"id":"1b6798fe-1d19-52a2-add1-70afa99577f6"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#e8c8c8","images":{"fallback":{"src":"/static/f810ca258787f64bbdd3da6cdcc2737d/1a361/gregormeehan.jpg","srcSet":"/static/f810ca258787f64bbdd3da6cdcc2737d/2c33f/gregormeehan.jpg 56w,\n/static/f810ca258787f64bbdd3da6cdcc2737d/fa873/gregormeehan.jpg 113w,\n/static/f810ca258787f64bbdd3da6cdcc2737d/1a361/gregormeehan.jpg 225w","sizes":"(min-width: 225px) 225px, 100vw"},"sources":[{"srcSet":"/static/f810ca258787f64bbdd3da6cdcc2737d/f8744/gregormeehan.webp 56w,\n/static/f810ca258787f64bbdd3da6cdcc2737d/26b1c/gregormeehan.webp 113w,\n/static/f810ca258787f64bbdd3da6cdcc2737d/252a0/gregormeehan.webp 225w","type":"image/webp","sizes":"(min-width: 225px) 225px, 100vw"}]},"width":225,"height":225}}},"name":"Gregor Meehan","url":null,"acadposition":"PhD Student","blurb":"Representation learning for musical audio using graph neural network-based recommender engines","themes":[],"role":"PhD"},"id":"7f9d1916-08ff-5b25-bd61-7eb01dcaff25"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/1f20a3627b1d6bd6a95ea727c59e9a75/96deb/haokuntian.jpg","srcSet":"/static/1f20a3627b1d6bd6a95ea727c59e9a75/c81d1/haokuntian.jpg 38w,\n/static/1f20a3627b1d6bd6a95ea727c59e9a75/91a6d/haokuntian.jpg 75w,\n/static/1f20a3627b1d6bd6a95ea727c59e9a75/96deb/haokuntian.jpg 150w","sizes":"(min-width: 150px) 150px, 100vw"},"sources":[{"srcSet":"/static/1f20a3627b1d6bd6a95ea727c59e9a75/0852d/haokuntian.webp 38w,\n/static/1f20a3627b1d6bd6a95ea727c59e9a75/18188/haokuntian.webp 75w,\n/static/1f20a3627b1d6bd6a95ea727c59e9a75/c65bc/haokuntian.webp 150w","type":"image/webp","sizes":"(min-width: 150px) 150px, 100vw"}]},"width":150,"height":150}}},"name":"Haokun Tian","url":null,"acadposition":"PhD Student","blurb":"Timbre Tools for the Digital Instrument Maker","themes":[],"role":"PhD"},"id":"697ee3f9-11f4-5b61-a144-e15d313113b4"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/78d403f536869a8d3f69509587dc97e9/192dd/harnickhera.png","srcSet":"/static/78d403f536869a8d3f69509587dc97e9/b024c/harnickhera.png 65w,\n/static/78d403f536869a8d3f69509587dc97e9/68010/harnickhera.png 129w,\n/static/78d403f536869a8d3f69509587dc97e9/192dd/harnickhera.png 258w","sizes":"(min-width: 258px) 258px, 100vw"},"sources":[{"srcSet":"/static/78d403f536869a8d3f69509587dc97e9/0c531/harnickhera.webp 65w,\n/static/78d403f536869a8d3f69509587dc97e9/f6a08/harnickhera.webp 129w,\n/static/78d403f536869a8d3f69509587dc97e9/c7cc5/harnickhera.webp 258w","type":"image/webp","sizes":"(min-width: 258px) 258px, 100vw"}]},"width":258,"height":258}}},"name":"Harnick Khera","url":"http://eecs.qmul.ac.uk/profiles/kheraharnicksingh.html","acadposition":"PhD Student","blurb":"Informed source separation for multi-mic production","themes":["mir","mlist"],"role":"PhD"},"id":"b71f6653-2c4e-5f2b-b283-2eb0ae2679b6"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#88b8f8","images":{"fallback":{"src":"/static/56fdf9525659e33f14b4049213e7f4a2/70617/huanzhang.jpg","srcSet":"/static/56fdf9525659e33f14b4049213e7f4a2/f2f05/huanzhang.jpg 146w,\n/static/56fdf9525659e33f14b4049213e7f4a2/b179d/huanzhang.jpg 291w,\n/static/56fdf9525659e33f14b4049213e7f4a2/70617/huanzhang.jpg 582w","sizes":"(min-width: 582px) 582px, 100vw"},"sources":[{"srcSet":"/static/56fdf9525659e33f14b4049213e7f4a2/9027a/huanzhang.webp 146w,\n/static/56fdf9525659e33f14b4049213e7f4a2/46435/huanzhang.webp 291w,\n/static/56fdf9525659e33f14b4049213e7f4a2/f27b2/huanzhang.webp 582w","type":"image/webp","sizes":"(min-width: 582px) 582px, 100vw"}]},"width":582,"height":582}}},"name":"Huan Zhang","url":"http://eecs.qmul.ac.uk/people/profiles/zhanghuan.html","acadposition":"PhD Student","blurb":"Computational Modelling of Expressive Piano Performance","themes":["mir"],"role":"PhD"},"id":"1918386c-730c-5e3e-9d42-0369c7ed79b1"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Iacopo Ghinassi","url":"https://github.com/Ighina","acadposition":"PhD Student","blurb":"Semantic understanding of TV programme content and structure to enable automatic enhancement and adjustment","themes":["mir"],"role":"PhD"},"id":"27c0231d-35a2-57d5-8273-8562fba3319f"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#d88868","images":{"fallback":{"src":"/static/5ebddded3dd854a7fb181cab4c068dec/97a19/ilariamanco.jpg","srcSet":"/static/5ebddded3dd854a7fb181cab4c068dec/73bb6/ilariamanco.jpg 120w,\n/static/5ebddded3dd854a7fb181cab4c068dec/f9edd/ilariamanco.jpg 240w,\n/static/5ebddded3dd854a7fb181cab4c068dec/97a19/ilariamanco.jpg 480w","sizes":"(min-width: 480px) 480px, 100vw"},"sources":[{"srcSet":"/static/5ebddded3dd854a7fb181cab4c068dec/507b0/ilariamanco.webp 120w,\n/static/5ebddded3dd854a7fb181cab4c068dec/8d565/ilariamanco.webp 240w,\n/static/5ebddded3dd854a7fb181cab4c068dec/21b1a/ilariamanco.webp 480w","type":"image/webp","sizes":"(min-width: 480px) 480px, 100vw"}]},"width":480,"height":480}}},"name":"Ilaria Manco","url":"http://eecs.qmul.ac.uk/profiles/mancoilaria.html","acadposition":"PhD Student","blurb":"Multimodal Deep Learning for Music Information Retrieval","themes":["mir","mlist"],"role":"PhD"},"id":"8f618d2e-72c3-5ff1-a65f-f14410f7ea3e"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#e8e8d8","images":{"fallback":{"src":"/static/b57035b0802024ae7531105179b51594/3fd2e/jacksonloth.jpg","srcSet":"/static/b57035b0802024ae7531105179b51594/4a00f/jacksonloth.jpg 756w,\n/static/b57035b0802024ae7531105179b51594/facf2/jacksonloth.jpg 1512w,\n/static/b57035b0802024ae7531105179b51594/3fd2e/jacksonloth.jpg 3024w","sizes":"(min-width: 3024px) 3024px, 100vw"},"sources":[{"srcSet":"/static/b57035b0802024ae7531105179b51594/0531b/jacksonloth.webp 756w,\n/static/b57035b0802024ae7531105179b51594/dd848/jacksonloth.webp 1512w,\n/static/b57035b0802024ae7531105179b51594/95aac/jacksonloth.webp 3024w","type":"image/webp","sizes":"(min-width: 3024px) 3024px, 100vw"}]},"width":3024,"height":3024}}},"name":"Jackson Loth","url":"https://www.qmul.ac.uk/eecs/people/profiles/lothjacksonjames.html","acadposition":"PhD Student","blurb":"Time to vibe together: cloud-based guitar and intelligent agent","themes":["augmi","soundsynthesis"],"role":"PhD"},"id":"961a0884-4935-5955-bb6a-176115804b95"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#a89888","images":{"fallback":{"src":"/static/6dd5cb05357d20e63e726634c9ff6079/035db/jamesbolt.jpg","srcSet":"/static/6dd5cb05357d20e63e726634c9ff6079/0f5ce/jamesbolt.jpg 750w,\n/static/6dd5cb05357d20e63e726634c9ff6079/1f55a/jamesbolt.jpg 1500w,\n/static/6dd5cb05357d20e63e726634c9ff6079/035db/jamesbolt.jpg 3000w","sizes":"(min-width: 3000px) 3000px, 100vw"},"sources":[{"srcSet":"/static/6dd5cb05357d20e63e726634c9ff6079/4f03f/jamesbolt.webp 750w,\n/static/6dd5cb05357d20e63e726634c9ff6079/07142/jamesbolt.webp 1500w,\n/static/6dd5cb05357d20e63e726634c9ff6079/19b60/jamesbolt.webp 3000w","type":"image/webp","sizes":"(min-width: 3000px) 3000px, 100vw"}]},"width":3000,"height":3000}}},"name":"James Bolt","url":"https://www.qmul.ac.uk/eecs/people/profiles/jamesbolt.html","acadposition":"PhD Student","blurb":"Intelligent audio and music editing with deep learning","themes":["mir"],"role":"PhD"},"id":"623d083c-bec9-58ac-b75f-deed805850e4"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/c0e05c5130551ba1ee404aae041b587e/4538b/jiawenhuang.jpg","srcSet":"/static/c0e05c5130551ba1ee404aae041b587e/d4e31/jiawenhuang.jpg 203w,\n/static/c0e05c5130551ba1ee404aae041b587e/3175f/jiawenhuang.jpg 406w,\n/static/c0e05c5130551ba1ee404aae041b587e/4538b/jiawenhuang.jpg 812w","sizes":"(min-width: 812px) 812px, 100vw"},"sources":[{"srcSet":"/static/c0e05c5130551ba1ee404aae041b587e/6f337/jiawenhuang.webp 203w,\n/static/c0e05c5130551ba1ee404aae041b587e/3a205/jiawenhuang.webp 406w,\n/static/c0e05c5130551ba1ee404aae041b587e/077a8/jiawenhuang.webp 812w","type":"image/webp","sizes":"(min-width: 812px) 812px, 100vw"}]},"width":812,"height":812}}},"name":"Jiawen Huang","url":"https://www.qmul.ac.uk/eecs/people/profiles/huangjiawen.html","acadposition":"PhD Student","blurb":"Lyrics Alignment For Polyphonic Music","themes":["mir, mlist"],"role":"PhD"},"id":"18cc82ce-28b5-5546-8b4d-4de704a7034f"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#282828","images":{"fallback":{"src":"/static/82ebdfea2d843cadbefef83efbcb21ca/a20e0/jingjingtang.jpg","srcSet":"/static/82ebdfea2d843cadbefef83efbcb21ca/71c2b/jingjingtang.jpg 52w,\n/static/82ebdfea2d843cadbefef83efbcb21ca/ac761/jingjingtang.jpg 104w,\n/static/82ebdfea2d843cadbefef83efbcb21ca/a20e0/jingjingtang.jpg 208w","sizes":"(min-width: 208px) 208px, 100vw"},"sources":[{"srcSet":"/static/82ebdfea2d843cadbefef83efbcb21ca/284ac/jingjingtang.webp 52w,\n/static/82ebdfea2d843cadbefef83efbcb21ca/5ca4c/jingjingtang.webp 104w,\n/static/82ebdfea2d843cadbefef83efbcb21ca/c95dc/jingjingtang.webp 208w","type":"image/webp","sizes":"(min-width: 208px) 208px, 100vw"}]},"width":208,"height":208}}},"name":"Jingjing Tang","url":"https://www.qmul.ac.uk/eecs/people/profiles/tangjingjing.html","acadposition":"PhD Student","blurb":"End-to-End System Design for Music Style Transfer with Neural Networks","themes":["mcog","audioeng"],"role":"PhD"},"id":"43e74b5f-21cb-5aa3-8925-294ae05510e0"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Jinhua Liang","url":"https://jinhualiang.github.io/","acadposition":"PhD Student","blurb":"AI for everyday sounds","themes":["mlist"],"role":"PhD"},"id":"8a1a3e61-edf2-52d6-971f-a78142996fe7"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#c8c8a8","images":{"fallback":{"src":"/static/df6b26adb79301333d78fd4c366b74ff/7e8c6/jordieshier.jpg","srcSet":"/static/df6b26adb79301333d78fd4c366b74ff/7e6ad/jordieshier.jpg 158w,\n/static/df6b26adb79301333d78fd4c366b74ff/f54cf/jordieshier.jpg 315w,\n/static/df6b26adb79301333d78fd4c366b74ff/7e8c6/jordieshier.jpg 630w","sizes":"(min-width: 630px) 630px, 100vw"},"sources":[{"srcSet":"/static/df6b26adb79301333d78fd4c366b74ff/c80df/jordieshier.webp 158w,\n/static/df6b26adb79301333d78fd4c366b74ff/c775c/jordieshier.webp 315w,\n/static/df6b26adb79301333d78fd4c366b74ff/4bb20/jordieshier.webp 630w","type":"image/webp","sizes":"(min-width: 630px) 630px, 100vw"}]},"width":630,"height":630}}},"name":"Jordie Shier","url":"https://jordieshier.com/","acadposition":"PhD Student","blurb":"Real-time timbral mapping for synthesized percussive performance","themes":["comma","augmi"],"role":"PhD"},"id":"df913101-13d0-56f9-9579-b4f5856ab887"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/160d1fb96547295d1505927194181268/30cdc/julienguinot.png","srcSet":"/static/160d1fb96547295d1505927194181268/7458e/julienguinot.png 75w,\n/static/160d1fb96547295d1505927194181268/de3a1/julienguinot.png 150w,\n/static/160d1fb96547295d1505927194181268/30cdc/julienguinot.png 300w","sizes":"(min-width: 300px) 300px, 100vw"},"sources":[{"srcSet":"/static/160d1fb96547295d1505927194181268/18188/julienguinot.webp 75w,\n/static/160d1fb96547295d1505927194181268/c65bc/julienguinot.webp 150w,\n/static/160d1fb96547295d1505927194181268/078c3/julienguinot.webp 300w","type":"image/webp","sizes":"(min-width: 300px) 300px, 100vw"}]},"width":300,"height":300}}},"name":"Julien Guinot","url":null,"acadposition":"PhD Student","blurb":"Beyond Supervised Learning for Musical Audio","themes":[],"role":"PhD"},"id":"5a979676-5bc8-5d5d-bb5f-40e8a3fbff0d"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8a898","images":{"fallback":{"src":"/static/234763aa1e5f363371e4bfff769cc48d/70fef/katarzynaadamska.jpg","srcSet":"/static/234763aa1e5f363371e4bfff769cc48d/463c6/katarzynaadamska.jpg 379w,\n/static/234763aa1e5f363371e4bfff769cc48d/53da6/katarzynaadamska.jpg 758w,\n/static/234763aa1e5f363371e4bfff769cc48d/70fef/katarzynaadamska.jpg 1516w","sizes":"(min-width: 1516px) 1516px, 100vw"},"sources":[{"srcSet":"/static/234763aa1e5f363371e4bfff769cc48d/d4721/katarzynaadamska.webp 379w,\n/static/234763aa1e5f363371e4bfff769cc48d/6420f/katarzynaadamska.webp 758w,\n/static/234763aa1e5f363371e4bfff769cc48d/79063/katarzynaadamska.webp 1516w","type":"image/webp","sizes":"(min-width: 1516px) 1516px, 100vw"}]},"width":1516,"height":1516}}},"name":"Katarzyna Adamska","url":"https://www.qmul.ac.uk/eecs/people/profiles/adamskakatarzynamaria.html","acadposition":"PhD Student","blurb":"Predicting hit songs: multimodal and data-driven approach","themes":["mir","audioeng","mcog"],"role":"PhD"},"id":"3ff53e35-a554-51cd-925e-caa7670bd21b"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#c8c8c8","images":{"fallback":{"src":"/static/3b589918aa1cfd8c1f2a0e928a803ece/1a361/keshavbhandari.jpg","srcSet":"/static/3b589918aa1cfd8c1f2a0e928a803ece/2c33f/keshavbhandari.jpg 56w,\n/static/3b589918aa1cfd8c1f2a0e928a803ece/fa873/keshavbhandari.jpg 113w,\n/static/3b589918aa1cfd8c1f2a0e928a803ece/1a361/keshavbhandari.jpg 225w","sizes":"(min-width: 225px) 225px, 100vw"},"sources":[{"srcSet":"/static/3b589918aa1cfd8c1f2a0e928a803ece/f8744/keshavbhandari.webp 56w,\n/static/3b589918aa1cfd8c1f2a0e928a803ece/26b1c/keshavbhandari.webp 113w,\n/static/3b589918aa1cfd8c1f2a0e928a803ece/252a0/keshavbhandari.webp 225w","type":"image/webp","sizes":"(min-width: 225px) 225px, 100vw"}]},"width":225,"height":225}}},"name":"Keshav Bhandari","url":null,"acadposition":"PhD Student","blurb":"Neuro-Symbolic Automated Music Composition","themes":[],"role":"PhD"},"id":"03e0d7d4-3398-5c4a-8755-d66da027f62d"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/9ec9117c4899bc5c3dd3148ac5489686/d4aca/leleliu.jpg","srcSet":"/static/9ec9117c4899bc5c3dd3148ac5489686/f9edd/leleliu.jpg 240w,\n/static/9ec9117c4899bc5c3dd3148ac5489686/97a19/leleliu.jpg 480w,\n/static/9ec9117c4899bc5c3dd3148ac5489686/d4aca/leleliu.jpg 960w","sizes":"(min-width: 960px) 960px, 100vw"},"sources":[{"srcSet":"/static/9ec9117c4899bc5c3dd3148ac5489686/8d565/leleliu.webp 240w,\n/static/9ec9117c4899bc5c3dd3148ac5489686/21b1a/leleliu.webp 480w,\n/static/9ec9117c4899bc5c3dd3148ac5489686/d6f60/leleliu.webp 960w","type":"image/webp","sizes":"(min-width: 960px) 960px, 100vw"}]},"width":960,"height":960}}},"name":"Lele Liu","url":"http://eecs.qmul.ac.uk/profiles/liulele.html","acadposition":"PhD Student","blurb":"Automatic music transcription with end-to-end deep neural networks","themes":["mir","mlist"],"role":"PhD"},"id":"12a808a3-c5af-5b8f-927d-9078a2f03d06"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#d8d8d8","images":{"fallback":{"src":"/static/1ef30d827f133a3aea147d5fee03407d/dbfe1/lewiswolstanholme.jpg","srcSet":"/static/1ef30d827f133a3aea147d5fee03407d/6f9b4/lewiswolstanholme.jpg 244w,\n/static/1ef30d827f133a3aea147d5fee03407d/51e33/lewiswolstanholme.jpg 488w,\n/static/1ef30d827f133a3aea147d5fee03407d/dbfe1/lewiswolstanholme.jpg 975w","sizes":"(min-width: 975px) 975px, 100vw"},"sources":[{"srcSet":"/static/1ef30d827f133a3aea147d5fee03407d/f44e0/lewiswolstanholme.webp 244w,\n/static/1ef30d827f133a3aea147d5fee03407d/65a55/lewiswolstanholme.webp 488w,\n/static/1ef30d827f133a3aea147d5fee03407d/ba444/lewiswolstanholme.webp 975w","type":"image/webp","sizes":"(min-width: 975px) 975px, 100vw"}]},"width":975,"height":975}}},"name":"Lewis Wolstanholme","url":"http://lewiswolstanholme.co.uk","acadposition":"PhD Student","blurb":"Meta-Physical Modelling","themes":["sounsynthesis","augmi"],"role":"PhD"},"id":"3111f86d-c11a-5074-9838-7ea3cb9e29ce"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#382818","images":{"fallback":{"src":"/static/6a67462118ce70d98991316784e33c4a/0fdf4/louisbradshaw.jpg","srcSet":"/static/6a67462118ce70d98991316784e33c4a/91a6d/louisbradshaw.jpg 75w,\n/static/6a67462118ce70d98991316784e33c4a/96deb/louisbradshaw.jpg 150w,\n/static/6a67462118ce70d98991316784e33c4a/0fdf4/louisbradshaw.jpg 300w","sizes":"(min-width: 300px) 300px, 100vw"},"sources":[{"srcSet":"/static/6a67462118ce70d98991316784e33c4a/18188/louisbradshaw.webp 75w,\n/static/6a67462118ce70d98991316784e33c4a/c65bc/louisbradshaw.webp 150w,\n/static/6a67462118ce70d98991316784e33c4a/078c3/louisbradshaw.webp 300w","type":"image/webp","sizes":"(min-width: 300px) 300px, 100vw"}]},"width":300,"height":300}}},"name":"Louis Bradshaw","url":null,"acadposition":"PhD Student","blurb":"Neuro-symbolic music models","themes":[],"role":"PhD"},"id":"f1e93580-37b7-5662-86c4-990399658efe"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/b412e85c9ed68b9f136ace2f49f08b77/8fc5d/lucamarinelli.png","srcSet":"/static/b412e85c9ed68b9f136ace2f49f08b77/69237/lucamarinelli.png 52w,\n/static/b412e85c9ed68b9f136ace2f49f08b77/658ae/lucamarinelli.png 104w,\n/static/b412e85c9ed68b9f136ace2f49f08b77/8fc5d/lucamarinelli.png 208w","sizes":"(min-width: 208px) 208px, 100vw"},"sources":[{"srcSet":"/static/b412e85c9ed68b9f136ace2f49f08b77/284ac/lucamarinelli.webp 52w,\n/static/b412e85c9ed68b9f136ace2f49f08b77/5ca4c/lucamarinelli.webp 104w,\n/static/b412e85c9ed68b9f136ace2f49f08b77/c95dc/lucamarinelli.webp 208w","type":"image/webp","sizes":"(min-width: 208px) 208px, 100vw"}]},"width":208,"height":208}}},"name":"Luca Marinelli","url":"http://eecs.qmul.ac.uk/profiles/marinelliluca.html","acadposition":"PhD Student","blurb":"Gender-coded sound: A multimodal data-driven analysis of gender encoding strategies in sound and music for advertising","themes":["mlist"],"role":"PhD"},"id":"cea004c4-13f5-5347-9b2a-38a2ac5bbfe0"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Madeline Hamilton","url":"http://eecs.qmul.ac.uk/profiles/hamiltonmadelineann.html","acadposition":"PhD Student","blurb":"Improving AI-generated Music with Pleasure Models","themes":["mcog"],"role":"PhD"},"id":"18a76b0f-6172-5ea5-8b46-f8a3f742a720"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/51634a254670aa2e07b701a172ad9361/8f4a8/marcocomunita.jpg","srcSet":"/static/51634a254670aa2e07b701a172ad9361/82155/marcocomunita.jpg 107w,\n/static/51634a254670aa2e07b701a172ad9361/61790/marcocomunita.jpg 214w,\n/static/51634a254670aa2e07b701a172ad9361/8f4a8/marcocomunita.jpg 428w","sizes":"(min-width: 428px) 428px, 100vw"},"sources":[{"srcSet":"/static/51634a254670aa2e07b701a172ad9361/d9648/marcocomunita.webp 107w,\n/static/51634a254670aa2e07b701a172ad9361/f6ee1/marcocomunita.webp 214w,\n/static/51634a254670aa2e07b701a172ad9361/4ec0e/marcocomunita.webp 428w","type":"image/webp","sizes":"(min-width: 428px) 428px, 100vw"}]},"width":428,"height":428}}},"name":"Marco Comunit√†","url":"http://eecs.qmul.ac.uk/profiles/comunitamarco.html","acadposition":"PhD Student","blurb":"Machine learning applied to sound synthesis models","themes":["audioeng","soundsynthesis"],"role":"PhD"},"id":"96844f85-b589-5907-be07-6cc39213f206"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#281818","images":{"fallback":{"src":"/static/6fdcde74d9bc2329870ebe730c31fad1/0fdf4/marcopasini.jpg","srcSet":"/static/6fdcde74d9bc2329870ebe730c31fad1/91a6d/marcopasini.jpg 75w,\n/static/6fdcde74d9bc2329870ebe730c31fad1/96deb/marcopasini.jpg 150w,\n/static/6fdcde74d9bc2329870ebe730c31fad1/0fdf4/marcopasini.jpg 300w","sizes":"(min-width: 300px) 300px, 100vw"},"sources":[{"srcSet":"/static/6fdcde74d9bc2329870ebe730c31fad1/18188/marcopasini.webp 75w,\n/static/6fdcde74d9bc2329870ebe730c31fad1/c65bc/marcopasini.webp 150w,\n/static/6fdcde74d9bc2329870ebe730c31fad1/078c3/marcopasini.webp 300w","type":"image/webp","sizes":"(min-width: 300px) 300px, 100vw"}]},"width":300,"height":300}}},"name":"Marco Pasini","url":null,"acadposition":"PhD Student","blurb":"Fast and Controllable Music Generation","themes":[],"role":"PhD"},"id":"fd08ed30-a145-5e10-9f68-d189fb8d440b"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8e8","images":{"fallback":{"src":"/static/dba175a523b3c9d78f62ce6131df86ad/a20e0/marypilataki.jpg","srcSet":"/static/dba175a523b3c9d78f62ce6131df86ad/71c2b/marypilataki.jpg 52w,\n/static/dba175a523b3c9d78f62ce6131df86ad/ac761/marypilataki.jpg 104w,\n/static/dba175a523b3c9d78f62ce6131df86ad/a20e0/marypilataki.jpg 208w","sizes":"(min-width: 208px) 208px, 100vw"},"sources":[{"srcSet":"/static/dba175a523b3c9d78f62ce6131df86ad/284ac/marypilataki.webp 52w,\n/static/dba175a523b3c9d78f62ce6131df86ad/5ca4c/marypilataki.webp 104w,\n/static/dba175a523b3c9d78f62ce6131df86ad/c95dc/marypilataki.webp 208w","type":"image/webp","sizes":"(min-width: 208px) 208px, 100vw"}]},"width":208,"height":208}}},"name":"Mary Pilataki","url":"https://github.com/marypilataki","acadposition":"PhD Student","blurb":"Deep Learning methods for Multi-Instrument Music Transcription","themes":["mir"],"role":"PhD"},"id":"9b6c0163-cfdc-560b-82fe-8b8853e02502"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#685848","images":{"fallback":{"src":"/static/da85b24f6f007f84c0993d8cdcbcec36/b26d3/maxgraf.jpg","srcSet":"/static/da85b24f6f007f84c0993d8cdcbcec36/47930/maxgraf.jpg 400w,\n/static/da85b24f6f007f84c0993d8cdcbcec36/baaed/maxgraf.jpg 800w,\n/static/da85b24f6f007f84c0993d8cdcbcec36/b26d3/maxgraf.jpg 1600w","sizes":"(min-width: 1600px) 1600px, 100vw"},"sources":[{"srcSet":"/static/da85b24f6f007f84c0993d8cdcbcec36/416c3/maxgraf.webp 400w,\n/static/da85b24f6f007f84c0993d8cdcbcec36/c1587/maxgraf.webp 800w,\n/static/da85b24f6f007f84c0993d8cdcbcec36/6e0f2/maxgraf.webp 1600w","type":"image/webp","sizes":"(min-width: 1600px) 1600px, 100vw"}]},"width":1600,"height":1600}}},"name":"Max Graf","url":"http://eecs.qmul.ac.uk/profiles/grafmax.html","acadposition":"PhD Student","blurb":"PERFORM-AI (Provide Extended Realities for Musical Performance using AI)","themes":["augmi","isam","soundsynthesis"],"role":"PhD"},"id":"c5ddd5bb-999c-511e-91f9-f17ae20473e7"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#282828","images":{"fallback":{"src":"/static/d0fca70a51998f2762c511a3a0a987f6/dd515/nellygarcia.jpg","srcSet":"/static/d0fca70a51998f2762c511a3a0a987f6/6ac16/nellygarcia.jpg 50w,\n/static/d0fca70a51998f2762c511a3a0a987f6/e07e1/nellygarcia.jpg 100w,\n/static/d0fca70a51998f2762c511a3a0a987f6/dd515/nellygarcia.jpg 200w","sizes":"(min-width: 200px) 200px, 100vw"},"sources":[{"srcSet":"/static/d0fca70a51998f2762c511a3a0a987f6/dbc4a/nellygarcia.webp 50w,\n/static/d0fca70a51998f2762c511a3a0a987f6/d8057/nellygarcia.webp 100w,\n/static/d0fca70a51998f2762c511a3a0a987f6/2e34e/nellygarcia.webp 200w","type":"image/webp","sizes":"(min-width: 200px) 200px, 100vw"}]},"width":200,"height":200}}},"name":"Nelly Garcia","url":"","acadposition":"PhD Student","blurb":"An investigation evaluating realism in sound design","themes":["comma","audioeng"],"role":"PhD"},"id":"ade5f066-9369-5605-9c73-a41b0112d3cc"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#d8c8c8","images":{"fallback":{"src":"/static/525d06e75294a7bf61d839700b1d2be5/3c342/ningzhiwang.jpg","srcSet":"/static/525d06e75294a7bf61d839700b1d2be5/19f6e/ningzhiwang.jpg 106w,\n/static/525d06e75294a7bf61d839700b1d2be5/a8230/ningzhiwang.jpg 212w,\n/static/525d06e75294a7bf61d839700b1d2be5/3c342/ningzhiwang.jpg 423w","sizes":"(min-width: 423px) 423px, 100vw"},"sources":[{"srcSet":"/static/525d06e75294a7bf61d839700b1d2be5/7a2a0/ningzhiwang.webp 106w,\n/static/525d06e75294a7bf61d839700b1d2be5/fbd6b/ningzhiwang.webp 212w,\n/static/525d06e75294a7bf61d839700b1d2be5/fb6ab/ningzhiwang.webp 423w","type":"image/webp","sizes":"(min-width: 423px) 423px, 100vw"}]},"width":423,"height":423}}},"name":"Ningzhi Wang","url":"","acadposition":"PhD Student","blurb":"Generative Models For Music Audio Representation And Understanding","themes":["mir","mcog","soundsynthesis"],"role":"PhD"},"id":"de8a6a9d-d9db-5d6f-aa49-d6383eab8ec3"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/static/7be4a640576b48155a0774a129e26848/5aead/oluremifalowo.png","srcSet":"/static/7be4a640576b48155a0774a129e26848/e9fba/oluremifalowo.png 50w,\n/static/7be4a640576b48155a0774a129e26848/15e42/oluremifalowo.png 100w,\n/static/7be4a640576b48155a0774a129e26848/5aead/oluremifalowo.png 200w","sizes":"(min-width: 200px) 200px, 100vw"},"sources":[{"srcSet":"/static/7be4a640576b48155a0774a129e26848/dbc4a/oluremifalowo.webp 50w,\n/static/7be4a640576b48155a0774a129e26848/d8057/oluremifalowo.webp 100w,\n/static/7be4a640576b48155a0774a129e26848/2e34e/oluremifalowo.webp 200w","type":"image/webp","sizes":"(min-width: 200px) 200px, 100vw"}]},"width":200,"height":200}}},"name":"Oluremi Falowo","url":null,"acadposition":"PhD Student","blurb":"E-AIM - Embodied Cognition in Intelligent Musical Systems","themes":["comma","mcog"],"role":"PhD"},"id":"2d224738-9643-599b-b5e9-e8922a346abd"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/static/3af86cc7e3b5e8e65a510f9f7e3c51ca/5a7c3/pablotablasdepaula.jpg","srcSet":"/static/3af86cc7e3b5e8e65a510f9f7e3c51ca/f505e/pablotablasdepaula.jpg 250w,\n/static/3af86cc7e3b5e8e65a510f9f7e3c51ca/be5ed/pablotablasdepaula.jpg 500w,\n/static/3af86cc7e3b5e8e65a510f9f7e3c51ca/5a7c3/pablotablasdepaula.jpg 1000w","sizes":"(min-width: 1000px) 1000px, 100vw"},"sources":[{"srcSet":"/static/3af86cc7e3b5e8e65a510f9f7e3c51ca/e7160/pablotablasdepaula.webp 250w,\n/static/3af86cc7e3b5e8e65a510f9f7e3c51ca/5f169/pablotablasdepaula.webp 500w,\n/static/3af86cc7e3b5e8e65a510f9f7e3c51ca/3cd29/pablotablasdepaula.webp 1000w","type":"image/webp","sizes":"(min-width: 1000px) 1000px, 100vw"}]},"width":1000,"height":1000}}},"name":"Pablo Tablas De Paula","url":null,"acadposition":"PhD Student","blurb":"Machine Learning of Physical Models","themes":[],"role":"PhD"},"id":"67b7f3dd-ed22-5517-bd45-e55e85adf73d"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/static/f6684da9fe5aacbc07b9ab48ebc1d398/0fdf4/qiaoxizhang.jpg","srcSet":"/static/f6684da9fe5aacbc07b9ab48ebc1d398/91a6d/qiaoxizhang.jpg 75w,\n/static/f6684da9fe5aacbc07b9ab48ebc1d398/96deb/qiaoxizhang.jpg 150w,\n/static/f6684da9fe5aacbc07b9ab48ebc1d398/0fdf4/qiaoxizhang.jpg 300w","sizes":"(min-width: 300px) 300px, 100vw"},"sources":[{"srcSet":"/static/f6684da9fe5aacbc07b9ab48ebc1d398/18188/qiaoxizhang.webp 75w,\n/static/f6684da9fe5aacbc07b9ab48ebc1d398/c65bc/qiaoxizhang.webp 150w,\n/static/f6684da9fe5aacbc07b9ab48ebc1d398/078c3/qiaoxizhang.webp 300w","type":"image/webp","sizes":"(min-width: 300px) 300px, 100vw"}]},"width":300,"height":300}}},"name":"Qiaoxi Zhang","url":null,"acadposition":"PhD Student","blurb":"Multimodal AI for musical collaboration in immersive environments","themes":[],"role":"PhD"},"id":"1f4c907a-170c-5c5d-ad91-2093f9d660d5"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#888898","images":{"fallback":{"src":"/static/1f570e342a9ad6add4f23ca0c3afce47/1b7c4/qingwang.jpg","srcSet":"/static/1f570e342a9ad6add4f23ca0c3afce47/2c33f/qingwang.jpg 56w,\n/static/1f570e342a9ad6add4f23ca0c3afce47/a5b6f/qingwang.jpg 111w,\n/static/1f570e342a9ad6add4f23ca0c3afce47/1b7c4/qingwang.jpg 222w","sizes":"(min-width: 222px) 222px, 100vw"},"sources":[{"srcSet":"/static/1f570e342a9ad6add4f23ca0c3afce47/f8744/qingwang.webp 56w,\n/static/1f570e342a9ad6add4f23ca0c3afce47/29391/qingwang.webp 111w,\n/static/1f570e342a9ad6add4f23ca0c3afce47/39e7e/qingwang.webp 222w","type":"image/webp","sizes":"(min-width: 222px) 222px, 100vw"}]},"width":222,"height":222}}},"name":"Qing Wang","url":null,"acadposition":"PhD Student","blurb":"Multi-modal Learning for Music Understanding","themes":[],"role":"PhD"},"id":"6b142246-be89-5d90-9e59-6b46c1dfb681"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#787878","images":{"fallback":{"src":"/static/7a140f866e4e9fa0d9f50cfd706a5f9b/a8c60/rodrigomauriciodiazfernandez.jpg","srcSet":"/static/7a140f866e4e9fa0d9f50cfd706a5f9b/67766/rodrigomauriciodiazfernandez.jpg 174w,\n/static/7a140f866e4e9fa0d9f50cfd706a5f9b/28fc8/rodrigomauriciodiazfernandez.jpg 349w,\n/static/7a140f866e4e9fa0d9f50cfd706a5f9b/a8c60/rodrigomauriciodiazfernandez.jpg 697w","sizes":"(min-width: 697px) 697px, 100vw"},"sources":[{"srcSet":"/static/7a140f866e4e9fa0d9f50cfd706a5f9b/0515f/rodrigomauriciodiazfernandez.webp 174w,\n/static/7a140f866e4e9fa0d9f50cfd706a5f9b/546d1/rodrigomauriciodiazfernandez.webp 349w,\n/static/7a140f866e4e9fa0d9f50cfd706a5f9b/dfdb2/rodrigomauriciodiazfernandez.webp 697w","type":"image/webp","sizes":"(min-width: 697px) 697px, 100vw"}]},"width":697,"height":697}}},"name":"Rodrigo Mauricio Diaz Fernandez","url":"https://www.qmul.ac.uk/eecs/people/profiles/diazfernandezrodrigomauricio.html","acadposition":"PhD Student","blurb":"Hybrid Neural Methods for Sound Synthesis","themes":["audioeng","soundsynthesis"],"role":"PhD"},"id":"aec8c261-f5de-537e-8871-b725d92c8ed0"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#7898b8","images":{"fallback":{"src":"/static/2046c4feea56b3899883569f75a5fcd1/e9d3f/rubycrocker.jpg","srcSet":"/static/2046c4feea56b3899883569f75a5fcd1/f3d60/rubycrocker.jpg 495w,\n/static/2046c4feea56b3899883569f75a5fcd1/8c337/rubycrocker.jpg 990w,\n/static/2046c4feea56b3899883569f75a5fcd1/e9d3f/rubycrocker.jpg 1980w","sizes":"(min-width: 1980px) 1980px, 100vw"},"sources":[{"srcSet":"/static/2046c4feea56b3899883569f75a5fcd1/72be1/rubycrocker.webp 495w,\n/static/2046c4feea56b3899883569f75a5fcd1/5c459/rubycrocker.webp 990w,\n/static/2046c4feea56b3899883569f75a5fcd1/83852/rubycrocker.webp 1980w","type":"image/webp","sizes":"(min-width: 1980px) 1980px, 100vw"}]},"width":1980,"height":1980}}},"name":"Ruby Crocker","url":"","acadposition":"PhD Student","blurb":"Continuous mood recognition in film music","themes":["mir","mcog"],"role":"PhD"},"id":"9be482ab-e543-5ed7-a0ec-2d37af553450"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#d8e8f8","images":{"fallback":{"src":"/static/c8bb01999fb8afd1799cd90568882943/afbce/saracardinale.jpg","srcSet":"/static/c8bb01999fb8afd1799cd90568882943/0caa4/saracardinale.jpg 193w,\n/static/c8bb01999fb8afd1799cd90568882943/b988d/saracardinale.jpg 385w,\n/static/c8bb01999fb8afd1799cd90568882943/afbce/saracardinale.jpg 770w","sizes":"(min-width: 770px) 770px, 100vw"},"sources":[{"srcSet":"/static/c8bb01999fb8afd1799cd90568882943/6a483/saracardinale.webp 193w,\n/static/c8bb01999fb8afd1799cd90568882943/daff6/saracardinale.webp 385w,\n/static/c8bb01999fb8afd1799cd90568882943/0a091/saracardinale.webp 770w","type":"image/webp","sizes":"(min-width: 770px) 770px, 100vw"}]},"width":770,"height":770}}},"name":"Sara Cardinale","url":"https://www.qmul.ac.uk/eecs/people/profiles/cardinalesara.html","acadposition":"PhD Student","blurb":"Character-based adaptive generative music for film and video games using Deep Learning and Hidden Markov Models","themes":[],"role":"PhD"},"id":"5e7a50f4-1f90-5080-87fe-b7a41fab22b4"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Sebasti√°n Ruiz","url":"","acadposition":"PhD Student","blurb":"Physiological Responses to Ensemble Interaction","themes":["mupae"],"role":"PhD"},"id":"40dfa6dc-8a20-5b1b-9126-67f99313d249"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Shahar Elisha","url":"https://www.linkedin.com/in/shaharelisha/","acadposition":"PhD Student","blurb":"Style classification of podcasts using audio","themes":["mlist"],"role":"PhD"},"id":"f9a12024-53cd-52d6-b01a-6c1145f3fce2"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/7194c98c40fd74305a818071553ec197/68d26/shubhrsingh.png","srcSet":"/static/7194c98c40fd74305a818071553ec197/d8a72/shubhrsingh.png 156w,\n/static/7194c98c40fd74305a818071553ec197/2e7f0/shubhrsingh.png 311w,\n/static/7194c98c40fd74305a818071553ec197/68d26/shubhrsingh.png 622w","sizes":"(min-width: 622px) 622px, 100vw"},"sources":[{"srcSet":"/static/7194c98c40fd74305a818071553ec197/d1e3d/shubhrsingh.webp 156w,\n/static/7194c98c40fd74305a818071553ec197/b699b/shubhrsingh.webp 311w,\n/static/7194c98c40fd74305a818071553ec197/3286e/shubhrsingh.webp 622w","type":"image/webp","sizes":"(min-width: 622px) 622px, 100vw"}]},"width":622,"height":622}}},"name":"Shubhr Singh","url":"http://eecs.qmul.ac.uk/profiles/singhshubhr.html","acadposition":"PhD Student","blurb":"Audio Applications of Novel Mathematical Methods in Deep Learning","themes":["soundsynthesis","mlist"],"role":"PhD"},"id":"72d3c05f-c5ce-5fe9-9c95-733813b814f7"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#282828","images":{"fallback":{"src":"/static/25f366d75096a98ef450dd293e963097/1a361/shuoyangzheng.jpg","srcSet":"/static/25f366d75096a98ef450dd293e963097/2c33f/shuoyangzheng.jpg 56w,\n/static/25f366d75096a98ef450dd293e963097/fa873/shuoyangzheng.jpg 113w,\n/static/25f366d75096a98ef450dd293e963097/1a361/shuoyangzheng.jpg 225w","sizes":"(min-width: 225px) 225px, 100vw"},"sources":[{"srcSet":"/static/25f366d75096a98ef450dd293e963097/f8744/shuoyangzheng.webp 56w,\n/static/25f366d75096a98ef450dd293e963097/26b1c/shuoyangzheng.webp 113w,\n/static/25f366d75096a98ef450dd293e963097/252a0/shuoyangzheng.webp 225w","type":"image/webp","sizes":"(min-width: 225px) 225px, 100vw"}]},"width":225,"height":225}}},"name":"Shuoyang Zheng","url":null,"acadposition":"PhD Student","blurb":"Explainability of AI Music Generation","themes":[],"role":"PhD"},"id":"6abdd323-5583-5572-a94c-81526ad83016"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8d8","images":{"fallback":{"src":"/static/fa7cb1f821b8a5e7891d12048a9bf77a/49d15/soumyavanka.jpg","srcSet":"/static/fa7cb1f821b8a5e7891d12048a9bf77a/18eda/soumyavanka.jpg 133w,\n/static/fa7cb1f821b8a5e7891d12048a9bf77a/e53cc/soumyavanka.jpg 266w,\n/static/fa7cb1f821b8a5e7891d12048a9bf77a/49d15/soumyavanka.jpg 531w","sizes":"(min-width: 531px) 531px, 100vw"},"sources":[{"srcSet":"/static/fa7cb1f821b8a5e7891d12048a9bf77a/a2d02/soumyavanka.webp 133w,\n/static/fa7cb1f821b8a5e7891d12048a9bf77a/fd489/soumyavanka.webp 266w,\n/static/fa7cb1f821b8a5e7891d12048a9bf77a/dc6f9/soumyavanka.webp 531w","type":"image/webp","sizes":"(min-width: 531px) 531px, 100vw"}]},"width":531,"height":531}}},"name":"Soumya Sai Vanka","url":"http://eecs.qmul.ac.uk/profiles/vankasaisoumya.html","acadposition":"PhD Student","blurb":"Music Production Style Transfer and Mix Similarity","themes":["audioeng","mir"],"role":"PhD"},"id":"64e68bf1-afa9-5242-ba95-3bbb3ff80584"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#d8c8a8","images":{"fallback":{"src":"/static/d1179903e45c8a94827072c6928a258a/13677/teodannemann.png","srcSet":"/static/d1179903e45c8a94827072c6928a258a/de391/teodannemann.png 250w,\n/static/d1179903e45c8a94827072c6928a258a/82c11/teodannemann.png 500w,\n/static/d1179903e45c8a94827072c6928a258a/13677/teodannemann.png 1000w","sizes":"(min-width: 1000px) 1000px, 100vw"},"sources":[{"srcSet":"/static/d1179903e45c8a94827072c6928a258a/e7160/teodannemann.webp 250w,\n/static/d1179903e45c8a94827072c6928a258a/5f169/teodannemann.webp 500w,\n/static/d1179903e45c8a94827072c6928a258a/3cd29/teodannemann.webp 1000w","type":"image/webp","sizes":"(min-width: 1000px) 1000px, 100vw"}]},"width":1000,"height":1000}}},"name":"Teodoro Dannemann","url":"https://teodannemann.wordpress.com/","acadposition":"PhD Student","blurb":"Sabotaging, errors and other mistakes as a source of new techniques in music improvisation","themes":["augmi"],"role":"PhD"},"id":"ba04e9fb-b90e-5945-91de-586f456c0186"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#c8c8b8","images":{"fallback":{"src":"/static/b81f17ae0db9a7d98b2b88a627769f50/4227f/teresapelinskiramos.jpg","srcSet":"/static/b81f17ae0db9a7d98b2b88a627769f50/9e7cd/teresapelinskiramos.jpg 432w,\n/static/b81f17ae0db9a7d98b2b88a627769f50/ead50/teresapelinskiramos.jpg 864w,\n/static/b81f17ae0db9a7d98b2b88a627769f50/4227f/teresapelinskiramos.jpg 1728w","sizes":"(min-width: 1728px) 1728px, 100vw"},"sources":[{"srcSet":"/static/b81f17ae0db9a7d98b2b88a627769f50/18bed/teresapelinskiramos.webp 432w,\n/static/b81f17ae0db9a7d98b2b88a627769f50/d0f84/teresapelinskiramos.webp 864w,\n/static/b81f17ae0db9a7d98b2b88a627769f50/ca0c6/teresapelinskiramos.webp 1728w","type":"image/webp","sizes":"(min-width: 1728px) 1728px, 100vw"}]},"width":1728,"height":1728}}},"name":"Teresa Pelinski","url":"https://www.teresapelinski.com/","acadposition":"PhD Student","blurb":"Sensor mesh as performance interface","themes":["augmi"],"role":"PhD"},"id":"5b4590da-aaa8-57d4-93eb-912b7e8a1654"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/static/4ea878db394500ed14f41f522a4afb01/b74b1/tylermcintosh.jpg","srcSet":"/static/4ea878db394500ed14f41f522a4afb01/d3fc0/tylermcintosh.jpg 270w,\n/static/4ea878db394500ed14f41f522a4afb01/19455/tylermcintosh.jpg 540w,\n/static/4ea878db394500ed14f41f522a4afb01/b74b1/tylermcintosh.jpg 1080w","sizes":"(min-width: 1080px) 1080px, 100vw"},"sources":[{"srcSet":"/static/4ea878db394500ed14f41f522a4afb01/ede49/tylermcintosh.webp 270w,\n/static/4ea878db394500ed14f41f522a4afb01/4cb34/tylermcintosh.webp 540w,\n/static/4ea878db394500ed14f41f522a4afb01/4f506/tylermcintosh.webp 1080w","type":"image/webp","sizes":"(min-width: 1080px) 1080px, 100vw"}]},"width":1080,"height":1080}}},"name":"Tyler Howard McIntosh","url":"","acadposition":"PhD Student","blurb":"Expressive Performance Rendering for Music Generation Systems","themes":["mcog","mir"],"role":"PhD"},"id":"aac95703-2ad6-5435-ba42-e404dc3a150e"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Vjosa Preniqi","url":"http://eecs.qmul.ac.uk/profiles/preniqivjosa.html","acadposition":"PhD Student","blurb":"Predicting demographics, personalities, and global values from digital media behaviours","themes":["mir","mcog"],"role":"PhD"},"id":"18bb4e4a-ca3d-51a0-9beb-5aea409131ab"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8e8","images":{"fallback":{"src":"/static/d9e4891e02c2d5580fb05022babf8891/4ce45/xavierriley.jpg","srcSet":"/static/d9e4891e02c2d5580fb05022babf8891/21980/xavierriley.jpg 167w,\n/static/d9e4891e02c2d5580fb05022babf8891/f560f/xavierriley.jpg 333w,\n/static/d9e4891e02c2d5580fb05022babf8891/4ce45/xavierriley.jpg 666w","sizes":"(min-width: 666px) 666px, 100vw"},"sources":[{"srcSet":"/static/d9e4891e02c2d5580fb05022babf8891/9b205/xavierriley.webp 167w,\n/static/d9e4891e02c2d5580fb05022babf8891/e1538/xavierriley.webp 333w,\n/static/d9e4891e02c2d5580fb05022babf8891/bf451/xavierriley.webp 666w","type":"image/webp","sizes":"(min-width: 666px) 666px, 100vw"}]},"width":666,"height":666}}},"name":"Xavier Riley","url":"http://eecs.qmul.ac.uk/profiles/rileyjohnxavier.html","acadposition":"PhD Student","blurb":"Pitch tracking for music applications - beyond 99% accuracy","themes":["mir","audioeng"],"role":"PhD"},"id":"d964f102-7a03-5e39-8ca5-d62171217446"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#e8e8e8","images":{"fallback":{"src":"/static/7371426282ff3e012ec154dd11dd684e/c29ce/xiaowanyi.jpg","srcSet":"/static/7371426282ff3e012ec154dd11dd684e/77813/xiaowanyi.jpg 545w,\n/static/7371426282ff3e012ec154dd11dd684e/160ed/xiaowanyi.jpg 1091w,\n/static/7371426282ff3e012ec154dd11dd684e/c29ce/xiaowanyi.jpg 2181w","sizes":"(min-width: 2181px) 2181px, 100vw"},"sources":[{"srcSet":"/static/7371426282ff3e012ec154dd11dd684e/9bbe1/xiaowanyi.webp 545w,\n/static/7371426282ff3e012ec154dd11dd684e/282f9/xiaowanyi.webp 1091w,\n/static/7371426282ff3e012ec154dd11dd684e/630ab/xiaowanyi.webp 2181w","type":"image/webp","sizes":"(min-width: 2181px) 2181px, 100vw"}]},"width":2181,"height":2181}}},"name":"Xiaowan Yi","url":"https://www.qmul.ac.uk/eecs/people/profiles/yixiaowan.html","acadposition":"PhD Student","blurb":"Composition-aware music recommendation system for music production","themes":["mir","audioeng","isam"],"role":"PhD"},"id":"f0e1d613-f724-56a3-bcb9-30f54b132dcb"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/0d574bf24113faca635dd75ded6cc9b4/b74b1/ioannisvasilakis.jpg","srcSet":"/static/0d574bf24113faca635dd75ded6cc9b4/d3fc0/ioannisvasilakis.jpg 270w,\n/static/0d574bf24113faca635dd75ded6cc9b4/19455/ioannisvasilakis.jpg 540w,\n/static/0d574bf24113faca635dd75ded6cc9b4/b74b1/ioannisvasilakis.jpg 1080w","sizes":"(min-width: 1080px) 1080px, 100vw"},"sources":[{"srcSet":"/static/0d574bf24113faca635dd75ded6cc9b4/ede49/ioannisvasilakis.webp 270w,\n/static/0d574bf24113faca635dd75ded6cc9b4/4cb34/ioannisvasilakis.webp 540w,\n/static/0d574bf24113faca635dd75ded6cc9b4/4f506/ioannisvasilakis.webp 1080w","type":"image/webp","sizes":"(min-width: 1080px) 1080px, 100vw"}]},"width":1080,"height":1080}}},"name":"Yannis (John) Vasilakis","url":"https://www.linkedin.com/in/yannis-vasilakis-6bb9b11b1/","acadposition":"PhD Student","blurb":"Active Learning for Interactive Music Transcription","themes":["mlist","mir"],"role":"PhD"},"id":"84af4731-c9f4-52e0-80a0-42d0f5b94f55"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#989898","images":{"fallback":{"src":"/static/b1d39ee566fdd8e6fcb02de5fd0b11d9/ba56e/yazhouli.jpg","srcSet":"/static/b1d39ee566fdd8e6fcb02de5fd0b11d9/64c35/yazhouli.jpg 173w,\n/static/b1d39ee566fdd8e6fcb02de5fd0b11d9/605c6/yazhouli.jpg 347w,\n/static/b1d39ee566fdd8e6fcb02de5fd0b11d9/ba56e/yazhouli.jpg 693w","sizes":"(min-width: 693px) 693px, 100vw"},"sources":[{"srcSet":"/static/b1d39ee566fdd8e6fcb02de5fd0b11d9/68ba0/yazhouli.webp 173w,\n/static/b1d39ee566fdd8e6fcb02de5fd0b11d9/d5fde/yazhouli.webp 347w,\n/static/b1d39ee566fdd8e6fcb02de5fd0b11d9/7c9d1/yazhouli.webp 693w","type":"image/webp","sizes":"(min-width: 693px) 693px, 100vw"}]},"width":693,"height":693}}},"name":"Yazhou Li","url":"https://www.qmul.ac.uk/eecs/people/profiles/liyazhou.html","acadposition":"PhD Student","blurb":"Virtual Placement of Objects in Acoustic Scenes","themes":["audioeng","soundsynthesis","isam","mlist"],"role":"PhD"},"id":"563606d0-e18c-52ae-a94f-124b16a4f868"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/863305ac2f181e0dea2b51d577948de1/1a361/yifanxie.jpg","srcSet":"/static/863305ac2f181e0dea2b51d577948de1/2c33f/yifanxie.jpg 56w,\n/static/863305ac2f181e0dea2b51d577948de1/fa873/yifanxie.jpg 113w,\n/static/863305ac2f181e0dea2b51d577948de1/1a361/yifanxie.jpg 225w","sizes":"(min-width: 225px) 225px, 100vw"},"sources":[{"srcSet":"/static/863305ac2f181e0dea2b51d577948de1/f8744/yifanxie.webp 56w,\n/static/863305ac2f181e0dea2b51d577948de1/26b1c/yifanxie.webp 113w,\n/static/863305ac2f181e0dea2b51d577948de1/252a0/yifanxie.webp 225w","type":"image/webp","sizes":"(min-width: 225px) 225px, 100vw"}]},"width":225,"height":225}}},"name":"Yifan Xie","url":null,"acadposition":"PhD Student","blurb":"Film score composer AI assistant: generating expressive mockups","themes":[],"role":"PhD"},"id":"9f049769-6ab7-5bf5-a12b-d2b2634b5876"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/89c68e42d18742a0131b975951dddd78/7eeb7/yinjyunluo.png","srcSet":"/static/89c68e42d18742a0131b975951dddd78/10479/yinjyunluo.png 115w,\n/static/89c68e42d18742a0131b975951dddd78/4437f/yinjyunluo.png 229w,\n/static/89c68e42d18742a0131b975951dddd78/7eeb7/yinjyunluo.png 458w","sizes":"(min-width: 458px) 458px, 100vw"},"sources":[{"srcSet":"/static/89c68e42d18742a0131b975951dddd78/f8466/yinjyunluo.webp 115w,\n/static/89c68e42d18742a0131b975951dddd78/4b05c/yinjyunluo.webp 229w,\n/static/89c68e42d18742a0131b975951dddd78/1efa3/yinjyunluo.webp 458w","type":"image/webp","sizes":"(min-width: 458px) 458px, 100vw"}]},"width":458,"height":458}}},"name":"Yin-Jyun Luo","url":"http://eecs.qmul.ac.uk/profiles/luoyin-jyun.html","acadposition":"PhD Student","blurb":"Industry-scale Machine Listening for Music and Audio Data","themes":["soundsynthesis","mlist"],"role":"PhD"},"id":"b699de08-bbee-5b10-bd93-9643604e2681"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/94b0e38320ab53f4542d711f270cc5c7/a89ca/yinghaoma.jpg","srcSet":"/static/94b0e38320ab53f4542d711f270cc5c7/96deb/yinghaoma.jpg 150w,\n/static/94b0e38320ab53f4542d711f270cc5c7/0fdf4/yinghaoma.jpg 300w,\n/static/94b0e38320ab53f4542d711f270cc5c7/a89ca/yinghaoma.jpg 600w","sizes":"(min-width: 600px) 600px, 100vw"},"sources":[{"srcSet":"/static/94b0e38320ab53f4542d711f270cc5c7/c65bc/yinghaoma.webp 150w,\n/static/94b0e38320ab53f4542d711f270cc5c7/078c3/yinghaoma.webp 300w,\n/static/94b0e38320ab53f4542d711f270cc5c7/6d09e/yinghaoma.webp 600w","type":"image/webp","sizes":"(min-width: 600px) 600px, 100vw"}]},"width":600,"height":600}}},"name":"Yinghao Ma","url":"https://nicolaus625.github.io/","acadposition":"PhD Student","blurb":"Self-supervision in machine listening","themes":["mir","mlist"],"role":"PhD"},"id":"e0104201-8b94-5c6c-862b-a813f7cdc558"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/406e06b0efaa63e877b039820154296d/2655f/yixiaozhang.png","srcSet":"/static/406e06b0efaa63e877b039820154296d/1f8a1/yixiaozhang.png 80w,\n/static/406e06b0efaa63e877b039820154296d/d8938/yixiaozhang.png 159w,\n/static/406e06b0efaa63e877b039820154296d/2655f/yixiaozhang.png 318w","sizes":"(min-width: 318px) 318px, 100vw"},"sources":[{"srcSet":"/static/406e06b0efaa63e877b039820154296d/61ca6/yixiaozhang.webp 80w,\n/static/406e06b0efaa63e877b039820154296d/0993c/yixiaozhang.webp 159w,\n/static/406e06b0efaa63e877b039820154296d/4ce64/yixiaozhang.webp 318w","type":"image/webp","sizes":"(min-width: 318px) 318px, 100vw"}]},"width":318,"height":318}}},"name":"Yixiao Zhang","url":"http://eecs.qmul.ac.uk/profiles/zhangyixiao.html","acadposition":"PhD Student","blurb":"Machine Learning Methods for Artificial Musicality","themes":["mir"],"role":"PhD"},"id":"8b81527a-163a-5125-9fc9-247a73a54a05"},{"frontmatter":{"image":null,"name":"Yukun Li","url":"http://eecs.qmul.ac.uk/profiles/liyukun.html","acadposition":"PhD Student","blurb":"Computational Comparison Between Different Genres of Music in Terms of the Singing Voice","themes":["mir"],"role":"PhD"},"id":"8d93ecc6-ed02-5662-a390-01b9e68ef5f5"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#182838","images":{"fallback":{"src":"/static/509c6285e54fcaf3ebf2ef6abefbf500/0fdf4/zixunguo.jpg","srcSet":"/static/509c6285e54fcaf3ebf2ef6abefbf500/91a6d/zixunguo.jpg 75w,\n/static/509c6285e54fcaf3ebf2ef6abefbf500/96deb/zixunguo.jpg 150w,\n/static/509c6285e54fcaf3ebf2ef6abefbf500/0fdf4/zixunguo.jpg 300w","sizes":"(min-width: 300px) 300px, 100vw"},"sources":[{"srcSet":"/static/509c6285e54fcaf3ebf2ef6abefbf500/18188/zixunguo.webp 75w,\n/static/509c6285e54fcaf3ebf2ef6abefbf500/c65bc/zixunguo.webp 150w,\n/static/509c6285e54fcaf3ebf2ef6abefbf500/078c3/zixunguo.webp 300w","type":"image/webp","sizes":"(min-width: 300px) 300px, 100vw"}]},"width":300,"height":300}}},"name":"Zixun (Nicolas) Guo","url":null,"acadposition":"PhD Student","blurb":"Towards Tonality-Aware Music Understanding: Modeling Complex Tonal Harmony","themes":[],"role":"PhD"},"id":"7c9054ff-1a2b-55f4-82fc-8910109b34db"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#88c8d8","images":{"fallback":{"src":"/static/fff2c93c274e1e81d42f39175f3dfa71/16df3/luigimarino.jpg","srcSet":"/static/fff2c93c274e1e81d42f39175f3dfa71/3f72f/luigimarino.jpg 519w,\n/static/fff2c93c274e1e81d42f39175f3dfa71/ad2cc/luigimarino.jpg 1037w,\n/static/fff2c93c274e1e81d42f39175f3dfa71/16df3/luigimarino.jpg 2074w","sizes":"(min-width: 2074px) 2074px, 100vw"},"sources":[{"srcSet":"/static/fff2c93c274e1e81d42f39175f3dfa71/2977b/luigimarino.webp 519w,\n/static/fff2c93c274e1e81d42f39175f3dfa71/04baf/luigimarino.webp 1037w,\n/static/fff2c93c274e1e81d42f39175f3dfa71/7bb8c/luigimarino.webp 2074w","type":"image/webp","sizes":"(min-width: 2074px) 2074px, 100vw"}]},"width":2074,"height":2074}}},"name":"Dr Luigi Marino","url":"https://www.luigimarino.net/","acadposition":"Research Fellow in Sound and Music Computing","blurb":"Networks able to display relationships between human and nonhuman actors. Project: Sensing the Forest.","themes":["mlist"],"role":"Postdoc"},"id":"89c87f82-5459-5595-baf2-f387e246b600"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#e8d8c8","images":{"fallback":{"src":"/static/becb17eb655c6af083c8d180ffcbce91/d1382/pedrosarmento.jpg","srcSet":"/static/becb17eb655c6af083c8d180ffcbce91/70617/pedrosarmento.jpg 582w,\n/static/becb17eb655c6af083c8d180ffcbce91/936a1/pedrosarmento.jpg 1165w,\n/static/becb17eb655c6af083c8d180ffcbce91/d1382/pedrosarmento.jpg 2329w","sizes":"(min-width: 2329px) 2329px, 100vw"},"sources":[{"srcSet":"/static/becb17eb655c6af083c8d180ffcbce91/f27b2/pedrosarmento.webp 582w,\n/static/becb17eb655c6af083c8d180ffcbce91/70259/pedrosarmento.webp 1165w,\n/static/becb17eb655c6af083c8d180ffcbce91/86c09/pedrosarmento.webp 2329w","type":"image/webp","sizes":"(min-width: 2329px) 2329px, 100vw"}]},"width":2329,"height":2329}}},"name":"Dr Pedro Sarmento","url":"https://otnemrasordep.github.io/","acadposition":"Postdoctoral Researcher","blurb":"music information retrieval, language models for music generation, guitar tablature generation, automatic guitar transcription, deep learning","themes":["mir"],"role":"Postdoc"},"id":"68891a8b-ed44-55dc-a3f7-f02f74bda436"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#383838","images":{"fallback":{"src":"/static/023e1317ff77915bcb3ef017389aa670/071a6/saurjyasarkar.jpg","srcSet":"/static/023e1317ff77915bcb3ef017389aa670/ec705/saurjyasarkar.jpg 451w,\n/static/023e1317ff77915bcb3ef017389aa670/e51ad/saurjyasarkar.jpg 901w,\n/static/023e1317ff77915bcb3ef017389aa670/071a6/saurjyasarkar.jpg 1802w","sizes":"(min-width: 1802px) 1802px, 100vw"},"sources":[{"srcSet":"/static/023e1317ff77915bcb3ef017389aa670/120d7/saurjyasarkar.webp 451w,\n/static/023e1317ff77915bcb3ef017389aa670/f57e4/saurjyasarkar.webp 901w,\n/static/023e1317ff77915bcb3ef017389aa670/73d77/saurjyasarkar.webp 1802w","type":"image/webp","sizes":"(min-width: 1802px) 1802px, 100vw"}]},"width":1802,"height":1802}}},"name":"Dr Saurjya Sarkar","url":"http://eecs.qmul.ac.uk/profiles/sarkarsaurjya-1.html","acadposition":"Postdoctoral Researcher","blurb":"Audio Source Separation, Music Information Retrieval, Sample Detection","themes":["mir"],"role":"Postdoc"},"id":"fa3ed1ea-d5f0-59f2-92de-1a1aaa953db9"},{"frontmatter":{"image":null,"name":"Dr Yuanyuan Liu","url":"http://eecs.qmul.ac.uk/profiles/liuyuanyuan.html","acadposition":"Postdoctoral Researcher","blurb":"Project: Digital Platforms for Craft in the UK and China","themes":["isam"],"role":"Postdoc"},"id":"48db11d9-63d2-55c1-86e6-f79ab92d8fac"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/c0f12a01968f7a264dd0c78e6139f6c0/f9edd/ivanhiggs.jpg","srcSet":"/static/c0f12a01968f7a264dd0c78e6139f6c0/93848/ivanhiggs.jpg 60w,\n/static/c0f12a01968f7a264dd0c78e6139f6c0/73bb6/ivanhiggs.jpg 120w,\n/static/c0f12a01968f7a264dd0c78e6139f6c0/f9edd/ivanhiggs.jpg 240w","sizes":"(min-width: 240px) 240px, 100vw"},"sources":[{"srcSet":"/static/c0f12a01968f7a264dd0c78e6139f6c0/927d1/ivanhiggs.webp 60w,\n/static/c0f12a01968f7a264dd0c78e6139f6c0/507b0/ivanhiggs.webp 120w,\n/static/c0f12a01968f7a264dd0c78e6139f6c0/8d565/ivanhiggs.webp 240w","type":"image/webp","sizes":"(min-width: 240px) 240px, 100vw"}]},"width":240,"height":240}}},"name":"Ivan Meresman Higgs","url":"https://github.com/ivanlmh","acadposition":"Research Assistant","blurb":"Sample Identification in Mastered Songs using Deep Learning Methods","themes":["mir"],"role":"Research Assistant"},"id":"8ee3a5c5-91a8-5ec8-8d03-f636b18d244d"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#081808","images":{"fallback":{"src":"/static/335b628a1c69b433d57e9201845b16d7/37851/jazasyed.jpg","srcSet":"/static/335b628a1c69b433d57e9201845b16d7/5b1fa/jazasyed.jpg 115w,\n/static/335b628a1c69b433d57e9201845b16d7/f0f53/jazasyed.jpg 230w,\n/static/335b628a1c69b433d57e9201845b16d7/37851/jazasyed.jpg 460w","sizes":"(min-width: 460px) 460px, 100vw"},"sources":[{"srcSet":"/static/335b628a1c69b433d57e9201845b16d7/f8466/jazasyed.webp 115w,\n/static/335b628a1c69b433d57e9201845b16d7/84992/jazasyed.webp 230w,\n/static/335b628a1c69b433d57e9201845b16d7/b5c5b/jazasyed.webp 460w","type":"image/webp","sizes":"(min-width: 460px) 460px, 100vw"}]},"width":460,"height":460}}},"name":"Jaza Syed","url":"https://jaza.xyz/","acadposition":"Research Assistant","blurb":"Audio ML, Automatic Lyrics Transcription","themes":["mir"],"role":"Research Assistant"},"id":"8209c891-60bc-5d31-b5de-2e332d1ae01c"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Sungkyun Chang","url":"https://mimbres.github.io/home/","acadposition":"Research Assistant","blurb":"Deep learning technologies for multi-instrument automatic music transcription","themes":["mir","mlist"],"role":"Research Assistant"},"id":"c4baa5bd-09a1-5c86-9e66-b8a4a269c168"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#787878","images":{"fallback":{"src":"/static/b176f58af7b3a51d042d0f1ce075804b/237c4/alvarobort.jpg","srcSet":"/static/b176f58af7b3a51d042d0f1ce075804b/1d5ba/alvarobort.jpg 341w,\n/static/b176f58af7b3a51d042d0f1ce075804b/5309a/alvarobort.jpg 683w,\n/static/b176f58af7b3a51d042d0f1ce075804b/237c4/alvarobort.jpg 1365w","sizes":"(min-width: 1365px) 1365px, 100vw"},"sources":[{"srcSet":"/static/b176f58af7b3a51d042d0f1ce075804b/2bf7d/alvarobort.webp 341w,\n/static/b176f58af7b3a51d042d0f1ce075804b/3e123/alvarobort.webp 683w,\n/static/b176f58af7b3a51d042d0f1ce075804b/04459/alvarobort.webp 1365w","type":"image/webp","sizes":"(min-width: 1365px) 1365px, 100vw"}]},"width":1365,"height":1365}}},"name":"Alvaro Bort","url":"http://eecs.qmul.ac.uk/profiles/bortalvaro.html","acadposition":"Research Programme Manager","blurb":"Projects: UKRI Centre for Doctoral Training in Artificial Intelligence and Music, New Frontiers in Music Information Processing (MIP-Frontiers)","themes":[],"role":"Support"},"id":"2e846599-fb90-5878-8c6c-ece409f50c2f"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Dr Matthias Mauch","url":"http://www.matthiasmauch.net/","acadposition":"Visiting Academic","blurb":"music transcription (chords, beats, drums, melody, ...), interactive music annotation, singing research, research in the evolution of musical styles","themes":["mir"],"role":"Visitor"},"id":"680dbd18-ee36-5c29-8ae3-3c894e883317"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Dr Satvik Venkatesh","url":"https://satvik-venkatesh.github.io/","acadposition":"L-Acoustics UK Ltd","blurb":"Online Speech Enhancement In Scenarios With Low Direct-to-Reverberant-Ratio","themes":["Visitor"],"role":"Visitor"},"id":"744bafa6-2b9d-5e52-813d-02b93c2c2eb0"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Hyon Kim","url":"https://scholar.google.com/citations?user=2XHZIuUAAAAJ&hl=en","acadposition":"Universitat Pompeu Fabra","blurb":"Automated Music Performance Assessment and Critique","themes":["mir"],"role":"Visitor"},"id":"d556fbfe-4709-5229-a169-86adac5a47c3"}]},"allTags":{"group":[{"fieldValue":"Academic","totalCount":14},{"fieldValue":"Academic Associate","totalCount":4},{"fieldValue":"PhD","totalCount":81},{"fieldValue":"Postdoc","totalCount":4},{"fieldValue":"Research Assistant","totalCount":3},{"fieldValue":"Support","totalCount":1},{"fieldValue":"Visitor","totalCount":3}]}}}
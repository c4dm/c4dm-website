{"data":{"text":{"html":"<!-- * [**News and reporting**](http://c4dm.eecs.qmul.ac.uk/intelligentsoundengineering/news/index.html) on our research -->\n<ul>\n<li><strong>Demonstration videos</strong> on <a href=\"http://www.youtube.com/user/IntelligentSoundEng\">YouTube Channel: Intelligent Sound Engineering</a></li>\n<li>Check out our <a href=\"https://intelligentsoundengineering.wordpress.com/\"><strong>blog, Intelligent Sound Engineering</strong></a></li>\n<li><a href=\"http://www.eecs.qmul.ac.uk/~josh/publications.htm\"><strong>Academic publications</strong></a></li>\n</ul>\n<p>Some of our current and recent research projects include;</p>\n<ul>\n<li><a href=\"https://github.com/BrechtDeMan/WebAudioEvaluationTool\">Web Audio Evaluation Tool</a>- a browser-based listening test environment. See the <a href=\"http://www.eecs.qmul.ac.uk/~josh/documents/2016/Jillings%20-%20WAET%20-%202016.pdf\">Web Audio Conference</a> and <a href=\"http://www.eecs.qmul.ac.uk/~josh/documents/2015/Jillings%20-%20SMC10%202015.pdf\">Sound and Music Computing</a> publications and <a href=\"https://youtu.be/aHmgSSVaPRY\">this demo video</a></li>\n<li><a href=\"https://www.semanticaudio.ac.uk/blog/the-open-multitrack-testbed/\">The Open Multitrack Testbed</a></li>\n<li><a href=\"http://c4dm.eecs.qmul.ac.uk/intelligentsoundengineering/automaticmixing/index.html\">Automatic mixing</a> - Methods for automatic multitrack audio production in real-time</li>\n<li><a href=\"../../../archive/soundsynthesis/\">Sound synthesis</a></li>\n<li><a href=\"http://c4dm.eecs.qmul.ac.uk/intelligentsoundengineering/spatialaudio/index.html\">Spatial audio production and reproduction</a></li>\n<li><a href=\"http://c4dm.eecs.qmul.ac.uk/intelligentsoundengineering/reverseengineering/index.html\">Reverse engineering the mix - a targeted mixing and mastering tool</a></li>\n<li><a href=\"http://c4dm.eecs.qmul.ac.uk/intelligentsoundengineering/microphoneartifacts/index.html\">Reducing Artifacts Between Source and Microphone in Live Sound</a></li>\n<li><a href=\"http://c4dm.eecs.qmul.ac.uk/intelligentsoundengineering/transientmodification/index.html\">Transient modification</a></li>\n<li><a href=\"http://c4dm.eecs.qmul.ac.uk/intelligentsoundengineering/sourceseparation/index.html\">Source separation for live sound</a> - Crosstalk, bleed and interference reduction techniques</li>\n<li><a href=\"http://c4dm.eecs.qmul.ac.uk/intelligentsoundengineering/gesturecontrol/index.html\">Gesture-controlled mixing of multichannel audio</a></li>\n<li><a href=\"http://www.eecs.qmul.ac.uk/~stevenh/multi_seg.html\">Multitrack segmentation</a></li>\n<li><a href=\"http://c4dm.eecs.qmul.ac.uk/intelligentsoundengineering/compressors/index.html\">Intelligent dynamic range compression</a></li>\n<li><a href=\"http://www.brechtdeman.com/blog/microphone-shootout.html\">Microphone shoot-out</a> (pairwise and multi-stimuli evaluation)</li>\n</ul>\n<p><a href=\"/get-involved/\">PhD Study</a> - interested in joining the team? We are currently accepting PhD applications.</p>\n<h2>Aims</h2>\n<p>We develop <strong>intelligent recording techniques</strong>, for use by audio editors, mixers and sound engineers, which speed up the recording process, minimise preparation for live performance, and enable easy preparation and transmission of high resolution audio.</p>\n<p>Advances in signal processing, machine learning and adaptive systems, have rarely been applied to the professional audio market. This is partly because most digital signal processing applications in these areas have remained focused on replicating or improving those techniques which could be applied in the analogue domain. And until recently, mixing consoles and audio workstations did not have the computing power to allow the introduction of multi-input, multi-output processing tools. Thus audio effects have traditionally been limited to those which operate only on single or stereo channels. There is now an opportunity to develop <strong>advanced audio effects</strong> which analyse all input channels in order to produce the ideal mix.</p>\n<p>Audio engineering for <strong>live sound production</strong> represents a field with strong potential for improvement and automation. Much of the effort of a sound engineer in preparation for a live performance is consumed by tedious, repetitive tasks. Levels must be set to avoid feedback, input channels must be panned to stereo or surround sound, equalisation, normalisation and compression must be applied to each channel, and all equipment must be tested along with establishing an optimal choice of microphone placement. Only after these tasks have been performed, if time and resources permit, may the sound engineer refine these choices to produce an aesthetically pleasing mix which best captures the intended sound. There is a need for tools which minimise sound-checks by automating complex but non-artistic tasks, establish recommended settings based on the input signals and acoustics, and identifying and avoid issues such as acoustic feedback and microphone crossover.</p>\n<p>We develop and test techniques to <strong>convert audio mixes</strong> between formats. We are working to devise methods to automatically create a surround sound mix, which minimise the masking of sources and places sources in positions that are most subjectively pleasing to the listener.</p>\n<p>We investigate <strong>methodologies of audio editing</strong> used by professional sound engineers, in order to better establish best practices and specify the metadata which will be used to enable automation of audio editing.</p>\n<p>We develop <strong>sound synthesis algorithms</strong> in both analogue and digital forms. It is an important application for cinema, multimedia, games and sound installations. It fits within the wider context of sound design, which is the discipline of acquiring, creating and manipulating sounds to achieve a desired effect or mood. Sound synthesis research within the Centre for Digital Music crosses several themes, including Intelligent Sound Engineering and Augmented Instruments. We seek to uncover new synthesis techniques, as well as enhance existing approaches and adapt them to new applications. With a strong emphasis on performance, expression and evaluation, much of our research is focused on real world applications, empowering users and bringing sound synthesis to the forefront of sound design in the creative industries.</p>\n<p>The benefits of these techniques are demonstrated by developing, evaluating and deploying prototype systems for intelligent recording and sound reproduction.\nPersonnel</p>\n<h2>Members</h2>","frontmatter":{"title":""}},"people":{"nodes":[{"id":"cc381a30-c5d2-5ff7-b60d-1f10d81dbf9b","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Alexander Williams","role":"PhD","url":"","acadposition":"PhD Student","blurb":"User-driven deep music generation in digital audio workstations","themes":["mir","audioeng"]}},{"id":"66b87075-3456-5b0c-9dfe-f91b43204096","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Brendan O'Connor","role":"PhD","url":"https://trebolium.github.io/","acadposition":"PhD Student","blurb":"Singing Voice Attribute Transformation","themes":["soundsynthesis","audioeng","mir"]}},{"id":"34ef929b-de59-5c95-a554-597f98d80226","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/83b515844814d95b7fc7d641a74fed63/9cf6c/chinyunyu.jpg","srcSet":"/static/83b515844814d95b7fc7d641a74fed63/2f83b/chinyunyu.jpg 351w,\n/static/83b515844814d95b7fc7d641a74fed63/b41ee/chinyunyu.jpg 702w,\n/static/83b515844814d95b7fc7d641a74fed63/9cf6c/chinyunyu.jpg 1403w","sizes":"(min-width: 1403px) 1403px, 100vw"},"sources":[{"srcSet":"/static/83b515844814d95b7fc7d641a74fed63/9bbac/chinyunyu.webp 351w,\n/static/83b515844814d95b7fc7d641a74fed63/ce9fc/chinyunyu.webp 702w,\n/static/83b515844814d95b7fc7d641a74fed63/f5914/chinyunyu.webp 1403w","type":"image/webp","sizes":"(min-width: 1403px) 1403px, 100vw"}]},"width":1403,"height":1403}}},"name":"Chin-Yun Yu","role":"PhD","url":"https://yoyololicon.github.io/","acadposition":"PhD Student","blurb":"Neural Audio Synthesis with Expressiveness Control","themes":["audioeng","mir","soundsynthesis"]}},{"id":"4dc93e11-7859-5d54-80fd-7f2c652e4c3a","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/144c0a0c0f7932d557eb785c18a2c6ce/3c367/christiansteinmetz.jpg","srcSet":"/static/144c0a0c0f7932d557eb785c18a2c6ce/19e71/christiansteinmetz.jpg 128w,\n/static/144c0a0c0f7932d557eb785c18a2c6ce/68974/christiansteinmetz.jpg 256w,\n/static/144c0a0c0f7932d557eb785c18a2c6ce/3c367/christiansteinmetz.jpg 512w","sizes":"(min-width: 512px) 512px, 100vw"},"sources":[{"srcSet":"/static/144c0a0c0f7932d557eb785c18a2c6ce/6766a/christiansteinmetz.webp 128w,\n/static/144c0a0c0f7932d557eb785c18a2c6ce/22bfc/christiansteinmetz.webp 256w,\n/static/144c0a0c0f7932d557eb785c18a2c6ce/d689f/christiansteinmetz.webp 512w","type":"image/webp","sizes":"(min-width: 512px) 512px, 100vw"}]},"width":512,"height":512}}},"name":"Christian Steinmetz","role":"PhD","url":"https://www.christiansteinmetz.com/","acadposition":"PhD Student","blurb":"End-to-end generative modeling of multitrack mixing with non-parallel data and adversarial networks","themes":["audioeng"]}},{"id":"c210c41b-f202-57d7-ab0c-14538a1c59bc","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#d8d8d8","images":{"fallback":{"src":"/static/48f61b38abf690cd1ce8981afef7140c/5c8f1/christophermitcheltree.jpg","srcSet":"/static/48f61b38abf690cd1ce8981afef7140c/be5ed/christophermitcheltree.jpg 500w,\n/static/48f61b38abf690cd1ce8981afef7140c/5a7c3/christophermitcheltree.jpg 1000w,\n/static/48f61b38abf690cd1ce8981afef7140c/5c8f1/christophermitcheltree.jpg 2000w","sizes":"(min-width: 2000px) 2000px, 100vw"},"sources":[{"srcSet":"/static/48f61b38abf690cd1ce8981afef7140c/5f169/christophermitcheltree.webp 500w,\n/static/48f61b38abf690cd1ce8981afef7140c/3cd29/christophermitcheltree.webp 1000w,\n/static/48f61b38abf690cd1ce8981afef7140c/62c39/christophermitcheltree.webp 2000w","type":"image/webp","sizes":"(min-width: 2000px) 2000px, 100vw"}]},"width":2000,"height":2000}}},"name":"Christopher Mitcheltree","role":"PhD","url":"https://christhetr.ee","acadposition":"PhD Student","blurb":"Representation Learning for Audio Production Style and Modulations","themes":["mlist","audioeng","mir"]}},{"id":"612a6483-8966-5db8-9744-f6fd1021a11a","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#987878","images":{"fallback":{"src":"/static/1811de1c418db6a7f0c4a8727b254b42/6fd26/davidsudholt.png","srcSet":"/static/1811de1c418db6a7f0c4a8727b254b42/b5a32/davidsudholt.png 78w,\n/static/1811de1c418db6a7f0c4a8727b254b42/d8a72/davidsudholt.png 156w,\n/static/1811de1c418db6a7f0c4a8727b254b42/6fd26/davidsudholt.png 312w","sizes":"(min-width: 312px) 312px, 100vw"},"sources":[{"srcSet":"/static/1811de1c418db6a7f0c4a8727b254b42/7a63e/davidsudholt.webp 78w,\n/static/1811de1c418db6a7f0c4a8727b254b42/d1e3d/davidsudholt.webp 156w,\n/static/1811de1c418db6a7f0c4a8727b254b42/da295/davidsudholt.webp 312w","type":"image/webp","sizes":"(min-width: 312px) 312px, 100vw"}]},"width":312,"height":312}}},"name":"David Südholt","role":"PhD","url":"https://dsuedholt.github.io/","acadposition":"PhD Student","blurb":"Machine Learning of Physical Models for Voice Synthesis","themes":["soundsynthesis","audioeng"]}},{"id":"1b6798fe-1d19-52a2-add1-70afa99577f6","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Gary Bromham","role":"PhD","url":"","acadposition":"PhD Student","blurb":"The role of nostalga in music production","themes":["mir","audioeng"]}},{"id":"43e74b5f-21cb-5aa3-8925-294ae05510e0","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#282828","images":{"fallback":{"src":"/static/82ebdfea2d843cadbefef83efbcb21ca/a20e0/jingjingtang.jpg","srcSet":"/static/82ebdfea2d843cadbefef83efbcb21ca/71c2b/jingjingtang.jpg 52w,\n/static/82ebdfea2d843cadbefef83efbcb21ca/ac761/jingjingtang.jpg 104w,\n/static/82ebdfea2d843cadbefef83efbcb21ca/a20e0/jingjingtang.jpg 208w","sizes":"(min-width: 208px) 208px, 100vw"},"sources":[{"srcSet":"/static/82ebdfea2d843cadbefef83efbcb21ca/284ac/jingjingtang.webp 52w,\n/static/82ebdfea2d843cadbefef83efbcb21ca/5ca4c/jingjingtang.webp 104w,\n/static/82ebdfea2d843cadbefef83efbcb21ca/c95dc/jingjingtang.webp 208w","type":"image/webp","sizes":"(min-width: 208px) 208px, 100vw"}]},"width":208,"height":208}}},"name":"Jingjing Tang","role":"PhD","url":"https://www.qmul.ac.uk/eecs/people/profiles/tangjingjing.html","acadposition":"PhD Student","blurb":"End-to-End System Design for Music Style Transfer with Neural Networks","themes":["mcog","audioeng"]}},{"id":"ca2575a9-c402-50c9-a660-eea9648428a6","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8d8f8","images":{"fallback":{"src":"/static/e3f990a7d5022b64b25e5c46e4a6d5bf/47930/joshuadreiss.jpg","srcSet":"/static/e3f990a7d5022b64b25e5c46e4a6d5bf/e07e1/joshuadreiss.jpg 100w,\n/static/e3f990a7d5022b64b25e5c46e4a6d5bf/dd515/joshuadreiss.jpg 200w,\n/static/e3f990a7d5022b64b25e5c46e4a6d5bf/47930/joshuadreiss.jpg 400w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/e3f990a7d5022b64b25e5c46e4a6d5bf/d8057/joshuadreiss.webp 100w,\n/static/e3f990a7d5022b64b25e5c46e4a6d5bf/2e34e/joshuadreiss.webp 200w,\n/static/e3f990a7d5022b64b25e5c46e4a6d5bf/416c3/joshuadreiss.webp 400w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":400}}},"name":"Prof. Joshua D Reiss","role":"Academic","url":"http://www.eecs.qmul.ac.uk/~josh/","acadposition":"Professor of Audio Engineering","blurb":"sound engineering, intelligent audio production, sound synthesis, audio effects, automatic mixing","themes":["audioeng","soundsynthesis"]}},{"id":"3ff53e35-a554-51cd-925e-caa7670bd21b","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8a898","images":{"fallback":{"src":"/static/234763aa1e5f363371e4bfff769cc48d/70fef/katarzynaadamska.jpg","srcSet":"/static/234763aa1e5f363371e4bfff769cc48d/463c6/katarzynaadamska.jpg 379w,\n/static/234763aa1e5f363371e4bfff769cc48d/53da6/katarzynaadamska.jpg 758w,\n/static/234763aa1e5f363371e4bfff769cc48d/70fef/katarzynaadamska.jpg 1516w","sizes":"(min-width: 1516px) 1516px, 100vw"},"sources":[{"srcSet":"/static/234763aa1e5f363371e4bfff769cc48d/d4721/katarzynaadamska.webp 379w,\n/static/234763aa1e5f363371e4bfff769cc48d/6420f/katarzynaadamska.webp 758w,\n/static/234763aa1e5f363371e4bfff769cc48d/79063/katarzynaadamska.webp 1516w","type":"image/webp","sizes":"(min-width: 1516px) 1516px, 100vw"}]},"width":1516,"height":1516}}},"name":"Katarzyna Adamska","role":"PhD","url":"https://www.qmul.ac.uk/eecs/people/profiles/adamskakatarzynamaria.html","acadposition":"PhD Student","blurb":"Predicting hit songs: multimodal and data-driven approach","themes":["mir","audioeng","mcog"]}},{"id":"96844f85-b589-5907-be07-6cc39213f206","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/51634a254670aa2e07b701a172ad9361/8f4a8/marcocomunita.jpg","srcSet":"/static/51634a254670aa2e07b701a172ad9361/82155/marcocomunita.jpg 107w,\n/static/51634a254670aa2e07b701a172ad9361/61790/marcocomunita.jpg 214w,\n/static/51634a254670aa2e07b701a172ad9361/8f4a8/marcocomunita.jpg 428w","sizes":"(min-width: 428px) 428px, 100vw"},"sources":[{"srcSet":"/static/51634a254670aa2e07b701a172ad9361/d9648/marcocomunita.webp 107w,\n/static/51634a254670aa2e07b701a172ad9361/f6ee1/marcocomunita.webp 214w,\n/static/51634a254670aa2e07b701a172ad9361/4ec0e/marcocomunita.webp 428w","type":"image/webp","sizes":"(min-width: 428px) 428px, 100vw"}]},"width":428,"height":428}}},"name":"Marco Comunità","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/comunitamarco.html","acadposition":"PhD Student","blurb":"Machine learning applied to sound synthesis models","themes":["audioeng","soundsynthesis"]}},{"id":"ade5f066-9369-5605-9c73-a41b0112d3cc","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#282828","images":{"fallback":{"src":"/static/d0fca70a51998f2762c511a3a0a987f6/dd515/nellygarcia.jpg","srcSet":"/static/d0fca70a51998f2762c511a3a0a987f6/6ac16/nellygarcia.jpg 50w,\n/static/d0fca70a51998f2762c511a3a0a987f6/e07e1/nellygarcia.jpg 100w,\n/static/d0fca70a51998f2762c511a3a0a987f6/dd515/nellygarcia.jpg 200w","sizes":"(min-width: 200px) 200px, 100vw"},"sources":[{"srcSet":"/static/d0fca70a51998f2762c511a3a0a987f6/dbc4a/nellygarcia.webp 50w,\n/static/d0fca70a51998f2762c511a3a0a987f6/d8057/nellygarcia.webp 100w,\n/static/d0fca70a51998f2762c511a3a0a987f6/2e34e/nellygarcia.webp 200w","type":"image/webp","sizes":"(min-width: 200px) 200px, 100vw"}]},"width":200,"height":200}}},"name":"Nelly Garcia","role":"PhD","url":"","acadposition":"PhD Student","blurb":"An investigation evaluating realism in sound design","themes":["comma","audioeng"]}},{"id":"aec8c261-f5de-537e-8871-b725d92c8ed0","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#787878","images":{"fallback":{"src":"/static/7a140f866e4e9fa0d9f50cfd706a5f9b/a8c60/rodrigomauriciodiazfernandez.jpg","srcSet":"/static/7a140f866e4e9fa0d9f50cfd706a5f9b/67766/rodrigomauriciodiazfernandez.jpg 174w,\n/static/7a140f866e4e9fa0d9f50cfd706a5f9b/28fc8/rodrigomauriciodiazfernandez.jpg 349w,\n/static/7a140f866e4e9fa0d9f50cfd706a5f9b/a8c60/rodrigomauriciodiazfernandez.jpg 697w","sizes":"(min-width: 697px) 697px, 100vw"},"sources":[{"srcSet":"/static/7a140f866e4e9fa0d9f50cfd706a5f9b/0515f/rodrigomauriciodiazfernandez.webp 174w,\n/static/7a140f866e4e9fa0d9f50cfd706a5f9b/546d1/rodrigomauriciodiazfernandez.webp 349w,\n/static/7a140f866e4e9fa0d9f50cfd706a5f9b/dfdb2/rodrigomauriciodiazfernandez.webp 697w","type":"image/webp","sizes":"(min-width: 697px) 697px, 100vw"}]},"width":697,"height":697}}},"name":"Rodrigo Mauricio Diaz Fernandez","role":"PhD","url":"https://www.qmul.ac.uk/eecs/people/profiles/diazfernandezrodrigomauricio.html","acadposition":"PhD Student","blurb":"Hybrid Neural Methods for Sound Synthesis","themes":["audioeng","soundsynthesis"]}},{"id":"64e68bf1-afa9-5242-ba95-3bbb3ff80584","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8d8","images":{"fallback":{"src":"/static/fa7cb1f821b8a5e7891d12048a9bf77a/49d15/soumyavanka.jpg","srcSet":"/static/fa7cb1f821b8a5e7891d12048a9bf77a/18eda/soumyavanka.jpg 133w,\n/static/fa7cb1f821b8a5e7891d12048a9bf77a/e53cc/soumyavanka.jpg 266w,\n/static/fa7cb1f821b8a5e7891d12048a9bf77a/49d15/soumyavanka.jpg 531w","sizes":"(min-width: 531px) 531px, 100vw"},"sources":[{"srcSet":"/static/fa7cb1f821b8a5e7891d12048a9bf77a/a2d02/soumyavanka.webp 133w,\n/static/fa7cb1f821b8a5e7891d12048a9bf77a/fd489/soumyavanka.webp 266w,\n/static/fa7cb1f821b8a5e7891d12048a9bf77a/dc6f9/soumyavanka.webp 531w","type":"image/webp","sizes":"(min-width: 531px) 531px, 100vw"}]},"width":531,"height":531}}},"name":"Soumya Sai Vanka","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/vankasaisoumya.html","acadposition":"PhD Student","blurb":"Music Production Style Transfer and Mix Similarity","themes":["audioeng","mir"]}},{"id":"f0e1d613-f724-56a3-bcb9-30f54b132dcb","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#e8e8e8","images":{"fallback":{"src":"/static/7371426282ff3e012ec154dd11dd684e/c29ce/xiaowanyi.jpg","srcSet":"/static/7371426282ff3e012ec154dd11dd684e/77813/xiaowanyi.jpg 545w,\n/static/7371426282ff3e012ec154dd11dd684e/160ed/xiaowanyi.jpg 1091w,\n/static/7371426282ff3e012ec154dd11dd684e/c29ce/xiaowanyi.jpg 2181w","sizes":"(min-width: 2181px) 2181px, 100vw"},"sources":[{"srcSet":"/static/7371426282ff3e012ec154dd11dd684e/9bbe1/xiaowanyi.webp 545w,\n/static/7371426282ff3e012ec154dd11dd684e/282f9/xiaowanyi.webp 1091w,\n/static/7371426282ff3e012ec154dd11dd684e/630ab/xiaowanyi.webp 2181w","type":"image/webp","sizes":"(min-width: 2181px) 2181px, 100vw"}]},"width":2181,"height":2181}}},"name":"Xiaowan Yi","role":"PhD","url":"https://www.qmul.ac.uk/eecs/people/profiles/yixiaowan.html","acadposition":"PhD Student","blurb":"Composition-aware music recommendation system for music production","themes":["mir","audioeng","isam"]}},{"id":"d964f102-7a03-5e39-8ca5-d62171217446","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8e8","images":{"fallback":{"src":"/static/d9e4891e02c2d5580fb05022babf8891/4ce45/xavierriley.jpg","srcSet":"/static/d9e4891e02c2d5580fb05022babf8891/21980/xavierriley.jpg 167w,\n/static/d9e4891e02c2d5580fb05022babf8891/f560f/xavierriley.jpg 333w,\n/static/d9e4891e02c2d5580fb05022babf8891/4ce45/xavierriley.jpg 666w","sizes":"(min-width: 666px) 666px, 100vw"},"sources":[{"srcSet":"/static/d9e4891e02c2d5580fb05022babf8891/9b205/xavierriley.webp 167w,\n/static/d9e4891e02c2d5580fb05022babf8891/e1538/xavierriley.webp 333w,\n/static/d9e4891e02c2d5580fb05022babf8891/bf451/xavierriley.webp 666w","type":"image/webp","sizes":"(min-width: 666px) 666px, 100vw"}]},"width":666,"height":666}}},"name":"Xavier Riley","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/rileyjohnxavier.html","acadposition":"PhD Student","blurb":"Pitch tracking for music applications - beyond 99% accuracy","themes":["mir","audioeng"]}},{"id":"563606d0-e18c-52ae-a94f-124b16a4f868","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#989898","images":{"fallback":{"src":"/static/b1d39ee566fdd8e6fcb02de5fd0b11d9/ba56e/yazhouli.jpg","srcSet":"/static/b1d39ee566fdd8e6fcb02de5fd0b11d9/64c35/yazhouli.jpg 173w,\n/static/b1d39ee566fdd8e6fcb02de5fd0b11d9/605c6/yazhouli.jpg 347w,\n/static/b1d39ee566fdd8e6fcb02de5fd0b11d9/ba56e/yazhouli.jpg 693w","sizes":"(min-width: 693px) 693px, 100vw"},"sources":[{"srcSet":"/static/b1d39ee566fdd8e6fcb02de5fd0b11d9/68ba0/yazhouli.webp 173w,\n/static/b1d39ee566fdd8e6fcb02de5fd0b11d9/d5fde/yazhouli.webp 347w,\n/static/b1d39ee566fdd8e6fcb02de5fd0b11d9/7c9d1/yazhouli.webp 693w","type":"image/webp","sizes":"(min-width: 693px) 693px, 100vw"}]},"width":693,"height":693}}},"name":"Yazhou Li","role":"PhD","url":"https://www.qmul.ac.uk/eecs/people/profiles/liyazhou.html","acadposition":"PhD Student","blurb":"Virtual Placement of Objects in Acoustic Scenes","themes":["audioeng","soundsynthesis","isam","mlist"]}}]}}}
{"data":{"text":{"html":"<p>With online music stores offering millions of songs to choose from, users need assistance. Using digital signal processing, machine learning, and the semantic web, our research explores new ways of intelligently analysing musical data, and assists people in finding the music they want.</p>\n<p>We have developed systems for automatic playlisting from personal collections (SoundBite), for looking inside the audio (Sonic Visualiser), for hardening/softening transients, and many others. We also regularly release some of our algorithms under Open Source licences, while maintaining a healthy portfolio of patents.</p>\n<p>This area is led by <a href=\"http://www.eecs.qmul.ac.uk/~simond/\">Dr Simon Dixon</a>. Projects in this area include:</p>\n<ul>\n<li>mid-level music descriptors: chords, keys, notes, beats, drums, instrumentation, timbre, structural segmentation, melody</li>\n<li>high-level concepts for music classification, retrieval and knowledge discovery: genre, mood, emotions</li>\n<li><a href=\"http://sonicvisualiser.org\">Sonic Visualser</a></li>\n<li>semantic music analysis for intelligent editing</li>\n<li>linking music-related information and audio data</li>\n<li>interactive auralisation with room impulse responses</li>\n</ul>\n<p><a href=\"/get-involved/\">PhD Study</a> - interested in joining the team? We are currently accepting PhD applications.</p>\n<h2>Members</h2>","frontmatter":{"title":""}},"people":{"nodes":[{"id":"226ac21f-bb0b-56df-9975-41dc5d56d6ca","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8d8e8","images":{"fallback":{"src":"/static/a0048693a658a807cb05a19252bb723c/02dff/adityabhattacharjee.jpg","srcSet":"/static/a0048693a658a807cb05a19252bb723c/0fdf4/adityabhattacharjee.jpg 300w,\n/static/a0048693a658a807cb05a19252bb723c/a89ca/adityabhattacharjee.jpg 600w,\n/static/a0048693a658a807cb05a19252bb723c/02dff/adityabhattacharjee.jpg 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/a0048693a658a807cb05a19252bb723c/078c3/adityabhattacharjee.webp 300w,\n/static/a0048693a658a807cb05a19252bb723c/6d09e/adityabhattacharjee.webp 600w,\n/static/a0048693a658a807cb05a19252bb723c/83805/adityabhattacharjee.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"name":"Aditya Bhattacharjee","role":"PhD","url":"https://www.linkedin.com/in/adibh/","acadposition":"PhD Student","blurb":"Self-supervision in Audio Fingerprinting","themes":["mir","mlist"]}},{"id":"87088d07-80bf-5486-9d68-2bc54fb7d6e9","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#e8f8f8","images":{"fallback":{"src":"/static/543ed3a102028b2ed1952a0a04a3d543/d558d/aidanhogg.jpg","srcSet":"/static/543ed3a102028b2ed1952a0a04a3d543/7fbe3/aidanhogg.jpg 142w,\n/static/543ed3a102028b2ed1952a0a04a3d543/46b1b/aidanhogg.jpg 284w,\n/static/543ed3a102028b2ed1952a0a04a3d543/d558d/aidanhogg.jpg 567w","sizes":"(min-width: 567px) 567px, 100vw"},"sources":[{"srcSet":"/static/543ed3a102028b2ed1952a0a04a3d543/bcdd4/aidanhogg.webp 142w,\n/static/543ed3a102028b2ed1952a0a04a3d543/5d551/aidanhogg.webp 284w,\n/static/543ed3a102028b2ed1952a0a04a3d543/b709e/aidanhogg.webp 567w","type":"image/webp","sizes":"(min-width: 567px) 567px, 100vw"}]},"width":567,"height":567}}},"name":"Dr Aidan Hogg","role":"Academic","url":"https://aidanhogg.uk/","acadposition":"Lecturer in Computer Science","blurb":"spatial and immersive audio, music signal processing, machine learning for audio, music information retrieval","themes":["comma","mir"]}},{"id":"cc381a30-c5d2-5ff7-b60d-1f10d81dbf9b","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Alexander Williams","role":"PhD","url":"","acadposition":"PhD Student","blurb":"User-driven deep music generation in digital audio workstations","themes":["mir","audioeng"]}},{"id":"d35868ef-7cda-5e98-aa09-bb356da9e5d3","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/static/cce06b1e41eda976989e159ecb5db09c/81ef7/andreamartelloni.jpg","srcSet":"/static/cce06b1e41eda976989e159ecb5db09c/96deb/andreamartelloni.jpg 150w,\n/static/cce06b1e41eda976989e159ecb5db09c/9a5ea/andreamartelloni.jpg 299w,\n/static/cce06b1e41eda976989e159ecb5db09c/81ef7/andreamartelloni.jpg 598w","sizes":"(min-width: 598px) 598px, 100vw"},"sources":[{"srcSet":"/static/cce06b1e41eda976989e159ecb5db09c/c65bc/andreamartelloni.webp 150w,\n/static/cce06b1e41eda976989e159ecb5db09c/47ed3/andreamartelloni.webp 299w,\n/static/cce06b1e41eda976989e159ecb5db09c/8c892/andreamartelloni.webp 598w","type":"image/webp","sizes":"(min-width: 598px) 598px, 100vw"}]},"width":598,"height":598}}},"name":"Andrea Martelloni","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/martelloniandrea-1.html","acadposition":"PhD Student","blurb":"Real-Time Gesture Classification on an Augmented Acoustic Guitar using Deep Learning to Improve Extended-Range and Percussive Solo Playing","themes":["mir"]}},{"id":"738dd58b-17cc-53bb-b319-0186faf690cc","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#989888","images":{"fallback":{"src":"/static/09388c6cc31edb4824a849da2f74754c/30f07/andrewedwards.jpg","srcSet":"/static/09388c6cc31edb4824a849da2f74754c/41624/andrewedwards.jpg 160w,\n/static/09388c6cc31edb4824a849da2f74754c/1b894/andrewedwards.jpg 320w,\n/static/09388c6cc31edb4824a849da2f74754c/30f07/andrewedwards.jpg 640w","sizes":"(min-width: 640px) 640px, 100vw"},"sources":[{"srcSet":"/static/09388c6cc31edb4824a849da2f74754c/60b4d/andrewedwards.webp 160w,\n/static/09388c6cc31edb4824a849da2f74754c/5e011/andrewedwards.webp 320w,\n/static/09388c6cc31edb4824a849da2f74754c/90d07/andrewedwards.webp 640w","type":"image/webp","sizes":"(min-width: 640px) 640px, 100vw"}]},"width":640,"height":640}}},"name":"Andrew (Drew) Edwards","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/edwardsandrewcharles.html","acadposition":"PhD Student","blurb":"Deep Learning for Jazz Piano: Transcription + Generative Modeling","themes":["mir","mlist","mcog"]}},{"id":"1f3913f4-5507-56c4-a92a-0782c7b095bb","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#484858","images":{"fallback":{"src":"/static/3d59fe8293835010c7ad4f180e4cc954/dd515/annaxambo.jpg","srcSet":"/static/3d59fe8293835010c7ad4f180e4cc954/6ac16/annaxambo.jpg 50w,\n/static/3d59fe8293835010c7ad4f180e4cc954/e07e1/annaxambo.jpg 100w,\n/static/3d59fe8293835010c7ad4f180e4cc954/dd515/annaxambo.jpg 200w","sizes":"(min-width: 200px) 200px, 100vw"},"sources":[{"srcSet":"/static/3d59fe8293835010c7ad4f180e4cc954/dbc4a/annaxambo.webp 50w,\n/static/3d59fe8293835010c7ad4f180e4cc954/d8057/annaxambo.webp 100w,\n/static/3d59fe8293835010c7ad4f180e4cc954/2e34e/annaxambo.webp 200w","type":"image/webp","sizes":"(min-width: 200px) 200px, 100vw"}]},"width":200,"height":200}}},"name":"Dr Anna Xamb√≥","role":"Academic","url":"https://annaxambo.me/","acadposition":"Senior Lecturer in Sound and Music Computing","blurb":"new interfaces for musical expression, performance study, human-computer interaction, interaction design","themes":["augmi","mir","isam"]}},{"id":"3e2bbfc8-0f9b-5aac-ba55-b0f8ffc899b9","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#282828","images":{"fallback":{"src":"/static/92ff73f501424e00ed9ec016481170c9/7706b/berkerbanar.jpg","srcSet":"/static/92ff73f501424e00ed9ec016481170c9/96deb/berkerbanar.jpg 150w,\n/static/92ff73f501424e00ed9ec016481170c9/ed539/berkerbanar.jpg 301w,\n/static/92ff73f501424e00ed9ec016481170c9/7706b/berkerbanar.jpg 601w","sizes":"(min-width: 601px) 601px, 100vw"},"sources":[{"srcSet":"/static/92ff73f501424e00ed9ec016481170c9/c65bc/berkerbanar.webp 150w,\n/static/92ff73f501424e00ed9ec016481170c9/214a8/berkerbanar.webp 301w,\n/static/92ff73f501424e00ed9ec016481170c9/2b014/berkerbanar.webp 601w","type":"image/webp","sizes":"(min-width: 601px) 601px, 100vw"}]},"width":601,"height":601}}},"name":"Berker Banar","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/banarberker.html","acadposition":"PhD Student","blurb":"Towards Composing Contemporary Classical Music using Generative Deep Learning","themes":["mir","soundsynthesis"]}},{"id":"9a6273ce-dc95-5d23-b5f9-7bcebfdc1772","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#685848","images":{"fallback":{"src":"/static/c0f574ea604c6ff9084a488c4bc7e4e5/0be83/ashleynoelhirst.png","srcSet":"/static/c0f574ea604c6ff9084a488c4bc7e4e5/5f035/ashleynoelhirst.png 320w,\n/static/c0f574ea604c6ff9084a488c4bc7e4e5/eadd3/ashleynoelhirst.png 640w,\n/static/c0f574ea604c6ff9084a488c4bc7e4e5/0be83/ashleynoelhirst.png 1280w","sizes":"(min-width: 1280px) 1280px, 100vw"},"sources":[{"srcSet":"/static/c0f574ea604c6ff9084a488c4bc7e4e5/5e011/ashleynoelhirst.webp 320w,\n/static/c0f574ea604c6ff9084a488c4bc7e4e5/90d07/ashleynoelhirst.webp 640w,\n/static/c0f574ea604c6ff9084a488c4bc7e4e5/9e21f/ashleynoelhirst.webp 1280w","type":"image/webp","sizes":"(min-width: 1280px) 1280px, 100vw"}]},"width":1280,"height":1280}}},"name":"Ashley Noel-Hirst","role":"PhD","url":"https://ashleynoelhirst.co.uk/","acadposition":"PhD Student","blurb":"Latent Spaces for Human-AI music generation","themes":["mir","isam"]}},{"id":"4f0a9d6f-a76c-5668-832e-de53c816bfd5","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#a88878","images":{"fallback":{"src":"/static/e2494d25fff51e99ef73d11bf688ea88/be4dc/careybunks.png","srcSet":"/static/e2494d25fff51e99ef73d11bf688ea88/d8938/careybunks.png 159w,\n/static/e2494d25fff51e99ef73d11bf688ea88/2655f/careybunks.png 318w,\n/static/e2494d25fff51e99ef73d11bf688ea88/be4dc/careybunks.png 636w","sizes":"(min-width: 636px) 636px, 100vw"},"sources":[{"srcSet":"/static/e2494d25fff51e99ef73d11bf688ea88/0993c/careybunks.webp 159w,\n/static/e2494d25fff51e99ef73d11bf688ea88/4ce64/careybunks.webp 318w,\n/static/e2494d25fff51e99ef73d11bf688ea88/1852a/careybunks.webp 636w","type":"image/webp","sizes":"(min-width: 636px) 636px, 100vw"}]},"width":636,"height":636}}},"name":"Carey Bunks","role":"PhD","url":"","acadposition":"PhD Student","blurb":"Cover Song Identification","themes":["mir"]}},{"id":"34ef929b-de59-5c95-a554-597f98d80226","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/83b515844814d95b7fc7d641a74fed63/9cf6c/chinyunyu.jpg","srcSet":"/static/83b515844814d95b7fc7d641a74fed63/2f83b/chinyunyu.jpg 351w,\n/static/83b515844814d95b7fc7d641a74fed63/b41ee/chinyunyu.jpg 702w,\n/static/83b515844814d95b7fc7d641a74fed63/9cf6c/chinyunyu.jpg 1403w","sizes":"(min-width: 1403px) 1403px, 100vw"},"sources":[{"srcSet":"/static/83b515844814d95b7fc7d641a74fed63/9bbac/chinyunyu.webp 351w,\n/static/83b515844814d95b7fc7d641a74fed63/ce9fc/chinyunyu.webp 702w,\n/static/83b515844814d95b7fc7d641a74fed63/f5914/chinyunyu.webp 1403w","type":"image/webp","sizes":"(min-width: 1403px) 1403px, 100vw"}]},"width":1403,"height":1403}}},"name":"Chin-Yun Yu","role":"PhD","url":"https://yoyololicon.github.io/","acadposition":"PhD Student","blurb":"Neural Audio Synthesis with Expressiveness Control","themes":["audioeng","mir","soundsynthesis"]}},{"id":"c210c41b-f202-57d7-ab0c-14538a1c59bc","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#d8d8d8","images":{"fallback":{"src":"/static/48f61b38abf690cd1ce8981afef7140c/5c8f1/christophermitcheltree.jpg","srcSet":"/static/48f61b38abf690cd1ce8981afef7140c/be5ed/christophermitcheltree.jpg 500w,\n/static/48f61b38abf690cd1ce8981afef7140c/5a7c3/christophermitcheltree.jpg 1000w,\n/static/48f61b38abf690cd1ce8981afef7140c/5c8f1/christophermitcheltree.jpg 2000w","sizes":"(min-width: 2000px) 2000px, 100vw"},"sources":[{"srcSet":"/static/48f61b38abf690cd1ce8981afef7140c/5f169/christophermitcheltree.webp 500w,\n/static/48f61b38abf690cd1ce8981afef7140c/3cd29/christophermitcheltree.webp 1000w,\n/static/48f61b38abf690cd1ce8981afef7140c/62c39/christophermitcheltree.webp 2000w","type":"image/webp","sizes":"(min-width: 2000px) 2000px, 100vw"}]},"width":2000,"height":2000}}},"name":"Christopher Mitcheltree","role":"PhD","url":"https://christhetr.ee","acadposition":"PhD Student","blurb":"Representation Learning for Audio Production Style and Modulations","themes":["mlist","audioeng","mir"]}},{"id":"1e774e43-fee0-57f5-b2b5-632fe8762c2a","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#988878","images":{"fallback":{"src":"/static/81f0d5ca330795a7366c18a6c1d1a10a/47faa/cyrusvahidi.jpg","srcSet":"/static/81f0d5ca330795a7366c18a6c1d1a10a/fcf11/cyrusvahidi.jpg 306w,\n/static/81f0d5ca330795a7366c18a6c1d1a10a/04fa2/cyrusvahidi.jpg 612w,\n/static/81f0d5ca330795a7366c18a6c1d1a10a/47faa/cyrusvahidi.jpg 1224w","sizes":"(min-width: 1224px) 1224px, 100vw"},"sources":[{"srcSet":"/static/81f0d5ca330795a7366c18a6c1d1a10a/c51a2/cyrusvahidi.webp 306w,\n/static/81f0d5ca330795a7366c18a6c1d1a10a/c089a/cyrusvahidi.webp 612w,\n/static/81f0d5ca330795a7366c18a6c1d1a10a/23828/cyrusvahidi.webp 1224w","type":"image/webp","sizes":"(min-width: 1224px) 1224px, 100vw"}]},"width":1224,"height":1224}}},"name":"Cyrus Vahidi","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/vahidicyrus.html","acadposition":"PhD Student","blurb":"Perceptual end to end learning for music understanding","themes":["mir"]}},{"id":"f855554f-b1f4-56be-8d59-801f8b9664fe","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8e8c8","images":{"fallback":{"src":"/static/c7d60cc8efa14a13b6dbba4bc7ccc137/a20e0/davefoster.jpg","srcSet":"/static/c7d60cc8efa14a13b6dbba4bc7ccc137/71c2b/davefoster.jpg 52w,\n/static/c7d60cc8efa14a13b6dbba4bc7ccc137/ac761/davefoster.jpg 104w,\n/static/c7d60cc8efa14a13b6dbba4bc7ccc137/a20e0/davefoster.jpg 208w","sizes":"(min-width: 208px) 208px, 100vw"},"sources":[{"srcSet":"/static/c7d60cc8efa14a13b6dbba4bc7ccc137/284ac/davefoster.webp 52w,\n/static/c7d60cc8efa14a13b6dbba4bc7ccc137/5ca4c/davefoster.webp 104w,\n/static/c7d60cc8efa14a13b6dbba4bc7ccc137/c95dc/davefoster.webp 208w","type":"image/webp","sizes":"(min-width: 208px) 208px, 100vw"}]},"width":208,"height":208}}},"name":"David Foster","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/fosterdavid.html","acadposition":"PhD Student","blurb":"Modelling the Creative Process of Jazz Improvisation","themes":["mir"]}},{"id":"7b043f0b-de2f-5155-9877-1dfff2b8c117","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/87ab058ab4e8a08974fd4a9a22ef1bac/36a05/elonashatri.jpg","srcSet":"/static/87ab058ab4e8a08974fd4a9a22ef1bac/a46b0/elonashatri.jpg 205w,\n/static/87ab058ab4e8a08974fd4a9a22ef1bac/3e7a6/elonashatri.jpg 410w,\n/static/87ab058ab4e8a08974fd4a9a22ef1bac/36a05/elonashatri.jpg 819w","sizes":"(min-width: 819px) 819px, 100vw"},"sources":[{"srcSet":"/static/87ab058ab4e8a08974fd4a9a22ef1bac/24bbe/elonashatri.webp 205w,\n/static/87ab058ab4e8a08974fd4a9a22ef1bac/d883f/elonashatri.webp 410w,\n/static/87ab058ab4e8a08974fd4a9a22ef1bac/8f6df/elonashatri.webp 819w","type":"image/webp","sizes":"(min-width: 819px) 819px, 100vw"}]},"width":819,"height":819}}},"name":"Elona Shatri","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/shatrielona-1.html","acadposition":"PhD Student","blurb":"Optical music recognition using deep learning","themes":["mir"]}},{"id":"ef8d4761-ec9b-58f0-bfa5-26159c60c38e","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/8106f707dd10b39434bd1c08f1f22f6d/30f07/emmanouilbenetos.jpg","srcSet":"/static/8106f707dd10b39434bd1c08f1f22f6d/41624/emmanouilbenetos.jpg 160w,\n/static/8106f707dd10b39434bd1c08f1f22f6d/1b894/emmanouilbenetos.jpg 320w,\n/static/8106f707dd10b39434bd1c08f1f22f6d/30f07/emmanouilbenetos.jpg 640w","sizes":"(min-width: 640px) 640px, 100vw"},"sources":[{"srcSet":"/static/8106f707dd10b39434bd1c08f1f22f6d/60b4d/emmanouilbenetos.webp 160w,\n/static/8106f707dd10b39434bd1c08f1f22f6d/5e011/emmanouilbenetos.webp 320w,\n/static/8106f707dd10b39434bd1c08f1f22f6d/90d07/emmanouilbenetos.webp 640w","type":"image/webp","sizes":"(min-width: 640px) 640px, 100vw"}]},"width":640,"height":640}}},"name":"Dr Emmanouil Benetos","role":"Academic","url":"http://www.eecs.qmul.ac.uk/people/view/4741/dr-emmanouil-benetos","acadposition":"Reader in Machine Listening","blurb":"Machine listening / computer audition, Machine learning for audio and sequential data, Music information retrieval, Multimodal AI, Resource-efficient AI","themes":["mir","mlist"]}},{"id":"1b6798fe-1d19-52a2-add1-70afa99577f6","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Gary Bromham","role":"PhD","url":"","acadposition":"PhD Student","blurb":"The role of nostalga in music production","themes":["mir","audioeng"]}},{"id":"9aa31664-07ac-5194-a7fe-146cab80fddb","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#a8b8e8","images":{"fallback":{"src":"/static/eaf04fbbdffdb3aca0caed16cfdf1102/7900e/gyorgyfazekas.jpg","srcSet":"/static/eaf04fbbdffdb3aca0caed16cfdf1102/f713e/gyorgyfazekas.jpg 55w,\n/static/eaf04fbbdffdb3aca0caed16cfdf1102/e2afd/gyorgyfazekas.jpg 110w,\n/static/eaf04fbbdffdb3aca0caed16cfdf1102/7900e/gyorgyfazekas.jpg 219w","sizes":"(min-width: 219px) 219px, 100vw"},"sources":[{"srcSet":"/static/eaf04fbbdffdb3aca0caed16cfdf1102/938d3/gyorgyfazekas.webp 55w,\n/static/eaf04fbbdffdb3aca0caed16cfdf1102/8c6ff/gyorgyfazekas.webp 110w,\n/static/eaf04fbbdffdb3aca0caed16cfdf1102/3f57d/gyorgyfazekas.webp 219w","type":"image/webp","sizes":"(min-width: 219px) 219px, 100vw"}]},"width":219,"height":219}}},"name":"Dr George Fazekas","role":"Academic","url":"http://eecs.qmul.ac.uk/~gyorgyf","acadposition":"Senior Lecturer","blurb":"Semantic Audio, Music Information Retrieval, Semantic Web for Music, Machine Learning and Data Science, Music Emotion Recognition, Interactive music sytems (e.g. intellignet editing, audio production and performance systems)","themes":["mir"]}},{"id":"b71f6653-2c4e-5f2b-b283-2eb0ae2679b6","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/78d403f536869a8d3f69509587dc97e9/192dd/harnickhera.png","srcSet":"/static/78d403f536869a8d3f69509587dc97e9/b024c/harnickhera.png 65w,\n/static/78d403f536869a8d3f69509587dc97e9/68010/harnickhera.png 129w,\n/static/78d403f536869a8d3f69509587dc97e9/192dd/harnickhera.png 258w","sizes":"(min-width: 258px) 258px, 100vw"},"sources":[{"srcSet":"/static/78d403f536869a8d3f69509587dc97e9/0c531/harnickhera.webp 65w,\n/static/78d403f536869a8d3f69509587dc97e9/f6a08/harnickhera.webp 129w,\n/static/78d403f536869a8d3f69509587dc97e9/c7cc5/harnickhera.webp 258w","type":"image/webp","sizes":"(min-width: 258px) 258px, 100vw"}]},"width":258,"height":258}}},"name":"Harnick Khera","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/kheraharnicksingh.html","acadposition":"PhD Student","blurb":"Informed source separation for multi-mic production","themes":["mir","mlist"]}},{"id":"1918386c-730c-5e3e-9d42-0369c7ed79b1","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#88b8f8","images":{"fallback":{"src":"/static/56fdf9525659e33f14b4049213e7f4a2/70617/huanzhang.jpg","srcSet":"/static/56fdf9525659e33f14b4049213e7f4a2/f2f05/huanzhang.jpg 146w,\n/static/56fdf9525659e33f14b4049213e7f4a2/b179d/huanzhang.jpg 291w,\n/static/56fdf9525659e33f14b4049213e7f4a2/70617/huanzhang.jpg 582w","sizes":"(min-width: 582px) 582px, 100vw"},"sources":[{"srcSet":"/static/56fdf9525659e33f14b4049213e7f4a2/9027a/huanzhang.webp 146w,\n/static/56fdf9525659e33f14b4049213e7f4a2/46435/huanzhang.webp 291w,\n/static/56fdf9525659e33f14b4049213e7f4a2/f27b2/huanzhang.webp 582w","type":"image/webp","sizes":"(min-width: 582px) 582px, 100vw"}]},"width":582,"height":582}}},"name":"Huan Zhang","role":"PhD","url":"http://eecs.qmul.ac.uk/people/profiles/zhanghuan.html","acadposition":"PhD Student","blurb":"Computational Modelling of Expressive Piano Performance","themes":["mir"]}},{"id":"8f618d2e-72c3-5ff1-a65f-f14410f7ea3e","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#d88868","images":{"fallback":{"src":"/static/5ebddded3dd854a7fb181cab4c068dec/97a19/ilariamanco.jpg","srcSet":"/static/5ebddded3dd854a7fb181cab4c068dec/73bb6/ilariamanco.jpg 120w,\n/static/5ebddded3dd854a7fb181cab4c068dec/f9edd/ilariamanco.jpg 240w,\n/static/5ebddded3dd854a7fb181cab4c068dec/97a19/ilariamanco.jpg 480w","sizes":"(min-width: 480px) 480px, 100vw"},"sources":[{"srcSet":"/static/5ebddded3dd854a7fb181cab4c068dec/507b0/ilariamanco.webp 120w,\n/static/5ebddded3dd854a7fb181cab4c068dec/8d565/ilariamanco.webp 240w,\n/static/5ebddded3dd854a7fb181cab4c068dec/21b1a/ilariamanco.webp 480w","type":"image/webp","sizes":"(min-width: 480px) 480px, 100vw"}]},"width":480,"height":480}}},"name":"Ilaria Manco","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/mancoilaria.html","acadposition":"PhD Student","blurb":"Multimodal Deep Learning for Music Information Retrieval","themes":["mir","mlist"]}},{"id":"d556fbfe-4709-5229-a169-86adac5a47c3","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Hyon Kim","role":"Visitor","url":"https://scholar.google.com/citations?user=2XHZIuUAAAAJ&hl=en","acadposition":"Universitat Pompeu Fabra","blurb":"Automated Music Performance Assessment and Critique","themes":["mir"]}},{"id":"27c0231d-35a2-57d5-8273-8562fba3319f","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Iacopo Ghinassi","role":"PhD","url":"https://github.com/Ighina","acadposition":"PhD Student","blurb":"Semantic understanding of TV programme content and structure to enable automatic enhancement and adjustment","themes":["mir"]}},{"id":"84af4731-c9f4-52e0-80a0-42d0f5b94f55","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/0d574bf24113faca635dd75ded6cc9b4/b74b1/ioannisvasilakis.jpg","srcSet":"/static/0d574bf24113faca635dd75ded6cc9b4/d3fc0/ioannisvasilakis.jpg 270w,\n/static/0d574bf24113faca635dd75ded6cc9b4/19455/ioannisvasilakis.jpg 540w,\n/static/0d574bf24113faca635dd75ded6cc9b4/b74b1/ioannisvasilakis.jpg 1080w","sizes":"(min-width: 1080px) 1080px, 100vw"},"sources":[{"srcSet":"/static/0d574bf24113faca635dd75ded6cc9b4/ede49/ioannisvasilakis.webp 270w,\n/static/0d574bf24113faca635dd75ded6cc9b4/4cb34/ioannisvasilakis.webp 540w,\n/static/0d574bf24113faca635dd75ded6cc9b4/4f506/ioannisvasilakis.webp 1080w","type":"image/webp","sizes":"(min-width: 1080px) 1080px, 100vw"}]},"width":1080,"height":1080}}},"name":"Yannis (John) Vasilakis","role":"PhD","url":"https://www.linkedin.com/in/yannis-vasilakis-6bb9b11b1/","acadposition":"PhD Student","blurb":"Active Learning for Interactive Music Transcription","themes":["mlist","mir"]}},{"id":"be6e7a39-4760-57e6-a607-f988d7a322da","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#c8a878","images":{"fallback":{"src":"/static/28c5745e61741b22792aa6d4a135584a/68974/iranroman.jpg","srcSet":"/static/28c5745e61741b22792aa6d4a135584a/d4a57/iranroman.jpg 64w,\n/static/28c5745e61741b22792aa6d4a135584a/19e71/iranroman.jpg 128w,\n/static/28c5745e61741b22792aa6d4a135584a/68974/iranroman.jpg 256w","sizes":"(min-width: 256px) 256px, 100vw"},"sources":[{"srcSet":"/static/28c5745e61741b22792aa6d4a135584a/8257c/iranroman.webp 64w,\n/static/28c5745e61741b22792aa6d4a135584a/6766a/iranroman.webp 128w,\n/static/28c5745e61741b22792aa6d4a135584a/22bfc/iranroman.webp 256w","type":"image/webp","sizes":"(min-width: 256px) 256px, 100vw"}]},"width":256,"height":256}}},"name":"Dr Iran Roman","role":"Academic","url":"https://iranroman.github.io/","acadposition":"Lecturer in Sound and Music Computing","blurb":"theoretical neuroscience, machine perception, artificial intelligence","themes":["comma","mir","mlist"]}},{"id":"8ee3a5c5-91a8-5ec8-8d03-f636b18d244d","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/c0f12a01968f7a264dd0c78e6139f6c0/f9edd/ivanhiggs.jpg","srcSet":"/static/c0f12a01968f7a264dd0c78e6139f6c0/93848/ivanhiggs.jpg 60w,\n/static/c0f12a01968f7a264dd0c78e6139f6c0/73bb6/ivanhiggs.jpg 120w,\n/static/c0f12a01968f7a264dd0c78e6139f6c0/f9edd/ivanhiggs.jpg 240w","sizes":"(min-width: 240px) 240px, 100vw"},"sources":[{"srcSet":"/static/c0f12a01968f7a264dd0c78e6139f6c0/927d1/ivanhiggs.webp 60w,\n/static/c0f12a01968f7a264dd0c78e6139f6c0/507b0/ivanhiggs.webp 120w,\n/static/c0f12a01968f7a264dd0c78e6139f6c0/8d565/ivanhiggs.webp 240w","type":"image/webp","sizes":"(min-width: 240px) 240px, 100vw"}]},"width":240,"height":240}}},"name":"Ivan Meresman Higgs","role":"Research Assistant","url":"https://github.com/ivanlmh","acadposition":"Research Assistant","blurb":"Sample Identification in Mastered Songs using Deep Learning Methods","themes":["mir"]}},{"id":"623d083c-bec9-58ac-b75f-deed805850e4","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#a89888","images":{"fallback":{"src":"/static/6dd5cb05357d20e63e726634c9ff6079/035db/jamesbolt.jpg","srcSet":"/static/6dd5cb05357d20e63e726634c9ff6079/0f5ce/jamesbolt.jpg 750w,\n/static/6dd5cb05357d20e63e726634c9ff6079/1f55a/jamesbolt.jpg 1500w,\n/static/6dd5cb05357d20e63e726634c9ff6079/035db/jamesbolt.jpg 3000w","sizes":"(min-width: 3000px) 3000px, 100vw"},"sources":[{"srcSet":"/static/6dd5cb05357d20e63e726634c9ff6079/4f03f/jamesbolt.webp 750w,\n/static/6dd5cb05357d20e63e726634c9ff6079/07142/jamesbolt.webp 1500w,\n/static/6dd5cb05357d20e63e726634c9ff6079/19b60/jamesbolt.webp 3000w","type":"image/webp","sizes":"(min-width: 3000px) 3000px, 100vw"}]},"width":3000,"height":3000}}},"name":"James Bolt","role":"PhD","url":"https://www.qmul.ac.uk/eecs/people/profiles/jamesbolt.html","acadposition":"PhD Student","blurb":"Intelligent audio and music editing with deep learning","themes":["mir"]}},{"id":"8209c891-60bc-5d31-b5de-2e332d1ae01c","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#081808","images":{"fallback":{"src":"/static/335b628a1c69b433d57e9201845b16d7/37851/jazasyed.jpg","srcSet":"/static/335b628a1c69b433d57e9201845b16d7/5b1fa/jazasyed.jpg 115w,\n/static/335b628a1c69b433d57e9201845b16d7/f0f53/jazasyed.jpg 230w,\n/static/335b628a1c69b433d57e9201845b16d7/37851/jazasyed.jpg 460w","sizes":"(min-width: 460px) 460px, 100vw"},"sources":[{"srcSet":"/static/335b628a1c69b433d57e9201845b16d7/f8466/jazasyed.webp 115w,\n/static/335b628a1c69b433d57e9201845b16d7/84992/jazasyed.webp 230w,\n/static/335b628a1c69b433d57e9201845b16d7/b5c5b/jazasyed.webp 460w","type":"image/webp","sizes":"(min-width: 460px) 460px, 100vw"}]},"width":460,"height":460}}},"name":"Jaza Syed","role":"Research Assistant","url":"https://jaza.xyz/","acadposition":"Research Assistant","blurb":"Audio ML, Automatic Lyrics Transcription","themes":["mir"]}},{"id":"b3e3ac4a-cd06-5d69-8ac9-f47d73ef5c0b","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8a898","images":{"fallback":{"src":"/static/5c4ebc68a7723fc56115ed3fe749ff7a/96deb/johanpauwels.jpg","srcSet":"/static/5c4ebc68a7723fc56115ed3fe749ff7a/c81d1/johanpauwels.jpg 38w,\n/static/5c4ebc68a7723fc56115ed3fe749ff7a/91a6d/johanpauwels.jpg 75w,\n/static/5c4ebc68a7723fc56115ed3fe749ff7a/96deb/johanpauwels.jpg 150w","sizes":"(min-width: 150px) 150px, 100vw"},"sources":[{"srcSet":"/static/5c4ebc68a7723fc56115ed3fe749ff7a/0852d/johanpauwels.webp 38w,\n/static/5c4ebc68a7723fc56115ed3fe749ff7a/18188/johanpauwels.webp 75w,\n/static/5c4ebc68a7723fc56115ed3fe749ff7a/c65bc/johanpauwels.webp 150w","type":"image/webp","sizes":"(min-width: 150px) 150px, 100vw"}]},"width":150,"height":150}}},"name":"Dr Johan Pauwels","role":"Academic","url":"http://www.eecs.qmul.ac.uk/people/view/50775/johan-pauwels","acadposition":"Lecturer in Audio Signal Processing","blurb":"automatic music labelling, music information retrieval, music signal processing, machine learning for audio, chord/key/structure (joint) estimation, instrument identification, multi-track/channel audio, music transcription, graphical models, big data science","themes":["mir","mlist"]}},{"id":"3ff53e35-a554-51cd-925e-caa7670bd21b","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8a898","images":{"fallback":{"src":"/static/234763aa1e5f363371e4bfff769cc48d/70fef/katarzynaadamska.jpg","srcSet":"/static/234763aa1e5f363371e4bfff769cc48d/463c6/katarzynaadamska.jpg 379w,\n/static/234763aa1e5f363371e4bfff769cc48d/53da6/katarzynaadamska.jpg 758w,\n/static/234763aa1e5f363371e4bfff769cc48d/70fef/katarzynaadamska.jpg 1516w","sizes":"(min-width: 1516px) 1516px, 100vw"},"sources":[{"srcSet":"/static/234763aa1e5f363371e4bfff769cc48d/d4721/katarzynaadamska.webp 379w,\n/static/234763aa1e5f363371e4bfff769cc48d/6420f/katarzynaadamska.webp 758w,\n/static/234763aa1e5f363371e4bfff769cc48d/79063/katarzynaadamska.webp 1516w","type":"image/webp","sizes":"(min-width: 1516px) 1516px, 100vw"}]},"width":1516,"height":1516}}},"name":"Katarzyna Adamska","role":"PhD","url":"https://www.qmul.ac.uk/eecs/people/profiles/adamskakatarzynamaria.html","acadposition":"PhD Student","blurb":"Predicting hit songs: multimodal and data-driven approach","themes":["mir","audioeng","mcog"]}},{"id":"12a808a3-c5af-5b8f-927d-9078a2f03d06","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/9ec9117c4899bc5c3dd3148ac5489686/d4aca/leleliu.jpg","srcSet":"/static/9ec9117c4899bc5c3dd3148ac5489686/f9edd/leleliu.jpg 240w,\n/static/9ec9117c4899bc5c3dd3148ac5489686/97a19/leleliu.jpg 480w,\n/static/9ec9117c4899bc5c3dd3148ac5489686/d4aca/leleliu.jpg 960w","sizes":"(min-width: 960px) 960px, 100vw"},"sources":[{"srcSet":"/static/9ec9117c4899bc5c3dd3148ac5489686/8d565/leleliu.webp 240w,\n/static/9ec9117c4899bc5c3dd3148ac5489686/21b1a/leleliu.webp 480w,\n/static/9ec9117c4899bc5c3dd3148ac5489686/d6f60/leleliu.webp 960w","type":"image/webp","sizes":"(min-width: 960px) 960px, 100vw"}]},"width":960,"height":960}}},"name":"Lele Liu","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/liulele.html","acadposition":"PhD Student","blurb":"Automatic music transcription with end-to-end deep neural networks","themes":["mir","mlist"]}},{"id":"aaea13f6-1fd2-57d5-b574-a5cc0cee002d","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#3898d8","images":{"fallback":{"src":"/static/7712519d3052de12533c72fb8d0992e0/73bb6/linwang.jpg","srcSet":"/static/7712519d3052de12533c72fb8d0992e0/3c559/linwang.jpg 30w,\n/static/7712519d3052de12533c72fb8d0992e0/93848/linwang.jpg 60w,\n/static/7712519d3052de12533c72fb8d0992e0/73bb6/linwang.jpg 120w","sizes":"(min-width: 120px) 120px, 100vw"},"sources":[{"srcSet":"/static/7712519d3052de12533c72fb8d0992e0/bde72/linwang.webp 30w,\n/static/7712519d3052de12533c72fb8d0992e0/927d1/linwang.webp 60w,\n/static/7712519d3052de12533c72fb8d0992e0/507b0/linwang.webp 120w","type":"image/webp","sizes":"(min-width: 120px) 120px, 100vw"}]},"width":120,"height":120}}},"name":"Dr Lin Wang","role":"Academic","url":"http://www.eecs.qmul.ac.uk/~linwang/","acadposition":"Lecturer in Applied Data Science and Signal Processing","blurb":"signal processing; machine learning; robot perception","themes":["mlist","mir"]}},{"id":"9b6c0163-cfdc-560b-82fe-8b8853e02502","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8e8","images":{"fallback":{"src":"/static/dba175a523b3c9d78f62ce6131df86ad/a20e0/marypilataki.jpg","srcSet":"/static/dba175a523b3c9d78f62ce6131df86ad/71c2b/marypilataki.jpg 52w,\n/static/dba175a523b3c9d78f62ce6131df86ad/ac761/marypilataki.jpg 104w,\n/static/dba175a523b3c9d78f62ce6131df86ad/a20e0/marypilataki.jpg 208w","sizes":"(min-width: 208px) 208px, 100vw"},"sources":[{"srcSet":"/static/dba175a523b3c9d78f62ce6131df86ad/284ac/marypilataki.webp 52w,\n/static/dba175a523b3c9d78f62ce6131df86ad/5ca4c/marypilataki.webp 104w,\n/static/dba175a523b3c9d78f62ce6131df86ad/c95dc/marypilataki.webp 208w","type":"image/webp","sizes":"(min-width: 208px) 208px, 100vw"}]},"width":208,"height":208}}},"name":"Mary Pilataki","role":"PhD","url":"https://github.com/marypilataki","acadposition":"PhD Student","blurb":"Deep Learning methods for Multi-Instrument Music Transcription","themes":["mir"]}},{"id":"3fffc866-8036-535d-8b12-673653a1e811","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#d8c8c8","images":{"fallback":{"src":"/static/22312dadef02a15635234aa9c85e6a6b/6f467/marksandler.jpg","srcSet":"/static/22312dadef02a15635234aa9c85e6a6b/99d04/marksandler.jpg 247w,\n/static/22312dadef02a15635234aa9c85e6a6b/72fdf/marksandler.jpg 494w,\n/static/22312dadef02a15635234aa9c85e6a6b/6f467/marksandler.jpg 988w","sizes":"(min-width: 988px) 988px, 100vw"},"sources":[{"srcSet":"/static/22312dadef02a15635234aa9c85e6a6b/f19fb/marksandler.webp 247w,\n/static/22312dadef02a15635234aa9c85e6a6b/dab72/marksandler.webp 494w,\n/static/22312dadef02a15635234aa9c85e6a6b/3c8a3/marksandler.webp 988w","type":"image/webp","sizes":"(min-width: 988px) 988px, 100vw"}]},"width":988,"height":988}}},"name":"Prof Mark Sandler ","role":"Academic","url":"http://www.eecs.qmul.ac.uk/people/view/3114/prof-mark-sandler","acadposition":"C4DM Director","blurb":"Digital Signal Processing, Digital Audio, Music Informatics, Audio Features, Semantic Audio, Immersive Audio, Studio Science, Music Data Science, Music Linked Data.","themes":["mir"]}},{"id":"d74e8c2b-56f9-50cf-a43b-8b0191803bff","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/661d84eba8a4676c5210510bd4b2c4b6/93106/mathieubarthet.webp","srcSet":"/static/661d84eba8a4676c5210510bd4b2c4b6/c23ec/mathieubarthet.webp 123w,\n/static/661d84eba8a4676c5210510bd4b2c4b6/02ea5/mathieubarthet.webp 246w,\n/static/661d84eba8a4676c5210510bd4b2c4b6/93106/mathieubarthet.webp 492w","sizes":"(min-width: 492px) 492px, 100vw"},"sources":[]},"width":492,"height":492}}},"name":"Dr Mathieu Barthet","role":"Academic","url":"http://www.eecs.qmul.ac.uk/people/view/4808/dr-mathieu-barthet","acadposition":"Senior Lecturer in Digital Media","blurb":"Music information research, Internet of musical things, Extended reality, New interfaces for musical expression, Semantic audio, Music perception (timbre, emotions), Audience-Performer interaction, Participatory art","themes":["mir","augmi","isam","mupae"]}},{"id":"680dbd18-ee36-5c29-8ae3-3c894e883317","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Dr Matthias Mauch","role":"Visitor","url":"http://www.matthiasmauch.net/","acadposition":"Visiting Academic","blurb":"music transcription (chords, beats, drums, melody, ...), interactive music annotation, singing research, research in the evolution of musical styles","themes":["mir"]}},{"id":"de8a6a9d-d9db-5d6f-aa49-d6383eab8ec3","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#d8c8c8","images":{"fallback":{"src":"/static/525d06e75294a7bf61d839700b1d2be5/3c342/ningzhiwang.jpg","srcSet":"/static/525d06e75294a7bf61d839700b1d2be5/19f6e/ningzhiwang.jpg 106w,\n/static/525d06e75294a7bf61d839700b1d2be5/a8230/ningzhiwang.jpg 212w,\n/static/525d06e75294a7bf61d839700b1d2be5/3c342/ningzhiwang.jpg 423w","sizes":"(min-width: 423px) 423px, 100vw"},"sources":[{"srcSet":"/static/525d06e75294a7bf61d839700b1d2be5/7a2a0/ningzhiwang.webp 106w,\n/static/525d06e75294a7bf61d839700b1d2be5/fbd6b/ningzhiwang.webp 212w,\n/static/525d06e75294a7bf61d839700b1d2be5/fb6ab/ningzhiwang.webp 423w","type":"image/webp","sizes":"(min-width: 423px) 423px, 100vw"}]},"width":423,"height":423}}},"name":"Ningzhi Wang","role":"PhD","url":"","acadposition":"PhD Student","blurb":"Generative Models For Music Audio Representation And Understanding","themes":["mir","mcog","soundsynthesis"]}},{"id":"68891a8b-ed44-55dc-a3f7-f02f74bda436","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#e8d8c8","images":{"fallback":{"src":"/static/becb17eb655c6af083c8d180ffcbce91/d1382/pedrosarmento.jpg","srcSet":"/static/becb17eb655c6af083c8d180ffcbce91/70617/pedrosarmento.jpg 582w,\n/static/becb17eb655c6af083c8d180ffcbce91/936a1/pedrosarmento.jpg 1165w,\n/static/becb17eb655c6af083c8d180ffcbce91/d1382/pedrosarmento.jpg 2329w","sizes":"(min-width: 2329px) 2329px, 100vw"},"sources":[{"srcSet":"/static/becb17eb655c6af083c8d180ffcbce91/f27b2/pedrosarmento.webp 582w,\n/static/becb17eb655c6af083c8d180ffcbce91/70259/pedrosarmento.webp 1165w,\n/static/becb17eb655c6af083c8d180ffcbce91/86c09/pedrosarmento.webp 2329w","type":"image/webp","sizes":"(min-width: 2329px) 2329px, 100vw"}]},"width":2329,"height":2329}}},"name":"Dr Pedro Sarmento","role":"Postdoc","url":"https://otnemrasordep.github.io/","acadposition":"Postdoctoral Researcher","blurb":"music information retrieval, language models for music generation, guitar tablature generation, automatic guitar transcription, deep learning","themes":["mir"]}},{"id":"9be482ab-e543-5ed7-a0ec-2d37af553450","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#7898b8","images":{"fallback":{"src":"/static/2046c4feea56b3899883569f75a5fcd1/e9d3f/rubycrocker.jpg","srcSet":"/static/2046c4feea56b3899883569f75a5fcd1/f3d60/rubycrocker.jpg 495w,\n/static/2046c4feea56b3899883569f75a5fcd1/8c337/rubycrocker.jpg 990w,\n/static/2046c4feea56b3899883569f75a5fcd1/e9d3f/rubycrocker.jpg 1980w","sizes":"(min-width: 1980px) 1980px, 100vw"},"sources":[{"srcSet":"/static/2046c4feea56b3899883569f75a5fcd1/72be1/rubycrocker.webp 495w,\n/static/2046c4feea56b3899883569f75a5fcd1/5c459/rubycrocker.webp 990w,\n/static/2046c4feea56b3899883569f75a5fcd1/83852/rubycrocker.webp 1980w","type":"image/webp","sizes":"(min-width: 1980px) 1980px, 100vw"}]},"width":1980,"height":1980}}},"name":"Ruby Crocker","role":"PhD","url":"","acadposition":"PhD Student","blurb":"Continuous mood recognition in film music","themes":["mir","mcog"]}},{"id":"fa3ed1ea-d5f0-59f2-92de-1a1aaa953db9","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#383838","images":{"fallback":{"src":"/static/023e1317ff77915bcb3ef017389aa670/071a6/saurjyasarkar.jpg","srcSet":"/static/023e1317ff77915bcb3ef017389aa670/ec705/saurjyasarkar.jpg 451w,\n/static/023e1317ff77915bcb3ef017389aa670/e51ad/saurjyasarkar.jpg 901w,\n/static/023e1317ff77915bcb3ef017389aa670/071a6/saurjyasarkar.jpg 1802w","sizes":"(min-width: 1802px) 1802px, 100vw"},"sources":[{"srcSet":"/static/023e1317ff77915bcb3ef017389aa670/120d7/saurjyasarkar.webp 451w,\n/static/023e1317ff77915bcb3ef017389aa670/f57e4/saurjyasarkar.webp 901w,\n/static/023e1317ff77915bcb3ef017389aa670/73d77/saurjyasarkar.webp 1802w","type":"image/webp","sizes":"(min-width: 1802px) 1802px, 100vw"}]},"width":1802,"height":1802}}},"name":"Dr Saurjya Sarkar","role":"Postdoc","url":"http://eecs.qmul.ac.uk/profiles/sarkarsaurjya-1.html","acadposition":"Postdoctoral Researcher","blurb":"Audio Source Separation, Music Information Retrieval, Sample Detection","themes":["mir"]}},{"id":"64e68bf1-afa9-5242-ba95-3bbb3ff80584","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8d8","images":{"fallback":{"src":"/static/fa7cb1f821b8a5e7891d12048a9bf77a/49d15/soumyavanka.jpg","srcSet":"/static/fa7cb1f821b8a5e7891d12048a9bf77a/18eda/soumyavanka.jpg 133w,\n/static/fa7cb1f821b8a5e7891d12048a9bf77a/e53cc/soumyavanka.jpg 266w,\n/static/fa7cb1f821b8a5e7891d12048a9bf77a/49d15/soumyavanka.jpg 531w","sizes":"(min-width: 531px) 531px, 100vw"},"sources":[{"srcSet":"/static/fa7cb1f821b8a5e7891d12048a9bf77a/a2d02/soumyavanka.webp 133w,\n/static/fa7cb1f821b8a5e7891d12048a9bf77a/fd489/soumyavanka.webp 266w,\n/static/fa7cb1f821b8a5e7891d12048a9bf77a/dc6f9/soumyavanka.webp 531w","type":"image/webp","sizes":"(min-width: 531px) 531px, 100vw"}]},"width":531,"height":531}}},"name":"Soumya Sai Vanka","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/vankasaisoumya.html","acadposition":"PhD Student","blurb":"Music Production Style Transfer and Mix Similarity","themes":["audioeng","mir"]}},{"id":"06196503-f058-5081-ab1e-21e9b406e714","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/91f7a353a15c6e68c71f84ff92066743/ffdfb/simondixon.png","srcSet":"/static/91f7a353a15c6e68c71f84ff92066743/5bd84/simondixon.png 92w,\n/static/91f7a353a15c6e68c71f84ff92066743/f9b49/simondixon.png 184w,\n/static/91f7a353a15c6e68c71f84ff92066743/ffdfb/simondixon.png 367w","sizes":"(min-width: 367px) 367px, 100vw"},"sources":[{"srcSet":"/static/91f7a353a15c6e68c71f84ff92066743/483b8/simondixon.webp 92w,\n/static/91f7a353a15c6e68c71f84ff92066743/42e3f/simondixon.webp 184w,\n/static/91f7a353a15c6e68c71f84ff92066743/46fdb/simondixon.webp 367w","type":"image/webp","sizes":"(min-width: 367px) 367px, 100vw"}]},"width":367,"height":367}}},"name":"Prof. Simon Dixon","role":"Academic","url":"http://www.eecs.qmul.ac.uk/~simond/","acadposition":"Professor of Computer Science, Deputy Director of C4DM, Director of the AIM CDT","blurb":"Music informatics, music signal processing, artificial intelligence, music cognition; extraction of musical content (e.g. rhythm, harmony, intonation) from audio signals: beat tracking, audio alignment, chord and note transcription, singing intonation; using signal processing approaches, probabilistic models, and deep learning.","themes":["mir"]}},{"id":"c4baa5bd-09a1-5c86-9e66-b8a4a269c168","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Sungkyun Chang","role":"Research Assistant","url":"https://mimbres.github.io/home/","acadposition":"Research Assistant","blurb":"Deep learning technologies for multi-instrument automatic music transcription","themes":["mir","mlist"]}},{"id":"aac95703-2ad6-5435-ba42-e404dc3a150e","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/static/4ea878db394500ed14f41f522a4afb01/b74b1/tylermcintosh.jpg","srcSet":"/static/4ea878db394500ed14f41f522a4afb01/d3fc0/tylermcintosh.jpg 270w,\n/static/4ea878db394500ed14f41f522a4afb01/19455/tylermcintosh.jpg 540w,\n/static/4ea878db394500ed14f41f522a4afb01/b74b1/tylermcintosh.jpg 1080w","sizes":"(min-width: 1080px) 1080px, 100vw"},"sources":[{"srcSet":"/static/4ea878db394500ed14f41f522a4afb01/ede49/tylermcintosh.webp 270w,\n/static/4ea878db394500ed14f41f522a4afb01/4cb34/tylermcintosh.webp 540w,\n/static/4ea878db394500ed14f41f522a4afb01/4f506/tylermcintosh.webp 1080w","type":"image/webp","sizes":"(min-width: 1080px) 1080px, 100vw"}]},"width":1080,"height":1080}}},"name":"Tyler Howard McIntosh","role":"PhD","url":"","acadposition":"PhD Student","blurb":"Expressive Performance Rendering for Music Generation Systems","themes":["mcog","mir"]}},{"id":"d964f102-7a03-5e39-8ca5-d62171217446","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8e8","images":{"fallback":{"src":"/static/d9e4891e02c2d5580fb05022babf8891/4ce45/xavierriley.jpg","srcSet":"/static/d9e4891e02c2d5580fb05022babf8891/21980/xavierriley.jpg 167w,\n/static/d9e4891e02c2d5580fb05022babf8891/f560f/xavierriley.jpg 333w,\n/static/d9e4891e02c2d5580fb05022babf8891/4ce45/xavierriley.jpg 666w","sizes":"(min-width: 666px) 666px, 100vw"},"sources":[{"srcSet":"/static/d9e4891e02c2d5580fb05022babf8891/9b205/xavierriley.webp 167w,\n/static/d9e4891e02c2d5580fb05022babf8891/e1538/xavierriley.webp 333w,\n/static/d9e4891e02c2d5580fb05022babf8891/bf451/xavierriley.webp 666w","type":"image/webp","sizes":"(min-width: 666px) 666px, 100vw"}]},"width":666,"height":666}}},"name":"Xavier Riley","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/rileyjohnxavier.html","acadposition":"PhD Student","blurb":"Pitch tracking for music applications - beyond 99% accuracy","themes":["mir","audioeng"]}},{"id":"18bb4e4a-ca3d-51a0-9beb-5aea409131ab","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Vjosa Preniqi","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/preniqivjosa.html","acadposition":"PhD Student","blurb":"Predicting demographics, personalities, and global values from digital media behaviours","themes":["mir","mcog"]}},{"id":"e0104201-8b94-5c6c-862b-a813f7cdc558","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/94b0e38320ab53f4542d711f270cc5c7/a89ca/yinghaoma.jpg","srcSet":"/static/94b0e38320ab53f4542d711f270cc5c7/96deb/yinghaoma.jpg 150w,\n/static/94b0e38320ab53f4542d711f270cc5c7/0fdf4/yinghaoma.jpg 300w,\n/static/94b0e38320ab53f4542d711f270cc5c7/a89ca/yinghaoma.jpg 600w","sizes":"(min-width: 600px) 600px, 100vw"},"sources":[{"srcSet":"/static/94b0e38320ab53f4542d711f270cc5c7/c65bc/yinghaoma.webp 150w,\n/static/94b0e38320ab53f4542d711f270cc5c7/078c3/yinghaoma.webp 300w,\n/static/94b0e38320ab53f4542d711f270cc5c7/6d09e/yinghaoma.webp 600w","type":"image/webp","sizes":"(min-width: 600px) 600px, 100vw"}]},"width":600,"height":600}}},"name":"Yinghao Ma","role":"PhD","url":"https://nicolaus625.github.io/","acadposition":"PhD Student","blurb":"Self-supervision in machine listening","themes":["mir","mlist"]}},{"id":"f0e1d613-f724-56a3-bcb9-30f54b132dcb","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#e8e8e8","images":{"fallback":{"src":"/static/7371426282ff3e012ec154dd11dd684e/c29ce/xiaowanyi.jpg","srcSet":"/static/7371426282ff3e012ec154dd11dd684e/77813/xiaowanyi.jpg 545w,\n/static/7371426282ff3e012ec154dd11dd684e/160ed/xiaowanyi.jpg 1091w,\n/static/7371426282ff3e012ec154dd11dd684e/c29ce/xiaowanyi.jpg 2181w","sizes":"(min-width: 2181px) 2181px, 100vw"},"sources":[{"srcSet":"/static/7371426282ff3e012ec154dd11dd684e/9bbe1/xiaowanyi.webp 545w,\n/static/7371426282ff3e012ec154dd11dd684e/282f9/xiaowanyi.webp 1091w,\n/static/7371426282ff3e012ec154dd11dd684e/630ab/xiaowanyi.webp 2181w","type":"image/webp","sizes":"(min-width: 2181px) 2181px, 100vw"}]},"width":2181,"height":2181}}},"name":"Xiaowan Yi","role":"PhD","url":"https://www.qmul.ac.uk/eecs/people/profiles/yixiaowan.html","acadposition":"PhD Student","blurb":"Composition-aware music recommendation system for music production","themes":["mir","audioeng","isam"]}},{"id":"8b81527a-163a-5125-9fc9-247a73a54a05","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/406e06b0efaa63e877b039820154296d/2655f/yixiaozhang.png","srcSet":"/static/406e06b0efaa63e877b039820154296d/1f8a1/yixiaozhang.png 80w,\n/static/406e06b0efaa63e877b039820154296d/d8938/yixiaozhang.png 159w,\n/static/406e06b0efaa63e877b039820154296d/2655f/yixiaozhang.png 318w","sizes":"(min-width: 318px) 318px, 100vw"},"sources":[{"srcSet":"/static/406e06b0efaa63e877b039820154296d/61ca6/yixiaozhang.webp 80w,\n/static/406e06b0efaa63e877b039820154296d/0993c/yixiaozhang.webp 159w,\n/static/406e06b0efaa63e877b039820154296d/4ce64/yixiaozhang.webp 318w","type":"image/webp","sizes":"(min-width: 318px) 318px, 100vw"}]},"width":318,"height":318}}},"name":"Yixiao Zhang","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/zhangyixiao.html","acadposition":"PhD Student","blurb":"Machine Learning Methods for Artificial Musicality","themes":["mir"]}},{"id":"8d93ecc6-ed02-5662-a390-01b9e68ef5f5","frontmatter":{"image":null,"name":"Yukun Li","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/liyukun.html","acadposition":"PhD Student","blurb":"Computational Comparison Between Different Genres of Music in Terms of the Singing Voice","themes":["mir"]}}]}}}
{"data":{"text":{"html":"<p>With online music stores offering millions of songs to choose from, users need assistance. Using digital signal processing, machine learning, and the semantic web, our research explores new ways of intelligently analysing musical data, and assists people in finding the music they want.</p>\n<p>We have developed systems for automatic playlisting from personal collections (SoundBite), for looking inside the audio (Sonic Visualiser), for hardening/softening transients, and many others. We also regularly release some of our algorithms under Open Source licences, while maintaining a healthy portfolio of patents.</p>\n<p>This area is led by <a href=\"http://www.eecs.qmul.ac.uk/~simond/\">Dr Simon Dixon</a>. Projects in this area include:</p>\n<ul>\n<li>mid-level music descriptors: chords, keys, notes, beats, drums, instrumentation, timbre, structural segmentation, melody</li>\n<li>high-level concepts for music classification, retrieval and knowledge discovery: genre, mood, emotions</li>\n<li><a href=\"http://sonicvisualiser.org\">Sonic Visualser</a></li>\n<li>semantic music analysis for intelligent editing</li>\n<li>linking music-related information and audio data</li>\n<li>interactive auralisation with room impulse responses</li>\n</ul>\n<p><a href=\"/get-involved/\">PhD Study</a> - interested in joining the team? We are currently accepting PhD applications.</p>\n<h2>Members</h2>","frontmatter":{"title":""}},"people":{"nodes":[{"id":"226ac21f-bb0b-56df-9975-41dc5d56d6ca","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8d8e8","images":{"fallback":{"src":"/static/a0048693a658a807cb05a19252bb723c/02dff/adityabhattacharjee.jpg","srcSet":"/static/a0048693a658a807cb05a19252bb723c/0fdf4/adityabhattacharjee.jpg 300w,\n/static/a0048693a658a807cb05a19252bb723c/a89ca/adityabhattacharjee.jpg 600w,\n/static/a0048693a658a807cb05a19252bb723c/02dff/adityabhattacharjee.jpg 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/a0048693a658a807cb05a19252bb723c/078c3/adityabhattacharjee.webp 300w,\n/static/a0048693a658a807cb05a19252bb723c/6d09e/adityabhattacharjee.webp 600w,\n/static/a0048693a658a807cb05a19252bb723c/83805/adityabhattacharjee.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"name":"Aditya Bhattacharjee","role":"PhD","url":"https://www.linkedin.com/in/adibh/","acadposition":"PhD Student","blurb":"Self-supervision in Audio Fingerprinting","themes":["mir","mlist"]}},{"id":"87088d07-80bf-5486-9d68-2bc54fb7d6e9","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#e8f8f8","images":{"fallback":{"src":"/static/543ed3a102028b2ed1952a0a04a3d543/d558d/aidanhogg.jpg","srcSet":"/static/543ed3a102028b2ed1952a0a04a3d543/7fbe3/aidanhogg.jpg 142w,\n/static/543ed3a102028b2ed1952a0a04a3d543/46b1b/aidanhogg.jpg 284w,\n/static/543ed3a102028b2ed1952a0a04a3d543/d558d/aidanhogg.jpg 567w","sizes":"(min-width: 567px) 567px, 100vw"},"sources":[{"srcSet":"/static/543ed3a102028b2ed1952a0a04a3d543/bcdd4/aidanhogg.webp 142w,\n/static/543ed3a102028b2ed1952a0a04a3d543/5d551/aidanhogg.webp 284w,\n/static/543ed3a102028b2ed1952a0a04a3d543/b709e/aidanhogg.webp 567w","type":"image/webp","sizes":"(min-width: 567px) 567px, 100vw"}]},"width":567,"height":567}}},"name":"Dr Aidan Hogg","role":"Academic","url":"https://aidanhogg.uk/","acadposition":"Lecturer in Computer Science","blurb":"spatial and immersive audio, music signal processing, machine learning for audio, music information retrieval","themes":["comma","mir"]}},{"id":"cc381a30-c5d2-5ff7-b60d-1f10d81dbf9b","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Alexander Williams","role":"PhD","url":"","acadposition":"PhD Student","blurb":"User-driven deep music generation in digital audio workstations","themes":["mir","audioeng"]}},{"id":"d35868ef-7cda-5e98-aa09-bb356da9e5d3","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/static/cce06b1e41eda976989e159ecb5db09c/81ef7/andreamartelloni.jpg","srcSet":"/static/cce06b1e41eda976989e159ecb5db09c/96deb/andreamartelloni.jpg 150w,\n/static/cce06b1e41eda976989e159ecb5db09c/9a5ea/andreamartelloni.jpg 299w,\n/static/cce06b1e41eda976989e159ecb5db09c/81ef7/andreamartelloni.jpg 598w","sizes":"(min-width: 598px) 598px, 100vw"},"sources":[{"srcSet":"/static/cce06b1e41eda976989e159ecb5db09c/c65bc/andreamartelloni.webp 150w,\n/static/cce06b1e41eda976989e159ecb5db09c/47ed3/andreamartelloni.webp 299w,\n/static/cce06b1e41eda976989e159ecb5db09c/8c892/andreamartelloni.webp 598w","type":"image/webp","sizes":"(min-width: 598px) 598px, 100vw"}]},"width":598,"height":598}}},"name":"Andrea Martelloni","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/martelloniandrea-1.html","acadposition":"PhD Student","blurb":"Real-Time Gesture Classification on an Augmented Acoustic Guitar using Deep Learning to Improve Extended-Range and Percussive Solo Playing","themes":["mir"]}},{"id":"738dd58b-17cc-53bb-b319-0186faf690cc","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#989888","images":{"fallback":{"src":"/static/09388c6cc31edb4824a849da2f74754c/30f07/andrewedwards.jpg","srcSet":"/static/09388c6cc31edb4824a849da2f74754c/41624/andrewedwards.jpg 160w,\n/static/09388c6cc31edb4824a849da2f74754c/1b894/andrewedwards.jpg 320w,\n/static/09388c6cc31edb4824a849da2f74754c/30f07/andrewedwards.jpg 640w","sizes":"(min-width: 640px) 640px, 100vw"},"sources":[{"srcSet":"/static/09388c6cc31edb4824a849da2f74754c/60b4d/andrewedwards.webp 160w,\n/static/09388c6cc31edb4824a849da2f74754c/5e011/andrewedwards.webp 320w,\n/static/09388c6cc31edb4824a849da2f74754c/90d07/andrewedwards.webp 640w","type":"image/webp","sizes":"(min-width: 640px) 640px, 100vw"}]},"width":640,"height":640}}},"name":"Andrew (Drew) Edwards","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/edwardsandrewcharles.html","acadposition":"PhD Student","blurb":"Deep Learning for Jazz Piano: Transcription + Generative Modeling","themes":["mir","mlist","mcog"]}},{"id":"1f3913f4-5507-56c4-a92a-0782c7b095bb","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#484858","images":{"fallback":{"src":"/static/3d59fe8293835010c7ad4f180e4cc954/dd515/annaxambo.jpg","srcSet":"/static/3d59fe8293835010c7ad4f180e4cc954/6ac16/annaxambo.jpg 50w,\n/static/3d59fe8293835010c7ad4f180e4cc954/e07e1/annaxambo.jpg 100w,\n/static/3d59fe8293835010c7ad4f180e4cc954/dd515/annaxambo.jpg 200w","sizes":"(min-width: 200px) 200px, 100vw"},"sources":[{"srcSet":"/static/3d59fe8293835010c7ad4f180e4cc954/dbc4a/annaxambo.webp 50w,\n/static/3d59fe8293835010c7ad4f180e4cc954/d8057/annaxambo.webp 100w,\n/static/3d59fe8293835010c7ad4f180e4cc954/2e34e/annaxambo.webp 200w","type":"image/webp","sizes":"(min-width: 200px) 200px, 100vw"}]},"width":200,"height":200}}},"name":"Dr Anna Xambó","role":"Academic","url":"https://annaxambo.me/","acadposition":"Senior Lecturer in Sound and Music Computing","blurb":"new interfaces for musical expression, performance study, human-computer interaction, interaction design","themes":["augmi","mir","isam"]}},{"id":"9a6273ce-dc95-5d23-b5f9-7bcebfdc1772","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#685848","images":{"fallback":{"src":"/static/c0f574ea604c6ff9084a488c4bc7e4e5/0be83/ashleynoelhirst.png","srcSet":"/static/c0f574ea604c6ff9084a488c4bc7e4e5/5f035/ashleynoelhirst.png 320w,\n/static/c0f574ea604c6ff9084a488c4bc7e4e5/eadd3/ashleynoelhirst.png 640w,\n/static/c0f574ea604c6ff9084a488c4bc7e4e5/0be83/ashleynoelhirst.png 1280w","sizes":"(min-width: 1280px) 1280px, 100vw"},"sources":[{"srcSet":"/static/c0f574ea604c6ff9084a488c4bc7e4e5/5e011/ashleynoelhirst.webp 320w,\n/static/c0f574ea604c6ff9084a488c4bc7e4e5/90d07/ashleynoelhirst.webp 640w,\n/static/c0f574ea604c6ff9084a488c4bc7e4e5/9e21f/ashleynoelhirst.webp 1280w","type":"image/webp","sizes":"(min-width: 1280px) 1280px, 100vw"}]},"width":1280,"height":1280}}},"name":"Ashley Noel-Hirst","role":"PhD","url":"https://ashleynoelhirst.co.uk/","acadposition":"PhD Student","blurb":"Latent Spaces for Human-AI music generation","themes":["mir","isam"]}},{"id":"34ef929b-de59-5c95-a554-597f98d80226","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/83b515844814d95b7fc7d641a74fed63/9cf6c/chinyunyu.jpg","srcSet":"/static/83b515844814d95b7fc7d641a74fed63/2f83b/chinyunyu.jpg 351w,\n/static/83b515844814d95b7fc7d641a74fed63/b41ee/chinyunyu.jpg 702w,\n/static/83b515844814d95b7fc7d641a74fed63/9cf6c/chinyunyu.jpg 1403w","sizes":"(min-width: 1403px) 1403px, 100vw"},"sources":[{"srcSet":"/static/83b515844814d95b7fc7d641a74fed63/9bbac/chinyunyu.webp 351w,\n/static/83b515844814d95b7fc7d641a74fed63/ce9fc/chinyunyu.webp 702w,\n/static/83b515844814d95b7fc7d641a74fed63/f5914/chinyunyu.webp 1403w","type":"image/webp","sizes":"(min-width: 1403px) 1403px, 100vw"}]},"width":1403,"height":1403}}},"name":"Chin-Yun Yu","role":"PhD","url":"https://www.qmul.ac.uk/eecs/people/profiles/chinyunyu.html","acadposition":"PhD Student","blurb":"Neural Audio Synthesis with Expressiveness Control","themes":["audioeng","mir","soundsynthesis"]}},{"id":"1e774e43-fee0-57f5-b2b5-632fe8762c2a","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#988878","images":{"fallback":{"src":"/static/81f0d5ca330795a7366c18a6c1d1a10a/47faa/cyrusvahidi.jpg","srcSet":"/static/81f0d5ca330795a7366c18a6c1d1a10a/fcf11/cyrusvahidi.jpg 306w,\n/static/81f0d5ca330795a7366c18a6c1d1a10a/04fa2/cyrusvahidi.jpg 612w,\n/static/81f0d5ca330795a7366c18a6c1d1a10a/47faa/cyrusvahidi.jpg 1224w","sizes":"(min-width: 1224px) 1224px, 100vw"},"sources":[{"srcSet":"/static/81f0d5ca330795a7366c18a6c1d1a10a/c51a2/cyrusvahidi.webp 306w,\n/static/81f0d5ca330795a7366c18a6c1d1a10a/c089a/cyrusvahidi.webp 612w,\n/static/81f0d5ca330795a7366c18a6c1d1a10a/23828/cyrusvahidi.webp 1224w","type":"image/webp","sizes":"(min-width: 1224px) 1224px, 100vw"}]},"width":1224,"height":1224}}},"name":"Cyrus Vahidi","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/vahidicyrus.html","acadposition":"PhD Student","blurb":"Perceptual end to end learning for music understanding","themes":["mir"]}},{"id":"f855554f-b1f4-56be-8d59-801f8b9664fe","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8e8c8","images":{"fallback":{"src":"/static/c7d60cc8efa14a13b6dbba4bc7ccc137/a20e0/davefoster.jpg","srcSet":"/static/c7d60cc8efa14a13b6dbba4bc7ccc137/71c2b/davefoster.jpg 52w,\n/static/c7d60cc8efa14a13b6dbba4bc7ccc137/ac761/davefoster.jpg 104w,\n/static/c7d60cc8efa14a13b6dbba4bc7ccc137/a20e0/davefoster.jpg 208w","sizes":"(min-width: 208px) 208px, 100vw"},"sources":[{"srcSet":"/static/c7d60cc8efa14a13b6dbba4bc7ccc137/284ac/davefoster.webp 52w,\n/static/c7d60cc8efa14a13b6dbba4bc7ccc137/5ca4c/davefoster.webp 104w,\n/static/c7d60cc8efa14a13b6dbba4bc7ccc137/c95dc/davefoster.webp 208w","type":"image/webp","sizes":"(min-width: 208px) 208px, 100vw"}]},"width":208,"height":208}}},"name":"David Foster","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/fosterdavid.html","acadposition":"PhD Student","blurb":"Modelling the Creative Process of Jazz Improvisation","themes":["mir"]}},{"id":"7b043f0b-de2f-5155-9877-1dfff2b8c117","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/87ab058ab4e8a08974fd4a9a22ef1bac/36a05/elonashatri.jpg","srcSet":"/static/87ab058ab4e8a08974fd4a9a22ef1bac/a46b0/elonashatri.jpg 205w,\n/static/87ab058ab4e8a08974fd4a9a22ef1bac/3e7a6/elonashatri.jpg 410w,\n/static/87ab058ab4e8a08974fd4a9a22ef1bac/36a05/elonashatri.jpg 819w","sizes":"(min-width: 819px) 819px, 100vw"},"sources":[{"srcSet":"/static/87ab058ab4e8a08974fd4a9a22ef1bac/24bbe/elonashatri.webp 205w,\n/static/87ab058ab4e8a08974fd4a9a22ef1bac/d883f/elonashatri.webp 410w,\n/static/87ab058ab4e8a08974fd4a9a22ef1bac/8f6df/elonashatri.webp 819w","type":"image/webp","sizes":"(min-width: 819px) 819px, 100vw"}]},"width":819,"height":819}}},"name":"Elona Shatri","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/shatrielona-1.html","acadposition":"PhD Student","blurb":"Optical music recognition using deep learning","themes":["mir"]}},{"id":"ef8d4761-ec9b-58f0-bfa5-26159c60c38e","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/8106f707dd10b39434bd1c08f1f22f6d/30f07/emmanouilbenetos.jpg","srcSet":"/static/8106f707dd10b39434bd1c08f1f22f6d/41624/emmanouilbenetos.jpg 160w,\n/static/8106f707dd10b39434bd1c08f1f22f6d/1b894/emmanouilbenetos.jpg 320w,\n/static/8106f707dd10b39434bd1c08f1f22f6d/30f07/emmanouilbenetos.jpg 640w","sizes":"(min-width: 640px) 640px, 100vw"},"sources":[{"srcSet":"/static/8106f707dd10b39434bd1c08f1f22f6d/60b4d/emmanouilbenetos.webp 160w,\n/static/8106f707dd10b39434bd1c08f1f22f6d/5e011/emmanouilbenetos.webp 320w,\n/static/8106f707dd10b39434bd1c08f1f22f6d/90d07/emmanouilbenetos.webp 640w","type":"image/webp","sizes":"(min-width: 640px) 640px, 100vw"}]},"width":640,"height":640}}},"name":"Dr Emmanouil Benetos","role":"Academic","url":"http://www.eecs.qmul.ac.uk/people/view/4741/dr-emmanouil-benetos","acadposition":"Reader in Machine Listening","blurb":"Machine listening / computer audition, Machine learning for audio and sequential data, Music information retrieval, Multimodal AI, Resource-efficient AI","themes":["mir","mlist"]}},{"id":"9aa31664-07ac-5194-a7fe-146cab80fddb","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#a8b8e8","images":{"fallback":{"src":"/static/eaf04fbbdffdb3aca0caed16cfdf1102/7900e/gyorgyfazekas.jpg","srcSet":"/static/eaf04fbbdffdb3aca0caed16cfdf1102/f713e/gyorgyfazekas.jpg 55w,\n/static/eaf04fbbdffdb3aca0caed16cfdf1102/e2afd/gyorgyfazekas.jpg 110w,\n/static/eaf04fbbdffdb3aca0caed16cfdf1102/7900e/gyorgyfazekas.jpg 219w","sizes":"(min-width: 219px) 219px, 100vw"},"sources":[{"srcSet":"/static/eaf04fbbdffdb3aca0caed16cfdf1102/938d3/gyorgyfazekas.webp 55w,\n/static/eaf04fbbdffdb3aca0caed16cfdf1102/8c6ff/gyorgyfazekas.webp 110w,\n/static/eaf04fbbdffdb3aca0caed16cfdf1102/3f57d/gyorgyfazekas.webp 219w","type":"image/webp","sizes":"(min-width: 219px) 219px, 100vw"}]},"width":219,"height":219}}},"name":"Dr George Fazekas","role":"Academic","url":"http://eecs.qmul.ac.uk/~gyorgyf","acadposition":"Senior Lecturer","blurb":"Semantic Audio, Music Information Retrieval, Semantic Web for Music, Machine Learning and Data Science, Music Emotion Recognition, Interactive music sytems (e.g. intellignet editing, audio production and performance systems)","themes":["mir"]}},{"id":"b71f6653-2c4e-5f2b-b283-2eb0ae2679b6","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/78d403f536869a8d3f69509587dc97e9/192dd/harnickhera.png","srcSet":"/static/78d403f536869a8d3f69509587dc97e9/b024c/harnickhera.png 65w,\n/static/78d403f536869a8d3f69509587dc97e9/68010/harnickhera.png 129w,\n/static/78d403f536869a8d3f69509587dc97e9/192dd/harnickhera.png 258w","sizes":"(min-width: 258px) 258px, 100vw"},"sources":[{"srcSet":"/static/78d403f536869a8d3f69509587dc97e9/0c531/harnickhera.webp 65w,\n/static/78d403f536869a8d3f69509587dc97e9/f6a08/harnickhera.webp 129w,\n/static/78d403f536869a8d3f69509587dc97e9/c7cc5/harnickhera.webp 258w","type":"image/webp","sizes":"(min-width: 258px) 258px, 100vw"}]},"width":258,"height":258}}},"name":"Harnick Khera","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/kheraharnicksingh.html","acadposition":"PhD Student","blurb":"Informed source separation for multi-mic production","themes":["mir","mlist"]}},{"id":"1918386c-730c-5e3e-9d42-0369c7ed79b1","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#88b8f8","images":{"fallback":{"src":"/static/56fdf9525659e33f14b4049213e7f4a2/70617/huanzhang.jpg","srcSet":"/static/56fdf9525659e33f14b4049213e7f4a2/f2f05/huanzhang.jpg 146w,\n/static/56fdf9525659e33f14b4049213e7f4a2/b179d/huanzhang.jpg 291w,\n/static/56fdf9525659e33f14b4049213e7f4a2/70617/huanzhang.jpg 582w","sizes":"(min-width: 582px) 582px, 100vw"},"sources":[{"srcSet":"/static/56fdf9525659e33f14b4049213e7f4a2/9027a/huanzhang.webp 146w,\n/static/56fdf9525659e33f14b4049213e7f4a2/46435/huanzhang.webp 291w,\n/static/56fdf9525659e33f14b4049213e7f4a2/f27b2/huanzhang.webp 582w","type":"image/webp","sizes":"(min-width: 582px) 582px, 100vw"}]},"width":582,"height":582}}},"name":"Huan Zhang","role":"PhD","url":"http://eecs.qmul.ac.uk/people/profiles/zhanghuan.html","acadposition":"PhD Student","blurb":"Computational Modelling of Expressive Piano Performance","themes":["mir"]}},{"id":"aa11410e-746b-527f-b4fe-5ca3c2d55c7e","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/a26f539170c6c364589c0d94f1939652/e268c/huwcheston.jpg","srcSet":"/static/a26f539170c6c364589c0d94f1939652/01ac2/huwcheston.jpg 786w,\n/static/a26f539170c6c364589c0d94f1939652/8610a/huwcheston.jpg 1572w,\n/static/a26f539170c6c364589c0d94f1939652/e268c/huwcheston.jpg 3143w","sizes":"(min-width: 3143px) 3143px, 100vw"},"sources":[{"srcSet":"/static/a26f539170c6c364589c0d94f1939652/f8e7c/huwcheston.webp 786w,\n/static/a26f539170c6c364589c0d94f1939652/f8c5f/huwcheston.webp 1572w,\n/static/a26f539170c6c364589c0d94f1939652/2537b/huwcheston.webp 3143w","type":"image/webp","sizes":"(min-width: 3143px) 3143px, 100vw"}]},"width":3143,"height":3143}}},"name":"Huw Cheston","role":"Research Assistant","url":"https://huwcheston.github.io","acadposition":"Research Assistant","blurb":"Multimodal machine perception","themes":["mir","mlist"]}},{"id":"27c0231d-35a2-57d5-8273-8562fba3319f","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Iacopo Ghinassi","role":"PhD","url":"https://github.com/Ighina","acadposition":"PhD Student","blurb":"Semantic understanding of TV programme content and structure to enable automatic enhancement and adjustment","themes":["mir"]}},{"id":"8f618d2e-72c3-5ff1-a65f-f14410f7ea3e","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#d88868","images":{"fallback":{"src":"/static/5ebddded3dd854a7fb181cab4c068dec/97a19/ilariamanco.jpg","srcSet":"/static/5ebddded3dd854a7fb181cab4c068dec/73bb6/ilariamanco.jpg 120w,\n/static/5ebddded3dd854a7fb181cab4c068dec/f9edd/ilariamanco.jpg 240w,\n/static/5ebddded3dd854a7fb181cab4c068dec/97a19/ilariamanco.jpg 480w","sizes":"(min-width: 480px) 480px, 100vw"},"sources":[{"srcSet":"/static/5ebddded3dd854a7fb181cab4c068dec/507b0/ilariamanco.webp 120w,\n/static/5ebddded3dd854a7fb181cab4c068dec/8d565/ilariamanco.webp 240w,\n/static/5ebddded3dd854a7fb181cab4c068dec/21b1a/ilariamanco.webp 480w","type":"image/webp","sizes":"(min-width: 480px) 480px, 100vw"}]},"width":480,"height":480}}},"name":"Ilaria Manco","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/mancoilaria.html","acadposition":"PhD Student","blurb":"Multimodal Deep Learning for Music Information Retrieval","themes":["mir","mlist"]}},{"id":"be6e7a39-4760-57e6-a607-f988d7a322da","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#c8a878","images":{"fallback":{"src":"/static/28c5745e61741b22792aa6d4a135584a/68974/iranroman.jpg","srcSet":"/static/28c5745e61741b22792aa6d4a135584a/d4a57/iranroman.jpg 64w,\n/static/28c5745e61741b22792aa6d4a135584a/19e71/iranroman.jpg 128w,\n/static/28c5745e61741b22792aa6d4a135584a/68974/iranroman.jpg 256w","sizes":"(min-width: 256px) 256px, 100vw"},"sources":[{"srcSet":"/static/28c5745e61741b22792aa6d4a135584a/8257c/iranroman.webp 64w,\n/static/28c5745e61741b22792aa6d4a135584a/6766a/iranroman.webp 128w,\n/static/28c5745e61741b22792aa6d4a135584a/22bfc/iranroman.webp 256w","type":"image/webp","sizes":"(min-width: 256px) 256px, 100vw"}]},"width":256,"height":256}}},"name":"Dr Iran Roman","role":"Academic","url":"https://iranroman.github.io/","acadposition":"Lecturer in Sound and Music Computing","blurb":"theoretical neuroscience, machine perception, artificial intelligence","themes":["comma","mir","mlist"]}},{"id":"8ee3a5c5-91a8-5ec8-8d03-f636b18d244d","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/c0f12a01968f7a264dd0c78e6139f6c0/f9edd/ivanhiggs.jpg","srcSet":"/static/c0f12a01968f7a264dd0c78e6139f6c0/93848/ivanhiggs.jpg 60w,\n/static/c0f12a01968f7a264dd0c78e6139f6c0/73bb6/ivanhiggs.jpg 120w,\n/static/c0f12a01968f7a264dd0c78e6139f6c0/f9edd/ivanhiggs.jpg 240w","sizes":"(min-width: 240px) 240px, 100vw"},"sources":[{"srcSet":"/static/c0f12a01968f7a264dd0c78e6139f6c0/927d1/ivanhiggs.webp 60w,\n/static/c0f12a01968f7a264dd0c78e6139f6c0/507b0/ivanhiggs.webp 120w,\n/static/c0f12a01968f7a264dd0c78e6139f6c0/8d565/ivanhiggs.webp 240w","type":"image/webp","sizes":"(min-width: 240px) 240px, 100vw"}]},"width":240,"height":240}}},"name":"Ivan Meresman Higgs","role":"Research Assistant","url":"https://github.com/ivanlmh","acadposition":"Research Assistant","blurb":"Sample Identification in Mastered Songs using Deep Learning Methods","themes":["mir"]}},{"id":"623d083c-bec9-58ac-b75f-deed805850e4","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#a89888","images":{"fallback":{"src":"/static/6dd5cb05357d20e63e726634c9ff6079/035db/jamesbolt.jpg","srcSet":"/static/6dd5cb05357d20e63e726634c9ff6079/0f5ce/jamesbolt.jpg 750w,\n/static/6dd5cb05357d20e63e726634c9ff6079/1f55a/jamesbolt.jpg 1500w,\n/static/6dd5cb05357d20e63e726634c9ff6079/035db/jamesbolt.jpg 3000w","sizes":"(min-width: 3000px) 3000px, 100vw"},"sources":[{"srcSet":"/static/6dd5cb05357d20e63e726634c9ff6079/4f03f/jamesbolt.webp 750w,\n/static/6dd5cb05357d20e63e726634c9ff6079/07142/jamesbolt.webp 1500w,\n/static/6dd5cb05357d20e63e726634c9ff6079/19b60/jamesbolt.webp 3000w","type":"image/webp","sizes":"(min-width: 3000px) 3000px, 100vw"}]},"width":3000,"height":3000}}},"name":"James Bolt","role":"PhD","url":"https://www.qmul.ac.uk/eecs/people/profiles/jamesbolt.html","acadposition":"PhD Student","blurb":"Intelligent audio and music editing with deep learning","themes":["mir"]}},{"id":"8209c891-60bc-5d31-b5de-2e332d1ae01c","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#081808","images":{"fallback":{"src":"/static/335b628a1c69b433d57e9201845b16d7/37851/jazasyed.jpg","srcSet":"/static/335b628a1c69b433d57e9201845b16d7/5b1fa/jazasyed.jpg 115w,\n/static/335b628a1c69b433d57e9201845b16d7/f0f53/jazasyed.jpg 230w,\n/static/335b628a1c69b433d57e9201845b16d7/37851/jazasyed.jpg 460w","sizes":"(min-width: 460px) 460px, 100vw"},"sources":[{"srcSet":"/static/335b628a1c69b433d57e9201845b16d7/f8466/jazasyed.webp 115w,\n/static/335b628a1c69b433d57e9201845b16d7/84992/jazasyed.webp 230w,\n/static/335b628a1c69b433d57e9201845b16d7/b5c5b/jazasyed.webp 460w","type":"image/webp","sizes":"(min-width: 460px) 460px, 100vw"}]},"width":460,"height":460}}},"name":"Jaza Syed","role":"Research Assistant","url":"https://jaza.xyz/","acadposition":"Research Assistant","blurb":"Audio ML, Automatic Lyrics Transcription","themes":["mir"]}},{"id":"b3e3ac4a-cd06-5d69-8ac9-f47d73ef5c0b","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8a898","images":{"fallback":{"src":"/static/5c4ebc68a7723fc56115ed3fe749ff7a/96deb/johanpauwels.jpg","srcSet":"/static/5c4ebc68a7723fc56115ed3fe749ff7a/c81d1/johanpauwels.jpg 38w,\n/static/5c4ebc68a7723fc56115ed3fe749ff7a/91a6d/johanpauwels.jpg 75w,\n/static/5c4ebc68a7723fc56115ed3fe749ff7a/96deb/johanpauwels.jpg 150w","sizes":"(min-width: 150px) 150px, 100vw"},"sources":[{"srcSet":"/static/5c4ebc68a7723fc56115ed3fe749ff7a/0852d/johanpauwels.webp 38w,\n/static/5c4ebc68a7723fc56115ed3fe749ff7a/18188/johanpauwels.webp 75w,\n/static/5c4ebc68a7723fc56115ed3fe749ff7a/c65bc/johanpauwels.webp 150w","type":"image/webp","sizes":"(min-width: 150px) 150px, 100vw"}]},"width":150,"height":150}}},"name":"Dr Johan Pauwels","role":"Academic","url":"http://www.eecs.qmul.ac.uk/people/view/50775/johan-pauwels","acadposition":"Lecturer in Audio Signal Processing","blurb":"automatic music labelling, music information retrieval, music signal processing, machine learning for audio, chord/key/structure (joint) estimation, instrument identification, multi-track/channel audio, music transcription, graphical models, big data science","themes":["mir","mlist"]}},{"id":"325ba8a0-81fc-52cb-9ef9-af050f2212c4","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Dr Ken O'Hanlon","role":"Postdoc","url":"https://dblp.org/pid/120/7143.html","acadposition":"Postdoctoral Researcher","blurb":"Project: Fine-grained music source separation with deep learning models","themes":["mir","mlist"]}},{"id":"3ff53e35-a554-51cd-925e-caa7670bd21b","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8a898","images":{"fallback":{"src":"/static/234763aa1e5f363371e4bfff769cc48d/70fef/katarzynaadamska.jpg","srcSet":"/static/234763aa1e5f363371e4bfff769cc48d/463c6/katarzynaadamska.jpg 379w,\n/static/234763aa1e5f363371e4bfff769cc48d/53da6/katarzynaadamska.jpg 758w,\n/static/234763aa1e5f363371e4bfff769cc48d/70fef/katarzynaadamska.jpg 1516w","sizes":"(min-width: 1516px) 1516px, 100vw"},"sources":[{"srcSet":"/static/234763aa1e5f363371e4bfff769cc48d/d4721/katarzynaadamska.webp 379w,\n/static/234763aa1e5f363371e4bfff769cc48d/6420f/katarzynaadamska.webp 758w,\n/static/234763aa1e5f363371e4bfff769cc48d/79063/katarzynaadamska.webp 1516w","type":"image/webp","sizes":"(min-width: 1516px) 1516px, 100vw"}]},"width":1516,"height":1516}}},"name":"Katarzyna Adamska","role":"PhD","url":"https://www.qmul.ac.uk/eecs/people/profiles/adamskakatarzynamaria.html","acadposition":"PhD Student","blurb":"Predicting hit songs: multimodal and data-driven approach","themes":["mir","audioeng","mcog"]}},{"id":"aaea13f6-1fd2-57d5-b574-a5cc0cee002d","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#3898d8","images":{"fallback":{"src":"/static/7712519d3052de12533c72fb8d0992e0/73bb6/linwang.jpg","srcSet":"/static/7712519d3052de12533c72fb8d0992e0/3c559/linwang.jpg 30w,\n/static/7712519d3052de12533c72fb8d0992e0/93848/linwang.jpg 60w,\n/static/7712519d3052de12533c72fb8d0992e0/73bb6/linwang.jpg 120w","sizes":"(min-width: 120px) 120px, 100vw"},"sources":[{"srcSet":"/static/7712519d3052de12533c72fb8d0992e0/bde72/linwang.webp 30w,\n/static/7712519d3052de12533c72fb8d0992e0/927d1/linwang.webp 60w,\n/static/7712519d3052de12533c72fb8d0992e0/507b0/linwang.webp 120w","type":"image/webp","sizes":"(min-width: 120px) 120px, 100vw"}]},"width":120,"height":120}}},"name":"Dr Lin Wang","role":"Academic","url":"http://www.eecs.qmul.ac.uk/~linwang/","acadposition":"Lecturer in Applied Data Science and Signal Processing","blurb":"signal processing; machine learning; robot perception","themes":["mlist","mir"]}},{"id":"1fa35ab0-908b-5140-a844-e188ef28ccbb","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#d8d8d8","images":{"fallback":{"src":"/static/7e22cecf02e6af782ab7980be82a36ba/d189d/marikaitiprimenta.jpg","srcSet":"/static/7e22cecf02e6af782ab7980be82a36ba/65912/marikaitiprimenta.jpg 245w,\n/static/7e22cecf02e6af782ab7980be82a36ba/87a09/marikaitiprimenta.jpg 489w,\n/static/7e22cecf02e6af782ab7980be82a36ba/d189d/marikaitiprimenta.jpg 978w","sizes":"(min-width: 978px) 978px, 100vw"},"sources":[{"srcSet":"/static/7e22cecf02e6af782ab7980be82a36ba/c4230/marikaitiprimenta.webp 245w,\n/static/7e22cecf02e6af782ab7980be82a36ba/96688/marikaitiprimenta.webp 489w,\n/static/7e22cecf02e6af782ab7980be82a36ba/69912/marikaitiprimenta.webp 978w","type":"image/webp","sizes":"(min-width: 978px) 978px, 100vw"}]},"width":978,"height":978}}},"name":"Marikaiti Primenta","role":"Research Assistant","url":"https://www.linkedin.com/in/marikaiti-primenta-547082253","acadposition":"Research Assistant","blurb":"Project Maestro - AI Musical Analysis Platform","themes":["mir"]}},{"id":"3fffc866-8036-535d-8b12-673653a1e811","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#d8c8c8","images":{"fallback":{"src":"/static/22312dadef02a15635234aa9c85e6a6b/6f467/marksandler.jpg","srcSet":"/static/22312dadef02a15635234aa9c85e6a6b/99d04/marksandler.jpg 247w,\n/static/22312dadef02a15635234aa9c85e6a6b/72fdf/marksandler.jpg 494w,\n/static/22312dadef02a15635234aa9c85e6a6b/6f467/marksandler.jpg 988w","sizes":"(min-width: 988px) 988px, 100vw"},"sources":[{"srcSet":"/static/22312dadef02a15635234aa9c85e6a6b/f19fb/marksandler.webp 247w,\n/static/22312dadef02a15635234aa9c85e6a6b/dab72/marksandler.webp 494w,\n/static/22312dadef02a15635234aa9c85e6a6b/3c8a3/marksandler.webp 988w","type":"image/webp","sizes":"(min-width: 988px) 988px, 100vw"}]},"width":988,"height":988}}},"name":"Prof Mark Sandler ","role":"Academic","url":"http://www.eecs.qmul.ac.uk/people/view/3114/prof-mark-sandler","acadposition":"C4DM Director","blurb":"Digital Signal Processing, Digital Audio, Music Informatics, Audio Features, Semantic Audio, Immersive Audio, Studio Science, Music Data Science, Music Linked Data.","themes":["mir"]}},{"id":"d74e8c2b-56f9-50cf-a43b-8b0191803bff","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/661d84eba8a4676c5210510bd4b2c4b6/93106/mathieubarthet.webp","srcSet":"/static/661d84eba8a4676c5210510bd4b2c4b6/c23ec/mathieubarthet.webp 123w,\n/static/661d84eba8a4676c5210510bd4b2c4b6/02ea5/mathieubarthet.webp 246w,\n/static/661d84eba8a4676c5210510bd4b2c4b6/93106/mathieubarthet.webp 492w","sizes":"(min-width: 492px) 492px, 100vw"},"sources":[]},"width":492,"height":492}}},"name":"Dr Mathieu Barthet","role":"Academic","url":"http://www.eecs.qmul.ac.uk/people/view/4808/dr-mathieu-barthet","acadposition":"Senior Lecturer in Digital Media","blurb":"Music information research, Internet of musical things, Extended reality, New interfaces for musical expression, Semantic audio, Music perception (timbre, emotions), Audience-Performer interaction, Participatory art","themes":["mir","augmi","isam","mupae"]}},{"id":"de8a6a9d-d9db-5d6f-aa49-d6383eab8ec3","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#d8c8c8","images":{"fallback":{"src":"/static/525d06e75294a7bf61d839700b1d2be5/3c342/ningzhiwang.jpg","srcSet":"/static/525d06e75294a7bf61d839700b1d2be5/19f6e/ningzhiwang.jpg 106w,\n/static/525d06e75294a7bf61d839700b1d2be5/a8230/ningzhiwang.jpg 212w,\n/static/525d06e75294a7bf61d839700b1d2be5/3c342/ningzhiwang.jpg 423w","sizes":"(min-width: 423px) 423px, 100vw"},"sources":[{"srcSet":"/static/525d06e75294a7bf61d839700b1d2be5/7a2a0/ningzhiwang.webp 106w,\n/static/525d06e75294a7bf61d839700b1d2be5/fbd6b/ningzhiwang.webp 212w,\n/static/525d06e75294a7bf61d839700b1d2be5/fb6ab/ningzhiwang.webp 423w","type":"image/webp","sizes":"(min-width: 423px) 423px, 100vw"}]},"width":423,"height":423}}},"name":"Ningzhi Wang","role":"PhD","url":"","acadposition":"PhD Student","blurb":"Generative Models For Music Audio Representation And Understanding","themes":["mir","mcog","soundsynthesis"]}},{"id":"9be482ab-e543-5ed7-a0ec-2d37af553450","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#7898b8","images":{"fallback":{"src":"/static/2046c4feea56b3899883569f75a5fcd1/e9d3f/rubycrocker.jpg","srcSet":"/static/2046c4feea56b3899883569f75a5fcd1/f3d60/rubycrocker.jpg 495w,\n/static/2046c4feea56b3899883569f75a5fcd1/8c337/rubycrocker.jpg 990w,\n/static/2046c4feea56b3899883569f75a5fcd1/e9d3f/rubycrocker.jpg 1980w","sizes":"(min-width: 1980px) 1980px, 100vw"},"sources":[{"srcSet":"/static/2046c4feea56b3899883569f75a5fcd1/72be1/rubycrocker.webp 495w,\n/static/2046c4feea56b3899883569f75a5fcd1/5c459/rubycrocker.webp 990w,\n/static/2046c4feea56b3899883569f75a5fcd1/83852/rubycrocker.webp 1980w","type":"image/webp","sizes":"(min-width: 1980px) 1980px, 100vw"}]},"width":1980,"height":1980}}},"name":"Ruby Crocker","role":"PhD","url":"","acadposition":"PhD Student","blurb":"Continuous mood recognition in film music","themes":["mir","mcog"]}},{"id":"b127d3bf-6578-5ca1-a678-3ca35503def2","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Sebastian Löbbers","role":"Research Assistant","url":"https://sebastianlobbers.com/","acadposition":"Research Assistant","blurb":"UKRI Centre for Doctoral Training in AI and Music","themes":["mir"]}},{"id":"06196503-f058-5081-ab1e-21e9b406e714","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/91f7a353a15c6e68c71f84ff92066743/ffdfb/simondixon.png","srcSet":"/static/91f7a353a15c6e68c71f84ff92066743/5bd84/simondixon.png 92w,\n/static/91f7a353a15c6e68c71f84ff92066743/f9b49/simondixon.png 184w,\n/static/91f7a353a15c6e68c71f84ff92066743/ffdfb/simondixon.png 367w","sizes":"(min-width: 367px) 367px, 100vw"},"sources":[{"srcSet":"/static/91f7a353a15c6e68c71f84ff92066743/483b8/simondixon.webp 92w,\n/static/91f7a353a15c6e68c71f84ff92066743/42e3f/simondixon.webp 184w,\n/static/91f7a353a15c6e68c71f84ff92066743/46fdb/simondixon.webp 367w","type":"image/webp","sizes":"(min-width: 367px) 367px, 100vw"}]},"width":367,"height":367}}},"name":"Prof Simon Dixon","role":"Academic","url":"http://www.eecs.qmul.ac.uk/~simond/","acadposition":"Professor of Computer Science, Deputy Director of C4DM, Director of the AIM CDT","blurb":"Music informatics, music signal processing, artificial intelligence, music cognition; extraction of musical content (e.g. rhythm, harmony, intonation) from audio signals: beat tracking, audio alignment, chord and note transcription, singing intonation; using signal processing approaches, probabilistic models, and deep learning.","themes":["mir"]}},{"id":"18bb4e4a-ca3d-51a0-9beb-5aea409131ab","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Vjosa Preniqi","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/preniqivjosa.html","acadposition":"PhD Student","blurb":"Predicting demographics, personalities, and global values from digital media behaviours","themes":["mir","mcog"]}},{"id":"aac95703-2ad6-5435-ba42-e404dc3a150e","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/static/4ea878db394500ed14f41f522a4afb01/b74b1/tylermcintosh.jpg","srcSet":"/static/4ea878db394500ed14f41f522a4afb01/d3fc0/tylermcintosh.jpg 270w,\n/static/4ea878db394500ed14f41f522a4afb01/19455/tylermcintosh.jpg 540w,\n/static/4ea878db394500ed14f41f522a4afb01/b74b1/tylermcintosh.jpg 1080w","sizes":"(min-width: 1080px) 1080px, 100vw"},"sources":[{"srcSet":"/static/4ea878db394500ed14f41f522a4afb01/ede49/tylermcintosh.webp 270w,\n/static/4ea878db394500ed14f41f522a4afb01/4cb34/tylermcintosh.webp 540w,\n/static/4ea878db394500ed14f41f522a4afb01/4f506/tylermcintosh.webp 1080w","type":"image/webp","sizes":"(min-width: 1080px) 1080px, 100vw"}]},"width":1080,"height":1080}}},"name":"Tyler Howard McIntosh","role":"PhD","url":"","acadposition":"PhD Student","blurb":"Expressive Performance Rendering for Music Generation Systems","themes":["mcog","mir"]}},{"id":"d964f102-7a03-5e39-8ca5-d62171217446","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8e8","images":{"fallback":{"src":"/static/d9e4891e02c2d5580fb05022babf8891/4ce45/xavierriley.jpg","srcSet":"/static/d9e4891e02c2d5580fb05022babf8891/21980/xavierriley.jpg 167w,\n/static/d9e4891e02c2d5580fb05022babf8891/f560f/xavierriley.jpg 333w,\n/static/d9e4891e02c2d5580fb05022babf8891/4ce45/xavierriley.jpg 666w","sizes":"(min-width: 666px) 666px, 100vw"},"sources":[{"srcSet":"/static/d9e4891e02c2d5580fb05022babf8891/9b205/xavierriley.webp 167w,\n/static/d9e4891e02c2d5580fb05022babf8891/e1538/xavierriley.webp 333w,\n/static/d9e4891e02c2d5580fb05022babf8891/bf451/xavierriley.webp 666w","type":"image/webp","sizes":"(min-width: 666px) 666px, 100vw"}]},"width":666,"height":666}}},"name":"Xavier Riley","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/rileyjohnxavier.html","acadposition":"PhD Student","blurb":"Pitch tracking for music applications - beyond 99% accuracy","themes":["mir","audioeng"]}},{"id":"f0e1d613-f724-56a3-bcb9-30f54b132dcb","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#e8e8e8","images":{"fallback":{"src":"/static/7371426282ff3e012ec154dd11dd684e/c29ce/xiaowanyi.jpg","srcSet":"/static/7371426282ff3e012ec154dd11dd684e/77813/xiaowanyi.jpg 545w,\n/static/7371426282ff3e012ec154dd11dd684e/160ed/xiaowanyi.jpg 1091w,\n/static/7371426282ff3e012ec154dd11dd684e/c29ce/xiaowanyi.jpg 2181w","sizes":"(min-width: 2181px) 2181px, 100vw"},"sources":[{"srcSet":"/static/7371426282ff3e012ec154dd11dd684e/9bbe1/xiaowanyi.webp 545w,\n/static/7371426282ff3e012ec154dd11dd684e/282f9/xiaowanyi.webp 1091w,\n/static/7371426282ff3e012ec154dd11dd684e/630ab/xiaowanyi.webp 2181w","type":"image/webp","sizes":"(min-width: 2181px) 2181px, 100vw"}]},"width":2181,"height":2181}}},"name":"Xiaowan Yi","role":"PhD","url":"https://www.qmul.ac.uk/eecs/people/profiles/yixiaowan.html","acadposition":"PhD Student","blurb":"Composition-aware music recommendation system for music production","themes":["mir","audioeng","isam"]}}]}}}
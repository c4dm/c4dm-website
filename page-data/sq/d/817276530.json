{"data":{"people":{"nodes":[{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#8898c8","images":{"fallback":{"src":"/static/bf8d7356ac6d442350ba1cb9bcc35445/baaed/andrewmcpherson.jpg","srcSet":"/static/bf8d7356ac6d442350ba1cb9bcc35445/dd515/andrewmcpherson.jpg 200w,\n/static/bf8d7356ac6d442350ba1cb9bcc35445/47930/andrewmcpherson.jpg 400w,\n/static/bf8d7356ac6d442350ba1cb9bcc35445/baaed/andrewmcpherson.jpg 800w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[{"srcSet":"/static/bf8d7356ac6d442350ba1cb9bcc35445/2e34e/andrewmcpherson.webp 200w,\n/static/bf8d7356ac6d442350ba1cb9bcc35445/416c3/andrewmcpherson.webp 400w,\n/static/bf8d7356ac6d442350ba1cb9bcc35445/c1587/andrewmcpherson.webp 800w","type":"image/webp","sizes":"(min-width: 800px) 800px, 100vw"}]},"width":800,"height":800}}},"name":"Prof Andrew McPherson","url":"http://www.eecs.qmul.ac.uk/~andrewm","acadposition":"Professor of Musical Interaction","blurb":"new interfaces for musical expression, augmented instruments, performance study, human-computer interaction, embedded hardware","themes":["augmi","soundsynthesis"],"role":"Academic"},"id":"5dd23ff3-3112-5b88-8b4e-f6493bdcdbac"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#685858","images":{"fallback":{"src":"/static/75b0458ab899e559446e8ecba1f3fb46/30f07/charissaitis.jpg","srcSet":"/static/75b0458ab899e559446e8ecba1f3fb46/41624/charissaitis.jpg 160w,\n/static/75b0458ab899e559446e8ecba1f3fb46/1b894/charissaitis.jpg 320w,\n/static/75b0458ab899e559446e8ecba1f3fb46/30f07/charissaitis.jpg 640w","sizes":"(min-width: 640px) 640px, 100vw"},"sources":[{"srcSet":"/static/75b0458ab899e559446e8ecba1f3fb46/60b4d/charissaitis.webp 160w,\n/static/75b0458ab899e559446e8ecba1f3fb46/5e011/charissaitis.webp 320w,\n/static/75b0458ab899e559446e8ecba1f3fb46/90d07/charissaitis.webp 640w","type":"image/webp","sizes":"(min-width: 640px) 640px, 100vw"}]},"width":640,"height":640}}},"name":"Dr Charalampos Saitis","url":"http://eecs.qmul.ac.uk/profiles/saitischaralampos.html","acadposition":"Lecturer in Digital Music Processing, Turing Fellow","blurb":"Communication acoustics, crossmodal correspondences, sound synthesis, cognitive audio, musical haptics","themes":["comma","mcog"],"role":"Academic"},"id":"05041b7b-ff55-5565-8f24-a036490ca8e5"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/8106f707dd10b39434bd1c08f1f22f6d/30f07/emmanouilbenetos.jpg","srcSet":"/static/8106f707dd10b39434bd1c08f1f22f6d/41624/emmanouilbenetos.jpg 160w,\n/static/8106f707dd10b39434bd1c08f1f22f6d/1b894/emmanouilbenetos.jpg 320w,\n/static/8106f707dd10b39434bd1c08f1f22f6d/30f07/emmanouilbenetos.jpg 640w","sizes":"(min-width: 640px) 640px, 100vw"},"sources":[{"srcSet":"/static/8106f707dd10b39434bd1c08f1f22f6d/60b4d/emmanouilbenetos.webp 160w,\n/static/8106f707dd10b39434bd1c08f1f22f6d/5e011/emmanouilbenetos.webp 320w,\n/static/8106f707dd10b39434bd1c08f1f22f6d/90d07/emmanouilbenetos.webp 640w","type":"image/webp","sizes":"(min-width: 640px) 640px, 100vw"}]},"width":640,"height":640}}},"name":"Dr Emmanouil Benetos","url":"http://www.eecs.qmul.ac.uk/people/view/4741/dr-emmanouil-benetos","acadposition":"Reader in Machine Listening, Turing Fellow","blurb":"Machine listening, music information retrieval, computational sound scene analysis, machine learning for audio analysis, language models for music and audio, computational musicology","themes":["mir","mlist"],"role":"Academic"},"id":"ef8d4761-ec9b-58f0-bfa5-26159c60c38e"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#c8c8c8","images":{"fallback":{"src":"/static/ac1b43fa43492b09e0c056f15501f65b/7683a/geraintwiggins.webp","srcSet":"/static/ac1b43fa43492b09e0c056f15501f65b/9a807/geraintwiggins.webp 36w,\n/static/ac1b43fa43492b09e0c056f15501f65b/de323/geraintwiggins.webp 72w,\n/static/ac1b43fa43492b09e0c056f15501f65b/7683a/geraintwiggins.webp 143w","sizes":"(min-width: 143px) 143px, 100vw"},"sources":[]},"width":143,"height":143}}},"name":"Prof Geraint Wiggins ","url":"https://ai.vub.ac.be/team/geraint-wiggins/?utm_source=www.google.com&utm_medium=organic&utm_campaign=Google&referrer-analytics=1","acadposition":"Professor of Computational Creativity","blurb":"Computational Creativity, Artificial Intelligence, Music Cognition","themes":["mcog"],"role":"Academic"},"id":"4234442f-d4f9-593d-9fd5-5a76d6d71124"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#a8b8e8","images":{"fallback":{"src":"/static/eaf04fbbdffdb3aca0caed16cfdf1102/7900e/gyorgyfazekas.jpg","srcSet":"/static/eaf04fbbdffdb3aca0caed16cfdf1102/f713e/gyorgyfazekas.jpg 55w,\n/static/eaf04fbbdffdb3aca0caed16cfdf1102/e2afd/gyorgyfazekas.jpg 110w,\n/static/eaf04fbbdffdb3aca0caed16cfdf1102/7900e/gyorgyfazekas.jpg 219w","sizes":"(min-width: 219px) 219px, 100vw"},"sources":[{"srcSet":"/static/eaf04fbbdffdb3aca0caed16cfdf1102/938d3/gyorgyfazekas.webp 55w,\n/static/eaf04fbbdffdb3aca0caed16cfdf1102/8c6ff/gyorgyfazekas.webp 110w,\n/static/eaf04fbbdffdb3aca0caed16cfdf1102/3f57d/gyorgyfazekas.webp 219w","type":"image/webp","sizes":"(min-width: 219px) 219px, 100vw"}]},"width":219,"height":219}}},"name":"Dr George Fazekas","url":"http://eecs.qmul.ac.uk/~gyorgyf","acadposition":"Senior Lecturer","blurb":"Semantic Audio, Music Information Retrieval, Semantic Web for Music, Machine Learning and Data Science, Music Emotion Recognition, Interactive music sytems (e.g. intellignet editing, audio production and performance systems)","themes":["mir"],"role":"Academic"},"id":"9aa31664-07ac-5194-a7fe-146cab80fddb"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8a898","images":{"fallback":{"src":"/static/5c4ebc68a7723fc56115ed3fe749ff7a/96deb/johanpauwels.jpg","srcSet":"/static/5c4ebc68a7723fc56115ed3fe749ff7a/c81d1/johanpauwels.jpg 38w,\n/static/5c4ebc68a7723fc56115ed3fe749ff7a/91a6d/johanpauwels.jpg 75w,\n/static/5c4ebc68a7723fc56115ed3fe749ff7a/96deb/johanpauwels.jpg 150w","sizes":"(min-width: 150px) 150px, 100vw"},"sources":[{"srcSet":"/static/5c4ebc68a7723fc56115ed3fe749ff7a/0852d/johanpauwels.webp 38w,\n/static/5c4ebc68a7723fc56115ed3fe749ff7a/18188/johanpauwels.webp 75w,\n/static/5c4ebc68a7723fc56115ed3fe749ff7a/c65bc/johanpauwels.webp 150w","type":"image/webp","sizes":"(min-width: 150px) 150px, 100vw"}]},"width":150,"height":150}}},"name":"Dr Johan Pauwels","url":"http://www.eecs.qmul.ac.uk/people/view/50775/johan-pauwels","acadposition":"Lecturer in Audio Signal Processing","blurb":"automatic music labelling, music information retrieval, music signal processing, machine learning for audio, chord/key/structure (joint) estimation, instrument identification, multi-track/channel audio, music transcription, graphical models, big data science","themes":["mir","mlist"],"role":"Academic"},"id":"b3e3ac4a-cd06-5d69-8ac9-f47d73ef5c0b"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8d8f8","images":{"fallback":{"src":"/static/e3f990a7d5022b64b25e5c46e4a6d5bf/47930/joshuadreiss.jpg","srcSet":"/static/e3f990a7d5022b64b25e5c46e4a6d5bf/e07e1/joshuadreiss.jpg 100w,\n/static/e3f990a7d5022b64b25e5c46e4a6d5bf/dd515/joshuadreiss.jpg 200w,\n/static/e3f990a7d5022b64b25e5c46e4a6d5bf/47930/joshuadreiss.jpg 400w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/e3f990a7d5022b64b25e5c46e4a6d5bf/d8057/joshuadreiss.webp 100w,\n/static/e3f990a7d5022b64b25e5c46e4a6d5bf/2e34e/joshuadreiss.webp 200w,\n/static/e3f990a7d5022b64b25e5c46e4a6d5bf/416c3/joshuadreiss.webp 400w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":400}}},"name":"Prof. Joshua D Reiss","url":"http://www.eecs.qmul.ac.uk/~josh/","acadposition":"Professor of Audio Engineering","blurb":"sound engineering, intelligent audio production, sound synthesis, audio effects, automatic mixing","themes":["audioeng","soundsynthesis"],"role":"Academic"},"id":"ca2575a9-c402-50c9-a660-eea9648428a6"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/41bb8cd95ee27c26627d3667c74c073f/0c4eb/marcuspearce.jpg","srcSet":"/static/41bb8cd95ee27c26627d3667c74c073f/7b799/marcuspearce.jpg 46w,\n/static/41bb8cd95ee27c26627d3667c74c073f/66e87/marcuspearce.jpg 92w,\n/static/41bb8cd95ee27c26627d3667c74c073f/0c4eb/marcuspearce.jpg 183w","sizes":"(min-width: 183px) 183px, 100vw"},"sources":[{"srcSet":"/static/41bb8cd95ee27c26627d3667c74c073f/1a9ee/marcuspearce.webp 46w,\n/static/41bb8cd95ee27c26627d3667c74c073f/483b8/marcuspearce.webp 92w,\n/static/41bb8cd95ee27c26627d3667c74c073f/04870/marcuspearce.webp 183w","type":"image/webp","sizes":"(min-width: 183px) 183px, 100vw"}]},"width":183,"height":183}}},"name":"Dr Marcus Pearce","url":"https://www.marcus-pearce.com","acadposition":"Senior Lecturer in Sound & Music Processing","blurb":"Music Cognition, Auditory Perception, Empirical Aesthetics, Statistical Learning, Probabilistic Modelling.","themes":["mcog"],"role":"Academic"},"id":"ceebf4f8-06c5-5a36-9420-47c585bd8247"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#d8c8c8","images":{"fallback":{"src":"/static/22312dadef02a15635234aa9c85e6a6b/6f467/marksandler.jpg","srcSet":"/static/22312dadef02a15635234aa9c85e6a6b/99d04/marksandler.jpg 247w,\n/static/22312dadef02a15635234aa9c85e6a6b/72fdf/marksandler.jpg 494w,\n/static/22312dadef02a15635234aa9c85e6a6b/6f467/marksandler.jpg 988w","sizes":"(min-width: 988px) 988px, 100vw"},"sources":[{"srcSet":"/static/22312dadef02a15635234aa9c85e6a6b/f19fb/marksandler.webp 247w,\n/static/22312dadef02a15635234aa9c85e6a6b/dab72/marksandler.webp 494w,\n/static/22312dadef02a15635234aa9c85e6a6b/3c8a3/marksandler.webp 988w","type":"image/webp","sizes":"(min-width: 988px) 988px, 100vw"}]},"width":988,"height":988}}},"name":"Prof Mark Sandler ","url":"http://www.eecs.qmul.ac.uk/people/view/3114/prof-mark-sandler","acadposition":"C4DM Director, Turing Fellow, Royal Society Wolfson Research Merit award holder","blurb":"Digital Signal Processing, Digital Audio, Music Informatics, Audio Features, Semantic Audio, Immersive Audio, Studio Science, Music Data Science, Music Linked Data.","themes":["mir"],"role":"Academic"},"id":"3fffc866-8036-535d-8b12-673653a1e811"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/661d84eba8a4676c5210510bd4b2c4b6/93106/mathieubarthet.webp","srcSet":"/static/661d84eba8a4676c5210510bd4b2c4b6/c23ec/mathieubarthet.webp 123w,\n/static/661d84eba8a4676c5210510bd4b2c4b6/02ea5/mathieubarthet.webp 246w,\n/static/661d84eba8a4676c5210510bd4b2c4b6/93106/mathieubarthet.webp 492w","sizes":"(min-width: 492px) 492px, 100vw"},"sources":[]},"width":492,"height":492}}},"name":"Dr Mathieu Barthet","url":"http://www.eecs.qmul.ac.uk/people/view/4808/dr-mathieu-barthet","acadposition":"Senior Lecturer in Digital Media","blurb":"Music information research, Internet of musical things, Extended reality, New interfaces for musical expression, Semantic audio, Music perception (timbre, emotions), Audience-Performer interaction, Participatory art","themes":["mir","augmi","isam","mupae"],"role":"Academic"},"id":"d74e8c2b-56f9-50cf-a43b-8b0191803bff"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#c8c8c8","images":{"fallback":{"src":"/static/ea69131e3ae0eac8063b55e74677282d/eadd3/nickbryan-kinns.png","srcSet":"/static/ea69131e3ae0eac8063b55e74677282d/e9a79/nickbryan-kinns.png 160w,\n/static/ea69131e3ae0eac8063b55e74677282d/5f035/nickbryan-kinns.png 320w,\n/static/ea69131e3ae0eac8063b55e74677282d/eadd3/nickbryan-kinns.png 640w","sizes":"(min-width: 640px) 640px, 100vw"},"sources":[{"srcSet":"/static/ea69131e3ae0eac8063b55e74677282d/60b4d/nickbryan-kinns.webp 160w,\n/static/ea69131e3ae0eac8063b55e74677282d/5e011/nickbryan-kinns.webp 320w,\n/static/ea69131e3ae0eac8063b55e74677282d/90d07/nickbryan-kinns.webp 640w","type":"image/webp","sizes":"(min-width: 640px) 640px, 100vw"}]},"width":640,"height":640}}},"name":"Dr Nick Bryan-Kinns","url":"http://www.eecs.qmul.ac.uk/%7Enickbk/","acadposition":"Professor of Interaction Design. Visiting Professor of Interaction Design, Hunan University, China. Turing Fellow.","blurb":"Interaction Design with Audio #IDwA. Interactive Art, Interactive Music, Interactive Sonification. Design, Evaluation. Collaboration, Multi-person Interaction. Cross-Modal Interaction, Tangible Interaction.","themes":["isam"],"role":"Academic"},"id":"b0710775-dc9e-5f34-a0d7-297690a17b15"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/91f7a353a15c6e68c71f84ff92066743/ffdfb/simondixon.png","srcSet":"/static/91f7a353a15c6e68c71f84ff92066743/5bd84/simondixon.png 92w,\n/static/91f7a353a15c6e68c71f84ff92066743/f9b49/simondixon.png 184w,\n/static/91f7a353a15c6e68c71f84ff92066743/ffdfb/simondixon.png 367w","sizes":"(min-width: 367px) 367px, 100vw"},"sources":[{"srcSet":"/static/91f7a353a15c6e68c71f84ff92066743/483b8/simondixon.webp 92w,\n/static/91f7a353a15c6e68c71f84ff92066743/42e3f/simondixon.webp 184w,\n/static/91f7a353a15c6e68c71f84ff92066743/46fdb/simondixon.webp 367w","type":"image/webp","sizes":"(min-width: 367px) 367px, 100vw"}]},"width":367,"height":367}}},"name":"Prof. Simon Dixon","url":"http://www.eecs.qmul.ac.uk/~simond/","acadposition":"Professor of Computer Science, Deputy Director of C4DM, Director of the AIM CDT, Turing Fellow","blurb":"Music informatics, music signal processing, artificial intelligence, music cognition; extraction of musical content (e.g. rhythm, harmony, intonation) from audio signals: beat tracking, audio alignment, chord and note transcription, singing intonation; using signal processing approaches, probabilistic models, and deep learning.","themes":["mir"],"role":"Academic"},"id":"06196503-f058-5081-ab1e-21e9b406e714"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/5e2239e574c579219804778dc3392423/e07e1/tonystockman.jpg","srcSet":"/static/5e2239e574c579219804778dc3392423/74ef0/tonystockman.jpg 25w,\n/static/5e2239e574c579219804778dc3392423/6ac16/tonystockman.jpg 50w,\n/static/5e2239e574c579219804778dc3392423/e07e1/tonystockman.jpg 100w","sizes":"(min-width: 100px) 100px, 100vw"},"sources":[{"srcSet":"/static/5e2239e574c579219804778dc3392423/2fa99/tonystockman.webp 25w,\n/static/5e2239e574c579219804778dc3392423/dbc4a/tonystockman.webp 50w,\n/static/5e2239e574c579219804778dc3392423/d8057/tonystockman.webp 100w","type":"image/webp","sizes":"(min-width: 100px) 100px, 100vw"}]},"width":100,"height":100}}},"name":"Dr Tony Stockman","url":"http://www.eecs.qmul.ac.uk/people/view/3026/dr-tony-stockman","acadposition":"Senior Lecturer","blurb":"Interaction Design, auditory displays, Data Sonification, Collaborative Systems, Cross-modal Interaction, Assistive Technology, Accessibility","themes":["isam"],"role":"Academic"},"id":"f6a0206a-60ef-5eb9-b2f5-ac4807ebb164"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Prof Matthew Purver","url":"http://www.eecs.qmul.ac.uk/~mpurver/","acadposition":"Professor of Computational Linguistics, Turing Fellow","blurb":"computational linguistics including models of language and music","themes":[],"role":"Academic Associate"},"id":"8f91717c-d7c5-5427-892b-0c25ab0e8f83"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Prof Pat Healey","url":"http://www.eecs.qmul.ac.uk/%7Eph/","acadposition":"Professor of Human Interaction, Turing Fellow","blurb":"","themes":["isam"],"role":"Academic Associate"},"id":"4416d8c8-6a83-56a1-bf38-acb52ed928f8"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Adam Andrew Garrow","url":"https://www.researchgate.net/profile/Adam-Garrow","acadposition":"PhD Student","blurb":": Probabilistic learning of sequential structures in music cognition","themes":["mcog"],"role":"PhD"},"id":"a912c176-1a88-5049-a749-4fb123746635"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Adán Benito","url":"","acadposition":"PhD Student","blurb":"Beyond the fret: gesture analysis on fretted instruments and its applications to instrument augmentation","themes":["augmi"],"role":"PhD"},"id":"b7db6ff1-b6a2-5313-897a-a6b32abcdff9"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Aditya Bhattacharjee","url":"https://www.linkedin.com/in/adibh/","acadposition":"PhD Student","blurb":"Self-supervision in Audio Fingerprinting","themes":["mir","mlist"],"role":"PhD"},"id":"226ac21f-bb0b-56df-9975-41dc5d56d6ca"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Alexander Williams","url":"","acadposition":"PhD Student","blurb":"User-driven deep music generation in digital audio workstations","themes":["mir","audioeng"],"role":"PhD"},"id":"cc381a30-c5d2-5ff7-b60d-1f10d81dbf9b"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Andrea Guidi","url":"https://mat.qmul.ac.uk/students/andrea-guidi","acadposition":"PhD Student","blurb":"Design for auditory imagery","themes":["mir"],"role":"PhD"},"id":"5bc5dee9-7eda-5f71-aed9-1b5319da59d7"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Andrea Martelloni","url":"http://eecs.qmul.ac.uk/profiles/martelloniandrea-1.html","acadposition":"PhD Student","blurb":"Real-Time Gesture Classification on an Augmented Acoustic Guitar using Deep Learning to Improve Extended-Range and Percussive Solo Playing","themes":["mir"],"role":"PhD"},"id":"d35868ef-7cda-5e98-aa09-bb356da9e5d3"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Andrew (Drew) Edwards","url":"http://eecs.qmul.ac.uk/profiles/edwardsandrewcharles.html","acadposition":"PhD Student","blurb":"Deep Learning for Jazz Piano: Transcription + Generative Modeling","themes":["mir","mlist","mcog"],"role":"PhD"},"id":"738dd58b-17cc-53bb-b319-0186faf690cc"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Antonella Torrisi","url":"https://www.researchgate.net/profile/Antonella-Torrisi","acadposition":"PhD Student","blurb":"Computational analysis of chick vocalisations: from categorisation to live feedback","themes":["mlist"],"role":"PhD"},"id":"58ba2962-4189-5258-a6b6-36025195cada"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Arjun Pankajakshan","url":"https://sites.google.com/view/arjunpc4dm/home","acadposition":"PhD Student","blurb":"Computational sound scene analysis","themes":["mlist"],"role":"PhD"},"id":"985d5a58-5247-595e-959a-3d8e6e7a38c9"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#685848","images":{"fallback":{"src":"/static/c0f574ea604c6ff9084a488c4bc7e4e5/0be83/ashleynoelhirst.png","srcSet":"/static/c0f574ea604c6ff9084a488c4bc7e4e5/5f035/ashleynoelhirst.png 320w,\n/static/c0f574ea604c6ff9084a488c4bc7e4e5/eadd3/ashleynoelhirst.png 640w,\n/static/c0f574ea604c6ff9084a488c4bc7e4e5/0be83/ashleynoelhirst.png 1280w","sizes":"(min-width: 1280px) 1280px, 100vw"},"sources":[{"srcSet":"/static/c0f574ea604c6ff9084a488c4bc7e4e5/5e011/ashleynoelhirst.webp 320w,\n/static/c0f574ea604c6ff9084a488c4bc7e4e5/90d07/ashleynoelhirst.webp 640w,\n/static/c0f574ea604c6ff9084a488c4bc7e4e5/9e21f/ashleynoelhirst.webp 1280w","type":"image/webp","sizes":"(min-width: 1280px) 1280px, 100vw"}]},"width":1280,"height":1280}}},"name":"Ashley Noel-Hirst","url":"https://ashleynoelhirst.co.uk/","acadposition":"PhD Student","blurb":"Latent Spaces for Human-AI music generation","themes":["mir","isam"],"role":"PhD"},"id":"9a6273ce-dc95-5d23-b5f9-7bcebfdc1772"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Benjamin Hayes","url":"http://eecs.qmul.ac.uk/profiles/hayesbenjaminjames.html","acadposition":"PhD Student","blurb":"Perceptually motivated deep learning approaches to creative sound synthesis","themes":["soundsynthesis","mcog"],"role":"PhD"},"id":"7920f116-7143-57a4-b40b-9325b220e835"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Berker Banar","url":"http://eecs.qmul.ac.uk/profiles/banarberker.html","acadposition":"PhD Student","blurb":"Towards Composing Contemporary Classical Music using Generative Deep Learning","themes":["mir","soundsynthesis"],"role":"PhD"},"id":"3e2bbfc8-0f9b-5aac-ba55-b0f8ffc899b9"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Bleiz Del Sette","url":"https://comma.eecs.qmul.ac.uk/people/bleiz/","acadposition":"PhD Student","blurb":"The Sound of Care: researching the use of Deep Learning and Sonification for the daily support of people with Chronic Primary Pain","themes":["comma"],"role":"PhD"},"id":"3aef98b5-03b4-5cbc-8469-3c762163024d"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Brendan O'Connor","url":"https://trebolium.github.io/","acadposition":"PhD Student","blurb":"Singing Voice Attribute Transformation","themes":["soundsynthesis","audioeng","mir"],"role":"PhD"},"id":"66b87075-3456-5b0c-9dfe-f91b43204096"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Carey Bunks","url":"","acadposition":"PhD Student","blurb":"Cover Song Identification","themes":["mir"],"role":"PhD"},"id":"4f0a9d6f-a76c-5668-832e-de53c816bfd5"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Carlos Lordelo","url":"https://cpvlordelo.github.io/","acadposition":"PhD Student","blurb":"Instrument modelling to aid polyphonic transcription","themes":["mir","mlist"],"role":"PhD"},"id":"fbc098ab-2b5e-5325-ae02-ad67d316619d"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Christopher Mitcheltree","url":"https://christhetr.ee","acadposition":"PhD Student","blurb":"Representation Learning for Audio Production Style and Modulations","themes":["mlist","audioeng","mir"],"role":"PhD"},"id":"c210c41b-f202-57d7-ab0c-14538a1c59bc"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Chris Winnard","url":"http://eecs.qmul.ac.uk/profiles/winnardchristopherjames.html","acadposition":"PhD Student","blurb":"Music Interestingness in the Brain","themes":["mcog"],"role":"PhD"},"id":"130ff14f-3659-5f6c-b154-4bacc73ae9de"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Corey Ford","url":"http://eecs.qmul.ac.uk/profiles/fordcoreyjohn.html","acadposition":"PhD Student","blurb":"Artificial Intelligence for Supporting Musical Creativity and Engagement in Child-Computer Interaction","themes":["isam"],"role":"PhD"},"id":"6979547e-9550-5346-8b55-b396e4bd2c33"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Cyrus Vahidi","url":"http://eecs.qmul.ac.uk/profiles/vahidicyrus.html","acadposition":"PhD Student","blurb":"Perceptual end to end learning for music understanding","themes":["mir"],"role":"PhD"},"id":"1e774e43-fee0-57f5-b2b5-632fe8762c2a"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Dalia Senvaityte","url":"http://eecs.qmul.ac.uk/profiles/senvaitytedalia.html","acadposition":"PhD Student","blurb":"Audio Source Separation for Advanced Digital Audio Effects","themes":["mlist"],"role":"PhD"},"id":"0ea57281-3f4b-5fad-8b3f-4a0ed88ad368"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"David Foster","url":"http://eecs.qmul.ac.uk/profiles/fosterdavid.html","acadposition":"PhD Student","blurb":"Modelling the Creative Process of Jazz Improvisation","themes":["mir"],"role":"PhD"},"id":"f855554f-b1f4-56be-8d59-801f8b9664fe"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"David Südholt","url":"https://dsuedholt.github.io/","acadposition":"PhD Student","blurb":"Machine Learning of Physical Models for Voice Synthesis","themes":["soundsynthesis","audioeng"],"role":"PhD"},"id":"612a6483-8966-5db8-9744-f6fd1021a11a"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Edward Hall","url":"https://mat.qmul.ac.uk/students/edward-hall","acadposition":"PhD Student","blurb":"Probabilistic modelling of thematic development and structural coherence in music","themes":["mir"],"role":"PhD"},"id":"70aeaa3d-8fcd-545e-913c-259db12737c3"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Eleanor Row","url":"http://eecs.qmul.ac.uk/profiles/roweleanorroxannevictoria.html","acadposition":"PhD Student","blurb":"Automatic micro-composition for professional/novice composers using generative models as creativity support tools","themes":["soundsynthesis"],"role":"PhD"},"id":"2b006670-df63-567c-9c51-62d19c6b1372"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Elizabeth Wilson","url":"https://lwlsn.github.io","acadposition":"PhD Student","blurb":"Co-creative Algorithmic Composition Based on Models of Affective Response","themes":["mir","isam"],"role":"PhD"},"id":"b50fe044-034c-561a-9c0c-70e7b9811648"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Elona Shatri","url":"http://eecs.qmul.ac.uk/profiles/shatrielona-1.html","acadposition":"PhD Student","blurb":"Optical music recognition using deep learning","themes":["mir"],"role":"PhD"},"id":"7b043f0b-de2f-5155-9877-1dfff2b8c117"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Gary Bromham","url":"","acadposition":"PhD Student","blurb":"The role of nostalga in music production","themes":["mir","audioeng"],"role":"PhD"},"id":"1b6798fe-1d19-52a2-add1-70afa99577f6"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Giacomo Lepri","url":"http://www.giacomolepri.com/","acadposition":"PhD Student","blurb":"Exploring the role of culture and community in the design of new musical instruments","themes":["augmi"],"role":"PhD"},"id":"1de88209-45cc-5adc-9c32-a383914bf687"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Harnick Khera","url":"http://eecs.qmul.ac.uk/profiles/kheraharnicksingh.html","acadposition":"PhD Student","blurb":"Informed source separation for multi-mic production","themes":["mir","mlist"],"role":"PhD"},"id":"b71f6653-2c4e-5f2b-b283-2eb0ae2679b6"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Huan Zhang","url":"http://eecs.qmul.ac.uk/people/profiles/zhanghuan.html","acadposition":"PhD Student","blurb":"Computational Modelling of Expressive Piano Performance","themes":["mir"],"role":"PhD"},"id":"1918386c-730c-5e3e-9d42-0369c7ed79b1"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Iacopo Ghinassi","url":"https://github.com/Ighina","acadposition":"PhD Student","blurb":"Semantic understanding of TV programme content and structure to enable automatic enhancement and adjustment","themes":["mir"],"role":"PhD"},"id":"27c0231d-35a2-57d5-8273-8562fba3319f"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Ilaria Manco","url":"http://eecs.qmul.ac.uk/profiles/mancoilaria.html","acadposition":"PhD Student","blurb":"Multimodal Deep Learning for Music Information Retrieval","themes":["mir","mlist"],"role":"PhD"},"id":"8f618d2e-72c3-5ff1-a65f-f14410f7ea3e"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Jinhua Liang","url":"https://jinhualiang.github.io/","acadposition":"PhD Student","blurb":"AI for everyday sounds","themes":["mlist"],"role":"PhD"},"id":"8a1a3e61-edf2-52d6-971f-a78142996fe7"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Jordie Shier","url":"https://jordieshier.com/","acadposition":"PhD Student","blurb":"Real-time timbral mapping for synthesized percussive performance","themes":["comma","augmi"],"role":"PhD"},"id":"df913101-13d0-56f9-9579-b4f5856ab887"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Lele Liu","url":"http://eecs.qmul.ac.uk/profiles/liulele.html","acadposition":"PhD Student","blurb":"Automatic music transcription with end-to-end deep neural networks","themes":["mir","mlist"],"role":"PhD"},"id":"12a808a3-c5af-5b8f-927d-9078a2f03d06"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Lewis Wolstanholme","url":"http://lewiswolstanholme.co.uk","acadposition":"PhD Student","blurb":"Meta-Physical Modelling","themes":["sounsynthesis","augmi"],"role":"PhD"},"id":"3111f86d-c11a-5074-9838-7ea3cb9e29ce"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Louise Thorpe","url":"","acadposition":"PhD Student","blurb":"Using Signal-informed Source Separation (SISS) principles to improve instrument separation from legacy recordings","themes":["mir","audioeng"],"role":"PhD"},"id":"f9561550-c8f0-5400-b97b-10f940bd70bd"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Luca Marinelli","url":"http://eecs.qmul.ac.uk/profiles/marinelliluca.html","acadposition":"PhD Student","blurb":"Gender-coded sound: A multimodal data-driven analysis of gender encoding strategies in sound and music for advertising","themes":["mlist"],"role":"PhD"},"id":"cea004c4-13f5-5347-9b2a-38a2ac5bbfe0"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Madeline Hamilton","url":"http://eecs.qmul.ac.uk/profiles/hamiltonmadelineann.html","acadposition":"PhD Student","blurb":"Improving AI-generated Music with Pleasure Models","themes":["mcog"],"role":"PhD"},"id":"18a76b0f-6172-5ea5-8b46-f8a3f742a720"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Marco Comunità","url":"http://eecs.qmul.ac.uk/profiles/comunitamarco.html","acadposition":"PhD Student","blurb":"Machine learning applied to sound synthesis models","themes":["audioeng","soundsynthesis"],"role":"PhD"},"id":"96844f85-b589-5907-be07-6cc39213f206"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Maryam Torshizi","url":"","acadposition":"PhD Student","blurb":"Music emotion modelling using graph analysis","themes":["comma","mir"],"role":"PhD"},"id":"4743033f-175f-5477-9ea2-bb9f890dcb85"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Mary Pilataki","url":"https://github.com/marypilataki","acadposition":"PhD Student","blurb":"Deep Learning methods for Multi-Instrument Music Transcription","themes":["mir"],"role":"PhD"},"id":"9b6c0163-cfdc-560b-82fe-8b8853e02502"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Max Graf","url":"http://eecs.qmul.ac.uk/profiles/grafmax.html","acadposition":"PhD Student","blurb":"PERFORM-AI (Provide Extended Realities for Musical Performance using AI)","themes":["augmi","isam","soundsynthesis"],"role":"PhD"},"id":"c5ddd5bb-999c-511e-91f9-f17ae20473e7"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Nelly Garcia","url":"","acadposition":"PhD Student","blurb":"An investigation evaluating realism in sound design","themes":["comma","audioeng"],"role":"PhD"},"id":"ade5f066-9369-5605-9c73-a41b0112d3cc"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Ningzhi Wang","url":"","acadposition":"PhD Student","blurb":"Generative Models For Music Audio Representation And Understanding","themes":["mir","mcog","soundsynthesis"],"role":"PhD"},"id":"de8a6a9d-d9db-5d6f-aa49-d6383eab8ec3"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Oluremi Falowo","url":null,"acadposition":"PhD Student","blurb":"E-AIM - Embodied Cognition in Intelligent Musical Systems","themes":["comma","mcog"],"role":"PhD"},"id":"2d224738-9643-599b-b5e9-e8922a346abd"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Pedro Sarmento","url":"https://otnemrasordep.github.io/","acadposition":"PhD Student","blurb":"Guitar-Oriented Neural Music Generation in Symbolic Format","themes":["mir"],"role":"PhD"},"id":"68891a8b-ed44-55dc-a3f7-f02f74bda436"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Teresa Pelinski","url":"https://www.teresapelinski.com/","acadposition":"PhD Student","blurb":"Sensor mesh as performance interface","themes":["augmi"],"role":"PhD"},"id":"7ac98d90-9688-59b9-b71a-d3215f550cf4"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Ruby Crocker","url":"","acadposition":"PhD Student","blurb":"Continuous mood recognition in film music","themes":["mir","mcog"],"role":"PhD"},"id":"9be482ab-e543-5ed7-a0ec-2d37af553450"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Saurjya Sarkar","url":"http://eecs.qmul.ac.uk/profiles/sarkarsaurjya-1.html","acadposition":"PhD Student","blurb":"New perspectives in instrument-based audio source separation","themes":["mir"],"role":"PhD"},"id":"fa3ed1ea-d5f0-59f2-92de-1a1aaa953db9"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Sebastián Ruiz","url":"","acadposition":"PhD Student","blurb":"Physiological Responses to Ensemble Interaction","themes":["mupae"],"role":"PhD"},"id":"40dfa6dc-8a20-5b1b-9126-67f99313d249"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Shubhr Singh","url":"http://eecs.qmul.ac.uk/profiles/singhshubhr.html","acadposition":"PhD Student","blurb":"Audio Applications of Novel Mathematical Methods in Deep Learning","themes":["soundsynthesis","mlist"],"role":"PhD"},"id":"72d3c05f-c5ce-5fe9-9c95-733813b814f7"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Soumya Sai Vanka","url":"http://eecs.qmul.ac.uk/profiles/vankasaisoumya.html","acadposition":"PhD Student","blurb":"Music Production Style Transfer and Mix Similarity","themes":["audioeng","mir"],"role":"PhD"},"id":"64e68bf1-afa9-5242-ba95-3bbb3ff80584"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#d8c8a8","images":{"fallback":{"src":"/static/d1179903e45c8a94827072c6928a258a/13677/teodannemann.png","srcSet":"/static/d1179903e45c8a94827072c6928a258a/de391/teodannemann.png 250w,\n/static/d1179903e45c8a94827072c6928a258a/82c11/teodannemann.png 500w,\n/static/d1179903e45c8a94827072c6928a258a/13677/teodannemann.png 1000w","sizes":"(min-width: 1000px) 1000px, 100vw"},"sources":[{"srcSet":"/static/d1179903e45c8a94827072c6928a258a/e7160/teodannemann.webp 250w,\n/static/d1179903e45c8a94827072c6928a258a/5f169/teodannemann.webp 500w,\n/static/d1179903e45c8a94827072c6928a258a/3cd29/teodannemann.webp 1000w","type":"image/webp","sizes":"(min-width: 1000px) 1000px, 100vw"}]},"width":1000,"height":1000}}},"name":"Teodoro Dannemann","url":"https://teodannemann.wordpress.com/","acadposition":"PhD Student","blurb":"Sabotaging, errors and other mistakes as a source of new techniques in music improvisation","themes":["augmi"],"role":"PhD"},"id":"ba04e9fb-b90e-5945-91de-586f456c0186"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Thomas Kaplan","url":"https://kappers.github.io/","acadposition":"PhD Student","blurb":"Probabilistic modelling of rhythm perception and production","themes":["mir"],"role":"PhD"},"id":"4fa7138d-03bd-56bc-8a7a-4c5cc66811f2"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Tyler Howard McIntosh","url":"","acadposition":"PhD Student","blurb":"Expressive Performance Rendering for Music Generation Systems","themes":["mcog","mir"],"role":"PhD"},"id":"aac95703-2ad6-5435-ba42-e404dc3a150e"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Vjosa Preniqi","url":"http://eecs.qmul.ac.uk/profiles/preniqivjosa.html","acadposition":"PhD Student","blurb":"Predicting demographics, personalities, and global values from digital media behaviours","themes":["mir","mcog"],"role":"PhD"},"id":"18bb4e4a-ca3d-51a0-9beb-5aea409131ab"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Xavier Riley","url":"http://eecs.qmul.ac.uk/profiles/rileyjohnxavier.html","acadposition":"PhD Student","blurb":"Pitch tracking for music applications - beyond 99% accuracy","themes":["mir","audioeng"],"role":"PhD"},"id":"d964f102-7a03-5e39-8ca5-d62171217446"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Yannis (John) Vasilakis","url":"https://www.linkedin.com/in/yannis-vasilakis-6bb9b11b1/","acadposition":"PhD Student","blurb":"Active Learning for Interactive Music Transcription","themes":["mlist","mir"],"role":"PhD"},"id":"04fb1f99-0166-5a9c-8f32-bbda54b39c18"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Yinghao Ma","url":"https://nicolaus625.github.io/","acadposition":"PhD Student","blurb":"Self-supervision in machine listening","themes":["mir","mlist"],"role":"PhD"},"id":"e0104201-8b94-5c6c-862b-a813f7cdc558"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Yin-Jyun Luo","url":"http://eecs.qmul.ac.uk/profiles/luoyin-jyun.html","acadposition":"PhD Student","blurb":"Industry-scale Machine Listening for Music and Audio Data","themes":["soundsynthesis","mlist"],"role":"PhD"},"id":"b699de08-bbee-5b10-bd93-9643604e2681"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Yixiao Zhang","url":"http://eecs.qmul.ac.uk/profiles/zhangyixiao.html","acadposition":"PhD Student","blurb":"Machine Learning Methods for Artificial Musicality","themes":["mir"],"role":"PhD"},"id":"8b81527a-163a-5125-9fc9-247a73a54a05"},{"frontmatter":{"image":null,"name":"Yukun Li","url":"http://eecs.qmul.ac.uk/profiles/liyukun.html","acadposition":"PhD Student","blurb":"Computational Comparison Between Different Genres of Music in Terms of the Singing Voice","themes":["mir"],"role":"PhD"},"id":"8d93ecc6-ed02-5662-a390-01b9e68ef5f5"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Chin-Yun Yu","url":"https://yoyololicon.github.io/","acadposition":"PhD Student","blurb":"Neural Audio Synthesis with Expressiveness Control","themes":["audioeng","mir","soundsynthesis"],"role":"PhD"},"id":"34ef929b-de59-5c95-a554-597f98d80226"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Chengye Wu","url":"","acadposition":"PhD Student","blurb":"Leveraging cross-sensory associations in communication","themes":["comma"],"role":"PhD"},"id":"f2cdbce0-2baf-5340-9d12-8e3926df8b52"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Christian Steinmetz","url":"https://www.christiansteinmetz.com/","acadposition":"PhD Student","blurb":"End-to-end generative modeling of multitrack mixing with non-parallel data and adversarial networks","themes":["audioeng"],"role":"PhD"},"id":"4dc93e11-7859-5d54-80fd-7f2c652e4c3a"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Dr Veronica Morfi","url":"https://scholar.google.co.uk/citations?user=8izRvu4AAAAJ&hl=en","acadposition":"Postdoctoral Researcher","blurb":"Machine transcription of wildlife bird sound scenes","themes":["mlist"],"role":"Postdoc"},"id":"dc0f7c30-da37-5bc0-b8a8-8e9adb2c3562"},{"frontmatter":{"image":null,"name":"Dr Yuanyuan Liu","url":"http://eecs.qmul.ac.uk/profiles/liuyuanyuan.html","acadposition":"Postdoctoral Researcher","blurb":"Project: Digital Platforms for Craft in the UK and China","themes":["isam"],"role":"Postdoc"},"id":"48db11d9-63d2-55c1-86e6-f79ab92d8fac"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Sungkyun Chang","url":"http://eecs.qmul.ac.uk/profiles/sungkyun-chang.html","acadposition":"Research Assistant","blurb":"Deep learning technologies for multi-instrument automatic music transcription","themes":["mir","mlist"],"role":"Research Assistant"},"id":"c4baa5bd-09a1-5c86-9e66-b8a4a269c168"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Alvaro Bort","url":"http://eecs.qmul.ac.uk/profiles/bortalvaro.html","acadposition":"Research Programme Manager","blurb":"Projects: UKRI Centre for Doctoral Training in Artificial Intelligence and Music, New Frontiers in Music Information Processing (MIP-Frontiers)","themes":[],"role":"Support"},"id":"2e846599-fb90-5878-8c6c-ece409f50c2f"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Jonathan Winfield","url":"http://eecs.qmul.ac.uk/profiles/winfieldjonathan.html","acadposition":"Research Programme Manager","blurb":"Project: Centre for Doctoral Training in Media and Arts Technology","themes":[],"role":"Support"},"id":"869629d0-2ddb-54b6-bc9c-00dbaa0fbc26"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Domenico Stefani","url":"https://domenicostefani.com/","acadposition":"University of Trento, Italy","blurb":"Embedded machine learning for smart musical instruments","themes":["mir"],"role":"Visitor"},"id":"50ed75b3-142a-592e-9edf-31ddba652941"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Dr Helen Bear","url":"http://www.eecs.qmul.ac.uk/profiles/bearhelen.html","acadposition":"Honorary Lecturer","blurb":"Integrating sound and context recognition for acoustic scene analysis","themes":["mlist"],"role":"Visitor"},"id":"c1949977-7fdd-5aae-a682-210665dbc32e"},{"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Dr Matthias Mauch","url":"http://www.matthiasmauch.net/","acadposition":"Visiting Academic","blurb":"music transcription (chords, beats, drums, melody, ...), interactive music annotation, singing research, research in the evolution of musical styles","themes":["mir"],"role":"Visitor"},"id":"680dbd18-ee36-5c29-8ae3-3c894e883317"}]},"allTags":{"group":[{"fieldValue":"Academic","totalCount":13},{"fieldValue":"Academic Associate","totalCount":2},{"fieldValue":"PhD","totalCount":67},{"fieldValue":"Postdoc","totalCount":2},{"fieldValue":"Research Assistant","totalCount":1},{"fieldValue":"Support","totalCount":2},{"fieldValue":"Visitor","totalCount":3}]}}}
{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/YIFgS8she58","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/Sandler-Artificial-Neuroscience"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png","srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/2fd20/EPSRC.png 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/de391/EPSRC.png 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/d66e1/EPSRC.webp 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/e7160/EPSRC.webp 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/5f169/EPSRC.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Artificial Neuroscience: metrology and engineering for Deep Learning using Linear Algebra","author":"Prof Mark Sandler (PI), Boris Khoruzhenko (CI)","date":null,"link":"https://www.ukri.org/councils/epsrc/"},"html":"","id":"c1314000-fe3b-5175-88a2-f239d861ddad"},{"fields":{"slug":"/projects/Barthet-Netz"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Netz: A Novel XR Musical Instrument","author":"Dr Mathieu Barthet (PI)","date":null,"link":"https://iuk-business-connect.org.uk/programme/icure/"},"html":"","id":"0040b455-65cf-5ea1-a7ce-38ab70570dae"},{"fields":{"slug":"/projects/Benetos-Algorivm"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Project Maestro - AI Musical Analysis Platform","author":"Dr Emmanouil Benetos (PI), Prof Simon Dixon (CI)","date":null,"link":"https://www.ukri.org/councils/innovate-uk/"},"html":"","id":"78c348d8-d78f-5acf-8441-382bc6c4d791"},{"fields":{"slug":"/projects/Benetos-L-Acoustics"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Online speech enhancement in scenarios with low direct-to-reverberant-ratio","author":"Dr Emmanouil Benetos (PI), Dr Aidan Hogg (CI)","date":null,"link":"https://www.l-acoustics.com/"},"html":"","id":"ce65fc6c-7228-5805-a9b2-6e14e5fd5086"},{"fields":{"slug":"/projects/Benetos-Moises"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Enhancing lyrics transcription with open-source architectures and fine-tuning techniques","author":"Dr Emmanouil Benetos (PI)","date":null,"link":"https://moises.ai/"},"html":"","id":"d61977ff-9345-55a4-b85e-44caffa4a7f0"},{"fields":{"slug":"/projects/Dixon-assessment"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Music Performance Assessment and Feedback","author":"Prof Simon Dixon (PI), Dr Emmanouil Benetos (CI)","date":null,"link":null},"html":"","id":"68ee56ea-59b7-58de-a8c9-805c4aba4357"}]},"news":{"nodes":[{"fields":{"slug":"/news/2025-09-05.C4DM-AAAI_26_EAIM_Workshop"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM will co-host AI for Music workshop at AAAI-26 in Singapore","author":"Keshav Bhandari","date":"Fri 05 Sep 2025"},"html":"<p>Author: Keshav Bhandari</p>\n<p>C4DM is co-hosting a full-day AI for Music workshop titled \"1st International Workshop on Emerging AI Technologies for Music (EAIM)\" at the AAAI-26 conference in Singapore, from January 20-27, 2026. The workshop will explore how AI can be designed for controllability and human-centered collaboration, moving beyond automation toward systems that empower people in creative processes.</p>\n<p>The accepted papers will be published as a volume in the Proceedings of Machine Learning Research (PMLR). The paper submission deadline is October 24, 2025.</p>\n<p>Further information on topics, submission requirements, keynote speakers, and more is avaliable <a href=\"https://amaai-lab.github.io/EAIM2026/\">here</a></p>","id":"c7c159df-6dac-533f-b6cd-e8eaca8638f7"},{"fields":{"slug":"/news/2025-09-03.C4DM-at_UKAIRS"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/0a520cc6b532ebe8e12d32e1128fe298/1ff6a/ukairs_2025.png","srcSet":"/static/0a520cc6b532ebe8e12d32e1128fe298/a5ae6/ukairs_2025.png 334w,\n/static/0a520cc6b532ebe8e12d32e1128fe298/40a65/ukairs_2025.png 668w,\n/static/0a520cc6b532ebe8e12d32e1128fe298/1ff6a/ukairs_2025.png 1336w","sizes":"(min-width: 1336px) 1336px, 100vw"},"sources":[{"srcSet":"/static/0a520cc6b532ebe8e12d32e1128fe298/c6d30/ukairs_2025.webp 334w,\n/static/0a520cc6b532ebe8e12d32e1128fe298/27806/ukairs_2025.webp 668w,\n/static/0a520cc6b532ebe8e12d32e1128fe298/93676/ukairs_2025.webp 1336w","type":"image/webp","sizes":"(min-width: 1336px) 1336px, 100vw"}]},"width":1336,"height":1336}}},"title":"C4DM at UKAIRS 2025","author":"Christos Plachouras","date":"Wed 03 Sep 2025"},"html":"<p>On 8-10 September, C4DM members will participate in the <a href=\"https://www.ukairs.ac.uk/\">UK AI Research Symposium (UKAIRS)</a> in Northumbria University, Newcastle. UKAIRS will bring together researchers from different disciplines, aiming to inform UK AI research priorities, foster interdisciplinary collaboration, and shape discourse on delivering AI for societal and economic benefit.</p>\n<p>The following works C4DM members have been involved in will be presented:</p>\n<ul>\n<li><a href=\"https://hal.science/hal-04943901v1/document\">Towards Music Industry 5.0: Perspectives on Artificial Intelligence</a>, Alexander Williams, Mathieu Barthet</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/103081/Xambo%20Sedo%20Human-machine%20agencies%20in%20live%20coding%20for%20music%20performance%202024%20Accepted.pdf?sequence=2&#x26;isAllowed=y\">Human–machine agencies in live coding for music performance</a>, Anna Xambó, Gerard Roma</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/106803\">Towards a Unified Representation Evaluation Framework Beyond Downstream Tasks</a>, Christos Plachouras, Julien Guinot, George Fazekas, Elio Quinton, Emmanouil Benetos, Johan Pauwels</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/110219/Xambo%20Soundscape-based%20music%202025%20Accepted.pdf?sequence=2\">Soundscape-based music and creative AI: Insights and promises</a>, Anna Xambó, Peter Batchelor, Luigi Marino, Gerard Roma, Mike Bell, George Xenakis</li>\n<li>Split Fine-Tuning of BERT-based Music Models in the Edge-Cloud Continuum: An Empirical Analysis, Bradley Aldous, Ahmed M. A. Sayed</li>\n<li>Composing Kernel Models with Self-Attention, Carey Bunks, Simon Dixon, Bruno Di Giorgi</li>\n</ul>\n<p>Additionally, on 10 September, during the UKAIRS Early Career Researcher Day, C4DM PhD student Elona Shatri is organising the workshop <a href=\"https://www.ukairs.ac.uk/ecr-day/\">\"Should Ethics Committees Expand Their Mandates to Review AI Research?\"</a>.</p>\n<p>See you at UKAIRS!</p>","id":"0b82bc45-d8be-59c4-8e5c-cfef869e665a"},{"fields":{"slug":"/news/2025-09-03.C4DM-organises_AIMLA_2025_conference"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#083858","images":{"fallback":{"src":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/18c0c/AIMLA-logo.jpg","srcSet":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/0fdf4/AIMLA-logo.jpg 300w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/7706b/AIMLA-logo.jpg 601w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/18c0c/AIMLA-logo.jpg 1201w","sizes":"(min-width: 1201px) 1201px, 100vw"},"sources":[{"srcSet":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/078c3/AIMLA-logo.webp 300w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/2b014/AIMLA-logo.webp 601w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/1fcf1/AIMLA-logo.webp 1201w","type":"image/webp","sizes":"(min-width: 1201px) 1201px, 100vw"}]},"width":1201,"height":1201}}},"title":"C4DM organises AES AIMLA 2025 conference","author":"Admin","date":"Wed 03 Sep 2025"},"html":"<p>The <a href=\"https://aes2.org/contributions/2025-1st-aes-international-conference-on-artificial-intelligence-and-machine-learning-for-audio-call-for-contributions/\">AES International Conference on Artificial Intelligence and Machine Learning for Audio (AIMLA 2025)</a> will be hosted by the <a href=\"https://www.c4dm.eecs.qmul.ac.uk/\">Centre for Digital Music</a> of Queen Mary University of London and is taking place on Sept. 8-10, 2025.</p>\n<p>Several C4DM members are involved in the organisation of the conference, including but not limited to:</p>\n<ul>\n<li>Josh Reiss (General Chair)</li>\n<li>George Fazekas (Papers Co-chair)</li>\n<li>Soumya Vanka (Special Sessions Co-Chair)</li>\n<li>Franco Caspe (Special Sessions Co-Chair)</li>\n<li>Farida Yusuf (Sponsorship Chair)</li>\n<li>Emmanouil Benetos (Publicity Chair)</li>\n<li>Nelly Garcia (Social Events Coordinator)</li>\n<li>Ilias Ibnyahya (Treasurer)</li>\n<li>Chin-Yun Yu (Late Breaking Papers Chair)</li>\n<li>Marikaiti Primenta (Invited Speakers Chair)</li>\n</ul>\n<p>Several papers and presentations will be made from C4DM members at AIMLA as well. The following peer-reviewed papers will be presented at the conference:</p>\n<ul>\n<li>\n<p><a href=\"https://aesaimla2025.sched.com/event/28Oay/nablafx-a-framework-for-differentiable-black-box-and-gray-box-modeling-of-audio-effects?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">NablAFx: A Framework for Differentiable Black-box and Gray-box Modeling of Audio Effects</a>, by Marco Comunità, Christian Steinmetz, Joshua Reiss</p>\n</li>\n<li>\n<p><a href=\"https://aesaimla2025.sched.com/event/28Ob1/transfer-learning-for-neural-modelling-of-nonlinear-distortion-effects?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">Transfer Learning for Neural Modelling of Nonlinear Distortion Effects</a>, by Tara Vanhatalo, Pierrick Legrand, Myriam Desainte-Catherine, Pierre Hanna, Guillaume Pille, Antoine Brusco, Joshua Reiss</p>\n</li>\n<li>\n<p><a href=\"https://aesaimla2025.sched.com/event/28Obz/sound-matching-an-analogue-levelling-amplifier-using-the-newton-raphson-method?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">Sound Matching an Analogue Levelling Amplifier Using the Newton-Raphson Method</a>, by Chin-Yun Yu, George Fazekas</p>\n</li>\n<li>\n<p><a href=\"https://aesaimla2025.sched.com/event/28OcW/procedural-music-generation-systems-in-games?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">Procedural Music Generation Systems in Games</a>, by Shangxuan Luo, Joshua Reiss</p>\n</li>\n<li>\n<p><a href=\"https://aesaimla2025.sched.com/event/28Obt/neutone-sdk-an-open-source-framework-for-neural-audio-processing?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">Neutone SDK: An Open Source Framework for Neural Audio Processing</a>, by Christopher Mitcheltree, Bogdan Teleaga, Andrew Fyfe, Naotake Masuda, Matthias Schäfer, Alfie Bradic, Nao Tokui</p>\n</li>\n</ul>\n<p>The following late-breaking posters from C4DM members will be presented at AIMLA:</p>\n<ul>\n<li>\n<p><a href=\"https://aesaimla2025.sched.com/event/28TJN/transformer-based-sustain-pedal-reconstruction-for-expressive-piano-performance-midi?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">Transformer-Based Sustain Pedal Reconstruction for Expressive Piano Performance MIDI</a>, by Wenhao Liu, George Fazekas, Jingjing Tang</p>\n</li>\n<li>\n<p><a href=\"https://aesaimla2025.sched.com/event/28TJ2/decoding-melodic-acoustic-features-from-neural-data?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">Decoding Melodic Acoustic Features from Neural Data</a>, by Zorka Bozilovic, Iran Roman</p>\n</li>\n<li>\n<p><a href=\"https://aesaimla2025.sched.com/event/28TJH/towards-intelligent-music-education-score-informed-transcription-and-performance-assessment?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">Towards Intelligent Music Education: Score-Informed Transcription and Performance Assessment</a>, by Jack Loth, Marikaiti Primenta, Jingjing Tang, Xavier Riley, Simon Dixon, Emmanouil Benetos</p>\n</li>\n<li>\n<p><a href=\"https://sched.co/28TJE\">MULTIVOX: Spatial &#x26; Multimodal Data of Singing Groups</a>, by Gerardo Meza Gómez, Iran Roman, Adrian Roman, Rodrigo Sigal Sefchovich, Mariana Sepúlveda</p>\n</li>\n</ul>\n<p>Last but not least, the following tutorial will be co-presented by C4DM PhD student Franco Caspe:</p>\n<ul>\n<li><a href=\"https://aesaimla2025.sched.com/event/28OaU/tutorial-real-time-neural-audio-inference?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">Real-Time Neural Audio Inference </a>, by Franco Caspe and Jatin Chowdhury</li>\n</ul>\n<p>See you in London!</p>","id":"8ca8145f-7bcc-55ae-b8a6-1a7b454f655e"},{"fields":{"slug":"/news/2025-07-29.C4DM-at_Interspeech_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#084898","images":{"fallback":{"src":"/static/fbc593813a2397e9f177cd960126bb83/869a6/Interspeech-2025.jpg","srcSet":"/static/fbc593813a2397e9f177cd960126bb83/97a19/Interspeech-2025.jpg 480w,\n/static/fbc593813a2397e9f177cd960126bb83/d4aca/Interspeech-2025.jpg 960w,\n/static/fbc593813a2397e9f177cd960126bb83/869a6/Interspeech-2025.jpg 1920w","sizes":"(min-width: 1920px) 1920px, 100vw"},"sources":[{"srcSet":"/static/fbc593813a2397e9f177cd960126bb83/21b1a/Interspeech-2025.webp 480w,\n/static/fbc593813a2397e9f177cd960126bb83/d6f60/Interspeech-2025.webp 960w,\n/static/fbc593813a2397e9f177cd960126bb83/26222/Interspeech-2025.webp 1920w","type":"image/webp","sizes":"(min-width: 1920px) 1920px, 100vw"}]},"width":1920,"height":1920}}},"title":"C4DM at Interspeech 2025","author":"admin","date":"Tue 29 Jul 2025"},"html":"<p>On 17-21 August, C4DM members will participate in <a href=\"https://www.interspeech2025.org/home\">Interspeech 2025</a>. Interspeech is the premier international conference for research on the science and technology of spoken language processing.</p>\n<p>The following papers authored/co-authored by C4DM members will be presented at Interspeech 2025:</p>\n<ul>\n<li>\n<p><a href=\"https://arxiv.org/abs/2505.23509\">Spectrotemporal Modulation: Efficient and Interpretable Feature Representation for Classifying Speech, Music, and Environmental Sounds</a> by Andrew Chang, Yike Li, Iran R. Roman, David Poeppel</p>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/abs/2506.02339\">Enhancing Lyrics Transcription on Music Mixtures with Consistency Loss</a> by Jiawen Huang, Felipe Sousa, Emir Demirel, Emmanouil Benetos, Igor Gadelha</p>\n</li>\n</ul>\n<p>See you at Interspeech!</p>","id":"6bc651c0-77cf-5031-a443-082420242949"},{"fields":{"slug":"/news/2025-07-28.AIM student to join the Alan Turing Institute in 2025-2026"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/84457edde2978607b183e69bea1840e1/cabd1/ATI_logo_black_W500px.jpg","srcSet":"/static/84457edde2978607b183e69bea1840e1/19e71/ATI_logo_black_W500px.jpg 128w,\n/static/84457edde2978607b183e69bea1840e1/68974/ATI_logo_black_W500px.jpg 256w,\n/static/84457edde2978607b183e69bea1840e1/cabd1/ATI_logo_black_W500px.jpg 511w","sizes":"(min-width: 511px) 511px, 100vw"},"sources":[{"srcSet":"/static/84457edde2978607b183e69bea1840e1/6766a/ATI_logo_black_W500px.webp 128w,\n/static/84457edde2978607b183e69bea1840e1/22bfc/ATI_logo_black_W500px.webp 256w,\n/static/84457edde2978607b183e69bea1840e1/9f973/ATI_logo_black_W500px.webp 511w","type":"image/webp","sizes":"(min-width: 511px) 511px, 100vw"}]},"width":511,"height":511}}},"title":"C4DM student to join the Alan Turing Institute in 2025-2026","author":"Admin","date":"Mon 28 Jul 2025"},"html":"<p>C4DM PhD student <a href=\"https://www.qmul.ac.uk/eecs/people/profiles/adityabhattacharjee-.html\">Aditya Bhattacharjee</a> has been awarded an <a href=\"https://www.turing.ac.uk/work-turing/studentships/enrichment\">enrichment placement</a> by the <a href=\"https://www.turing.ac.uk/\">Alan Turing Institute</a>, the UK’s national institute in artificial intelligence and data science, enabling him to join and interact with institute researchers and its community in the 2025/26 academic year. Aditya’s placement will be hosted by the Turing’s <a href=\"https://www.turing.ac.uk/research/research-programmes/fundamental-research\">Fundamental research in data science and AI</a> research programme.</p>\n<p>Congratulations to Aditya!</p>","id":"049b3ca8-72c8-5274-af77-62fb37ecb440"},{"fields":{"slug":"/news/2025-06-30.CfLBDP.AIMLA.2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#083858","images":{"fallback":{"src":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/18c0c/AIMLA-logo.jpg","srcSet":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/0fdf4/AIMLA-logo.jpg 300w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/7706b/AIMLA-logo.jpg 601w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/18c0c/AIMLA-logo.jpg 1201w","sizes":"(min-width: 1201px) 1201px, 100vw"},"sources":[{"srcSet":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/078c3/AIMLA-logo.webp 300w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/2b014/AIMLA-logo.webp 601w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/1fcf1/AIMLA-logo.webp 1201w","type":"image/webp","sizes":"(min-width: 1201px) 1201px, 100vw"}]},"width":1201,"height":1201}}},"title":"Call for Late Breaking Demo Papers: First AES International Conference on AI and Machine Learning for Audio (AIMLA 2025)","author":"Christos Plachouras","date":"Mon 30 Jun 2025"},"html":"<p>The AES International Conference on Artificial Intelligence and Machine Learning for Audio (AIMLA 2025), hosted at the Centre for Digital Music of Queen Mary University of London and taking place on Sept. 8-10, 2025 is calling for Late Breaking Demo Paper submissions.</p>\n<p>We are seeking 2-page extended abstracts showcasing prototype systems and early research results that are highly relevant to the conference theme. At least one author must register for the conference and present their work as a poster in person during the main track poster session.</p>\n<p>Submissions open on July 1, 2025 with a deadline of August 1, 2025. Submissions will be reviewed on a rolling basis.</p>\n<p>For more information on submission guidelines, templates, and technical requirements, please visit: <a href=\"https://aes2.org/contributions/2025-1st-aes-international-conference-on-artificial-intelligence-and-machine-learning-for-audio-call-for-contributions/\">https://aes2.org/contributions/2025-1st-aes-international-conference-on-artificial-intelligence-and-machine-learning-for-audio-call-for-contributions/</a></p>","id":"8db047da-7a1b-554b-b451-7b22c99c199a"}]}}}
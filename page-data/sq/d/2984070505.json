{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/YIFgS8she58","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/Benetos-L-Acoustics"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Online speech enhancement in scenarios with low direct-to-reverberant-ratio","author":"Dr Emmanouil Benetos (PI), Dr Aidan Hogg (CI)","date":null,"link":"https://www.l-acoustics.com/"},"html":"","id":"ce65fc6c-7228-5805-a9b2-6e14e5fd5086"},{"fields":{"slug":"/projects/Benetos-Moises"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Enhancing lyrics transcription with open-source architectures and fine-tuning techniques","author":"Dr Emmanouil Benetos (PI)","date":null,"link":"https://moises.ai/"},"html":"","id":"d61977ff-9345-55a4-b85e-44caffa4a7f0"},{"fields":{"slug":"/projects/Benetos-Algorivm"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Project Maestro - AI Musical Analysis Platform","author":"Dr Emmanouil Benetos (PI), Prof Simon Dixon (CI)","date":null,"link":"https://www.ukri.org/councils/innovate-uk/"},"html":"","id":"78c348d8-d78f-5acf-8441-382bc6c4d791"},{"fields":{"slug":"/projects/Dixon-assessment"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Music Performance Assessment and Feedback","author":"Prof Simon Dixon (PI), Dr Emmanouil Benetos (CI)","date":null,"link":null},"html":"","id":"68ee56ea-59b7-58de-a8c9-805c4aba4357"},{"fields":{"slug":"/projects/Dixon-guitar-transcription"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"High Resolution Guitar Transcription","author":"Prof Simon Dixon (PI)","date":null,"link":null},"html":"","id":"4dee84b0-faa4-549a-a883-d4de0a21e205"},{"fields":{"slug":"/projects/Fazekas-SmartEQ-AIM"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png","srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/4b686/aimcdt.png 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/d2213/aimcdt.png 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png 615w","sizes":"(min-width: 615px) 615px, 100vw"},"sources":[{"srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/b2942/aimcdt.webp 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/46581/aimcdt.webp 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/fa942/aimcdt.webp 615w","type":"image/webp","sizes":"(min-width: 615px) 615px, 100vw"}]},"width":615,"height":615}}},"title":"Smart EQ: Personalising Audio with Context-aware AI using Listener Preferences and Psychological Factors","author":"Dr György Fazekas (PI), Dr Charalampos Saitis (CI)","date":null,"link":null},"html":"","id":"8df63b90-0094-5360-a9c8-6d3c758d558e"}]},"news":{"nodes":[{"fields":{"slug":"/news/2024-10-28.C4DM-at_ISMIR_2024"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/23c0c3bffae6b50f360808e214b5c26b/f054e/ismir2024logo.png","srcSet":"/static/23c0c3bffae6b50f360808e214b5c26b/a49f4/ismir2024logo.png 188w,\n/static/23c0c3bffae6b50f360808e214b5c26b/98376/ismir2024logo.png 375w,\n/static/23c0c3bffae6b50f360808e214b5c26b/f054e/ismir2024logo.png 750w","sizes":"(min-width: 750px) 750px, 100vw"},"sources":[{"srcSet":"/static/23c0c3bffae6b50f360808e214b5c26b/e6874/ismir2024logo.webp 188w,\n/static/23c0c3bffae6b50f360808e214b5c26b/e3305/ismir2024logo.webp 375w,\n/static/23c0c3bffae6b50f360808e214b5c26b/4f03f/ismir2024logo.webp 750w","type":"image/webp","sizes":"(min-width: 750px) 750px, 100vw"}]},"width":750,"height":750}}},"title":"C4DM at ISMIR 2024","author":"Emmanouil Benetos","date":"Mon 28 Oct 2024"},"html":"<p></p>\n<p>On 10-14 November 2024, several C4DM researchers will participate at the <b><a href=\"https://ismir2024.ismir.net/\">25th International Society for Music Information Retrieval Conference (ISMIR 2024)</a></b>. ISMIR is the leading conference in the field of music informatics, and is currently the <a href=\"https://scholar.google.com/citations?view_op=top_venues&#x26;hl=en&#x26;vq=hum_musicmusicology\">top-cited publication for Music &#x26; Musicology</a> (source: Google Scholar). This year ISMIR will take place onsite in San Francisco (CA, USA) and online.</p>\n<p>Similar to previous years, the Centre for Digital Music will have a strong presence at ISMIR 2024.</p>\n<p><br>In the <b>Scientific Programme</b>, the following papers are authored/co-authored by C4DM members:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2409.11498\">Augment, Drop &#x26; Swap: Improving Diversity in LLM Captions for Efficient Music-Text Representation Learning</a> (Ilaria Manco, Justin Salamon, Oriol Nieto)</li>\n<li><a href=\"https://arxiv.org/abs/2407.18787\">Automatic Detection of Moral Values in Music Lyrics</a> (Vjosa Preniqi, Iacopo Ghinassi, Julia Ive, Kyriaki Kalimeri, Charalampos Saitis)</li>\n<li><a href=\"https://arxiv.org/abs/2407.21615\">Between the AI and Me: Analysing Listeners' Perspectives on AI- and Human-Composed Progressive Metal Music</a> (Pedro Sarmento, Jackson Lothn, Mathieu Barthet)</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/98625\">Can LLMs \"Reason\" in Music? An Evaluation of LLMs' Capability of Music Understanding and Generation</a> (Ziya Zhou, Yuhang Wu, Zhiyue Wu, Xinyue Zhang, Ruibin Yuan, Yinghao Ma, Lu Wang, Emmanouil Benetos, Wei Xue, Yike Guo)</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/98627\">ComposerX: Multi-Agent Music Generation with LLMs</a> (Qixin Deng, Qikai Yang, Ruibin Yuan, Yipeng Huang, Yi Wang, Xubo Liu, Zeyue Tian, Jiahao Pan, Ge Zhang, Hanfeng Lin, Yizhi Li, Yinghao Ma, Jie Fu, Chenghua Lin, Emmanouil Benetos, Wenwu Wang, Guangyu Xia, Wei Xue, Yike Guo)</li>\n<li><a href=\"https://arxiv.org/abs/2310.17162\">Content-based Controls for Music Large-scale Language Modeling</a> (Liwei Lin, Gus Xia, Junyan Jiang, Yixiao Zhang)</li>\n<li><a href=\"https://arxiv.org/abs/2406.08384\">Diff-A-Riff: Musical Accompaniment Co-creation via Latent Diffusion Models</a> (Javier Nistal, Marco Pasini, Cyran Aouameur, Maarten Grachten, Stefan Lattner)</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/98161\">Diff-MST: Differentiable Mixing Style Transfer</a> (Soumya Sai Vanka, Christian J. Steinmetz, Jean-Baptiste Rolland, Joshua D. Reiss, George Fazekas)</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/98362\">From Audio Encoders to Piano Judges: Benchmarking Performance Understanding for Solo Piano</a> (Huan Zhang, Jinhua Liang, Simon Dixon)</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/97938\">GAPS: A Large and Diverse Classical Guitar Dataset and Benchmark Transcription Model</a> (Xavier Riley, Zixun Guo, Drew Edwards, Simon Dixon)</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/99324\">I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition</a> (Yannis Vasilakis, Rachel Bittner, Johan Pauwels)</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/97939\">MIDI-to-Tab: Guitar Tablature Inference via Masked Language Modeling</a> (Drew Edwards, Xavier Riley, Pedro Sarmento, Simon Dixon)</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/98705\">MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models</a> (Benno Weck, Ilaria Manco, Emmanouil Benetos, Elio Quinton, George Fazekas, Dmitry Bogdanov)<br><b>Best Paper Nomination</b></li>\n<li><a href=\"https://www.arxiv.org/abs/2408.06500\">Music2Latent: Consistency Autoencoders for Latent Audio Compression</a> (Marco Pasini, Stefan Lattner, George Fazekas)</li>\n<li><a href=\"https://arxiv.org/abs/2407.13840\">Semi-Supervised Contrastive Learning of Musical Representations</a> (Julien Guinot, Elio Quinton, George Fazekas)</li>\n<li><a href=\"https://arxiv.org/abs/2406.17672\">SpecMaskGIT: Masked Generative Modelling of Audio Spectrogram for Efficient Audio Synthesis and Beyond</a> (Marco Comunità, Zhi Zhong, Akira Takahashi, Shiqi Yang, Mengjie Zhao, Koichi Saito, Yukara Ikemiya, Takashi Shibuya, Shusuke Takahashi, Yuki Mitsufuji)</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/98593\">ST-ITO: Controlling audio effects for style transfer with inference-time optimization</a> (Christian J. Steinmetz, Shubhr Singh, Marco Comunità, Ilias Ibnyahya, Shanxin Yuan, Emmanouil Benetos, Joshua D. Reiss)<br><b>Best Paper Nomination</b></li>\n</ul>\n<p><br>The following <b>Tutorial</b> will be presented by C4DM PhD student <a href=\"https://ilariamanco.com/\">Ilaria Manco</a>:</p>\n<ul>\n<li><a href=\"https://ismir2024.ismir.net/tutorials\">Connecting Music Audio and Natural Language</a> (Seung Heon Doh, Ilaria Manco, Zachary Novack, Jong Wook Kim and Ke Chen)</li>\n</ul>\n<p><br>The following journal paper published at <b>TISMIR</b> will be presented at the conference:</p>\n<ul>\n<li><a href=\"https://transactions.ismir.net/articles/10.5334/tismir.162\">PiJAMA: Piano Jazz with Automatic MIDI Annotations</a> (Drew Edwards, Simon Dixon, Emmanouil Benetos)</li>\n</ul>\n<p><br>As part of the <b>MIREX public evaluations</b>, C4DM PhD student <a href=\"https://ldzhangyx.github.io/\">Yixiao Zhang</a> is task captain for the <a href=\"https://www.music-ir.org/mirex/wiki/2024:Music_Description_%26_Captioning\">Music Description &#x26; Captioning task</a>.</p>\n<p><br>Finally, the following C4DM members are organising <b>Satellite Events</b>:</p>\n<ul>\n<li><a href=\"https://elonashatri.github.io/\">Elona Shatri</a> as General Chair for <a href=\"https://sites.google.com/view/worms2024/\">WoRMS 2024</a></li>\n<li><a href=\"https://ilariamanco.com/\">Ilaria Manco</a> as Organising Committee member for <a href=\"https://sites.google.com/view/nlp4musa-2024/\">NLP4MUSA 2024</a></li>\n</ul>\n<p><br>See you at ISMIR!</p>","id":"e41051f1-61cb-51c3-b392-59c30c42cb96"},{"fields":{"slug":"/news/2024-10-24.AIM student to join the Alan Turing Institute in 2024-2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/84457edde2978607b183e69bea1840e1/cabd1/ATI_logo_black_W500px.jpg","srcSet":"/static/84457edde2978607b183e69bea1840e1/19e71/ATI_logo_black_W500px.jpg 128w,\n/static/84457edde2978607b183e69bea1840e1/68974/ATI_logo_black_W500px.jpg 256w,\n/static/84457edde2978607b183e69bea1840e1/cabd1/ATI_logo_black_W500px.jpg 511w","sizes":"(min-width: 511px) 511px, 100vw"},"sources":[{"srcSet":"/static/84457edde2978607b183e69bea1840e1/6766a/ATI_logo_black_W500px.webp 128w,\n/static/84457edde2978607b183e69bea1840e1/22bfc/ATI_logo_black_W500px.webp 256w,\n/static/84457edde2978607b183e69bea1840e1/9f973/ATI_logo_black_W500px.webp 511w","type":"image/webp","sizes":"(min-width: 511px) 511px, 100vw"}]},"width":511,"height":511}}},"title":"C4DM student to join the Alan Turing Institute in 2024-2025","author":"Admin","date":"Thu 24 Oct 2024"},"html":"<p>C4DM PhD student <a href=\"https://www.turing.ac.uk/people/doctoral-students/ashley-noel-hirst\">Ashley Noel-Hirst</a> has been awarded an <a href=\"https://www.turing.ac.uk/work-turing/studentships/enrichment\">enrichment placement</a> by the <a href=\"https://www.turing.ac.uk/\">Alan Turing Institute</a>, the UK’s national institute in artificial intelligence and data science, enabling Ashley to join and interact with institute researchers and its community in the 2024/25 academic year.</p>\n<p>Specifically, Ashley’s placement is hosted by the Turing’s <a href=\"https://www.turing.ac.uk/research/research-programmes/data-centric-engineering\">Data-Centric Engineering research programme</a>, and will be supported by <a href=\"https://www.turing.ac.uk/people/researchers/drew-hemment\">Prof Drew Hemment</a>, Theme Lead for Humanities, Arts and Social Sciences in Data-Centric Engineering.</p>\n<p>Congratulations to Ashley!</p>","id":"10d03fa8-db76-5ae6-8910-6643fe624d9c"},{"fields":{"slug":"/news/2024-09-30.AIMLA-2025-CfT"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#083858","images":{"fallback":{"src":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/18c0c/AIMLA-logo.jpg","srcSet":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/0fdf4/AIMLA-logo.jpg 300w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/7706b/AIMLA-logo.jpg 601w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/18c0c/AIMLA-logo.jpg 1201w","sizes":"(min-width: 1201px) 1201px, 100vw"},"sources":[{"srcSet":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/078c3/AIMLA-logo.webp 300w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/2b014/AIMLA-logo.webp 601w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/1fcf1/AIMLA-logo.webp 1201w","type":"image/webp","sizes":"(min-width: 1201px) 1201px, 100vw"}]},"width":1201,"height":1201}}},"title":"Call for Tutorials: AES International Conference on AI and Machine Learning for Audio (AIMLA 2025)","author":"Emmanouil Benetos","date":"Mon 30 Sep 2024"},"html":"<p>The <a href=\"https://aes2.org/contributions/2025-1st-aes-international-conference-on-artificial-intelligence-and-machine-learning-for-audio-call-for-contributions/\">AES International Conference on Artificial Intelligence and Machine Learning for Audio (AIMLA 2025)</a>, hosted at the <a href=\"https://www.c4dm.eecs.qmul.ac.uk/\">Centre for Digital Music</a> of Queen Mary University of London and taking place on Sept. 8-10, 2025 is calling for <b>Tutorial</b> submissions.</p>\n<p>We are seeking proposals for 120-minute hands-on tutorials on the conference topics. The proposal should include a title, an abstract (60-120 words), a list of topics, and a description (up to 500 words). Additionally, the submission should include presenters’ names, qualifications, and technical requirements (sound requirements during the presentation, such as stereo, multichannel, etc.). We encourage tutorials to be supported by an elaborate collation of discussed content and code to support learning and building resources for a given topic. The deadline for tutorial proposals is on <b>October 25, 2024</b> and the tutorial proposal submission portal can be found at: <a href=\"https://easychair.org/conferences/?conf=2025aesaimlapaneltut\">https://easychair.org/conferences/?conf=2025aesaimlapaneltut</a></p>\n<p>For more information on the Calls for Papers, Special Sessions, Tutorials, and Challenges, please visit the conference website: <a href=\"https://aes2.org/events-calendar/2025-aes-international-conference-on-artificial-intelligence-and-machine-learning-for-audio/\">https://aes2.org/events-calendar/2025-aes-international-conference-on-artificial-intelligence-and-machine-learning-for-audio/</a></p>","id":"827b732b-79ec-5a1b-98ac-99dc0eb65f4a"},{"fields":{"slug":"/news/2024-09-25.AIMLA-2025-CfC"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#083858","images":{"fallback":{"src":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/18c0c/AIMLA-logo.jpg","srcSet":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/0fdf4/AIMLA-logo.jpg 300w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/7706b/AIMLA-logo.jpg 601w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/18c0c/AIMLA-logo.jpg 1201w","sizes":"(min-width: 1201px) 1201px, 100vw"},"sources":[{"srcSet":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/078c3/AIMLA-logo.webp 300w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/2b014/AIMLA-logo.webp 601w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/1fcf1/AIMLA-logo.webp 1201w","type":"image/webp","sizes":"(min-width: 1201px) 1201px, 100vw"}]},"width":1201,"height":1201}}},"title":"Call for Challenges: AES International Conference on AI and Machine Learning for Audio (AIMLA 2025)","author":"Emmanouil Benetos","date":"Wed 25 Sep 2024"},"html":"<p>The <a href=\"https://aes2.org/contributions/2025-1st-aes-international-conference-on-artificial-intelligence-and-machine-learning-for-audio-call-for-contributions/\">AES International Conference on Artificial Intelligence and Machine Learning for Audio (AIMLA 2025)</a>, hosted at the <a href=\"https://www.c4dm.eecs.qmul.ac.uk/\">Centre for Digital Music</a> of Queen Mary University of London and taking place on Sept. 8-10, 2025 is calling for proposal submissions for <b>Challenges</b>.</p>\n<p>The conference promotes knowledge sharing among researchers, professionals, and engineers in AI and audio. Special Sessions include pre-conference challenges hosted by industry or academic teams to drive technology improvements and explore new research directions. Each team manages the organization, data provision, participation instructions, mentoring, scoring, summaries, and results presentation.</p>\n<p>Challenges are selected based on their scientific and technological significance, data quality and relevance, and proposal feasibility. Collaborative proposals from different labs are encouraged and prioritized. We expect an initial expression of interest via email to <a href=\"mailto:special-sessions-aimla@qmul.ac.uk\">special-sessions-aimla@qmul.ac.uk</a> by October 15, 2024, followed by a full submission on EasyChair by the final submission deadline.</p>\n<p>For more information on the Calls for Papers, Special Sessions, Tutorials, and Challenges, please visit the conference website: <a href=\"https://aes2.org/events-calendar/2025-aes-international-conference-on-artificial-intelligence-and-machine-learning-for-audio/\">https://aes2.org/events-calendar/2025-aes-international-conference-on-artificial-intelligence-and-machine-learning-for-audio/</a></p>","id":"322c6ade-f7fc-556a-805e-c6a057aa25f0"},{"fields":{"slug":"/news/2024-25-09.C4DM-Seminar_Kavya_Ranjan_Saxena"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar:  Kavya Ranjan Saxena","author":"Admin","date":"Wed 25 Sep 2024"},"html":"<h3>C4DM Seminar: Kavya Ranjan Saxena: Meta-learning-based domain adaptation for melody extraction</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nKavya Ranjan Saxena</p>\n<p><strong>Date/time:  Wednesday, 25th September 2024, 11am</strong></p>\n<p>**Location: GC205, Graduate Centre Building, Mile End Campus, QMUL, E1 4NS **\nZoom: <a href=\"https://qmul-ac-uk.zoom.us/j/2387202947\">https://qmul-ac-uk.zoom.us/j/2387202947</a></p>\n<h2><b>Title</b>: Meta-learning-based domain adaptation for melody extraction</h2>\n<p><b>Abstract</b>: The task of extracting the dominant pitch from polyphonic audio is crucial in the music information retrieval field. A substantial amount of labelled audio data is required to effectively train the machine learning models to perform the task. Generally, the traditional models trained on audios of one domain, i.e., source, may not accurately extract pitch from audios of different domains, i.e., target. To boost the performance, the models are adapted on minimal labelled data from the target domain, a method known as the supervised domain adaptation. We use the meta-learning algorithm as the supervised domain adaptation method for the task of melody extraction, by proposing a novel weighting technique to handle the class imbalance when adapting to a few audios in the target domain. Further, this method can be extended as an efficient interactive melody extraction method based on active adaptation. This method selects the regions in the target audio that require human annotation using a confidence criterion based on normalized true class probability. The annotations are used by the model to adapt itself to the target domain using meta-learning. The meta-learning-based domain adaptation method is model-agnostic and can be applied to other non-adaptive melody extraction models to boost their performance.</p>\n<p><b>Bio</b>: Kavya Ranjan Saxena is a Ph.D. student at the Indian Institute of Technology Kanpur, India. Her research interests are in machine learning for signal processing with a focus on domain adaptation for melody extraction in the field of music information retrieval. Currently, she is working as an Intern – Speech Research Scientist at Krutrim (an Ola company), where her work focuses on Audio LLMs.</p>\n<p><b>Presentation</b>:\n<a href=\"https://drive.google.com/file/d/1Dt0QIDp4IMH2FLi3eYG281rkl3D0Quw9/view?usp=sharing\">[PDF Slides]</a></p>","id":"93032c40-6437-5799-a4cc-3a6ef2c5de8c"},{"fields":{"slug":"/news/2024-09-19.MoralBERTApp"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"Moral Values Detection App","author":"Admin","date":"Thu 19 Sep 2024"},"html":"<p>C4DM PhD researchers Vjosa Preniqi and Iacopo Ghinassi, funded via the <a href=\"https://dame.qmul.ac.uk/\">DAME CDT</a>, have developed <a href=\"https://huggingface.co/spaces/vjosap/MoralBERTApp\">a new web app that automatically predicts moral values in text and music lyrics</a>. Users can upload CSV or Excel files containing text or lyrics, and the app will assign probabilities for 10 moral categories. It is particularly effective for social media posts and music lyrics. This new tool provides a simple, user-friendly interface for analysing how language reflects moral beliefs, offering useful insights for research in music and media analysis. Try it out!</p>\n<p>The app is based on models from two recent papers:</p>\n<ul>\n<li><a href=\"https://dl.acm.org/doi/10.1145/3677525.3678694\">MoralBERT: A Fine-Tuned Language Model for Capturing Moral Values in Social Discussions</a> (ACM GoodIT 2024)</li>\n<li><a href=\"https://arxiv.org/abs/2407.18787\">Automatic Detection of Moral Values in Music Lyrics</a> (ISMIR 2024)</li>\n</ul>\n<p>This work is a collaboration between <a href=\"https://www.c4dm.eecs.qmul.ac.uk/\">C4DM</a> and <a href=\"https://www.isi.it/\">ISI Foundation</a>.</p>","id":"6771d8b5-21db-5edc-b861-cd71e808ca26"}]}}}
{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/YIFgS8she58","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/Sandler-Artificial-Neuroscience"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png","srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/2fd20/EPSRC.png 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/de391/EPSRC.png 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/d66e1/EPSRC.webp 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/e7160/EPSRC.webp 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/5f169/EPSRC.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Artificial Neuroscience: metrology and engineering for Deep Learning using Linear Algebra","author":"Prof Mark Sandler (PI), Boris Khoruzhenko (CI)","date":null,"link":"https://www.ukri.org/councils/epsrc/"},"html":"","id":"c1314000-fe3b-5175-88a2-f239d861ddad"},{"fields":{"slug":"/projects/Barthet-Netz"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Netz: A Novel XR Musical Instrument","author":"Dr Mathieu Barthet (PI)","date":null,"link":"https://iuk-business-connect.org.uk/programme/icure/"},"html":"","id":"0040b455-65cf-5ea1-a7ce-38ab70570dae"},{"fields":{"slug":"/projects/Benetos-L-Acoustics"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Online speech enhancement in scenarios with low direct-to-reverberant-ratio","author":"Dr Emmanouil Benetos (PI), Dr Aidan Hogg (CI)","date":null,"link":"https://www.l-acoustics.com/"},"html":"","id":"ce65fc6c-7228-5805-a9b2-6e14e5fd5086"},{"fields":{"slug":"/projects/Benetos-Algorivm"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Project Maestro - AI Musical Analysis Platform","author":"Dr Emmanouil Benetos (PI), Prof Simon Dixon (CI)","date":null,"link":"https://www.ukri.org/councils/innovate-uk/"},"html":"","id":"78c348d8-d78f-5acf-8441-382bc6c4d791"},{"fields":{"slug":"/projects/Benetos-Moises"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Enhancing lyrics transcription with open-source architectures and fine-tuning techniques","author":"Dr Emmanouil Benetos (PI)","date":null,"link":"https://moises.ai/"},"html":"","id":"d61977ff-9345-55a4-b85e-44caffa4a7f0"},{"fields":{"slug":"/projects/Dixon-assessment"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Music Performance Assessment and Feedback","author":"Prof Simon Dixon (PI), Dr Emmanouil Benetos (CI)","date":null,"link":null},"html":"","id":"68ee56ea-59b7-58de-a8c9-805c4aba4357"}]},"news":{"nodes":[{"fields":{"slug":"/news/2025-09-03.C4DM-organises_AIMLA_2025_conference"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#083858","images":{"fallback":{"src":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/18c0c/AIMLA-logo.jpg","srcSet":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/0fdf4/AIMLA-logo.jpg 300w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/7706b/AIMLA-logo.jpg 601w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/18c0c/AIMLA-logo.jpg 1201w","sizes":"(min-width: 1201px) 1201px, 100vw"},"sources":[{"srcSet":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/078c3/AIMLA-logo.webp 300w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/2b014/AIMLA-logo.webp 601w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/1fcf1/AIMLA-logo.webp 1201w","type":"image/webp","sizes":"(min-width: 1201px) 1201px, 100vw"}]},"width":1201,"height":1201}}},"title":"C4DM organises AES AIMLA 2025 conference","author":"Admin","date":"Wed 03 Sep 2025"},"html":"<p>The <a href=\"https://aes2.org/contributions/2025-1st-aes-international-conference-on-artificial-intelligence-and-machine-learning-for-audio-call-for-contributions/\">AES International Conference on Artificial Intelligence and Machine Learning for Audio (AIMLA 2025)</a> will be hosted by the <a href=\"https://www.c4dm.eecs.qmul.ac.uk/\">Centre for Digital Music</a> of Queen Mary University of London and is taking place on Sept. 8-10, 2025.</p>\n<p>Several C4DM members are involved in the organisation of the conference, including but not limited to:</p>\n<ul>\n<li>Josh Reiss (General Chair)</li>\n<li>George Fazekas (Papers Co-chair)</li>\n<li>Soumya Vanka (Special Sessions Co-Chair)</li>\n<li>Franco Caspe (Special Sessions Co-Chair)</li>\n<li>Farida Yusuf (Sponsorship Chair)</li>\n<li>Emmanouil Benetos (Publicity Chair)</li>\n<li>Nelly Garcia (Social Events Coordinator)</li>\n<li>Ilias Ibnyahya (Treasurer)</li>\n<li>Chin-Yun Yu (Late Breaking Papers Chair)</li>\n<li>Marikaiti Primenta (Invited Speakers Chair)</li>\n</ul>\n<p>Several papers and presentations will be made from C4DM members at AIMLA as well. The following peer-reviewed papers will be presented at the conference:</p>\n<ul>\n<li>\n<p><a href=\"https://aesaimla2025.sched.com/event/28Oay/nablafx-a-framework-for-differentiable-black-box-and-gray-box-modeling-of-audio-effects?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">NablAFx: A Framework for Differentiable Black-box and Gray-box Modeling of Audio Effects</a>, by Marco Comunità, Christian Steinmetz, Joshua Reiss</p>\n</li>\n<li>\n<p><a href=\"https://aesaimla2025.sched.com/event/28Ob1/transfer-learning-for-neural-modelling-of-nonlinear-distortion-effects?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">Transfer Learning for Neural Modelling of Nonlinear Distortion Effects</a>, by Tara Vanhatalo, Pierrick Legrand, Myriam Desainte-Catherine, Pierre Hanna, Guillaume Pille, Antoine Brusco, Joshua Reiss</p>\n</li>\n<li>\n<p><a href=\"https://aesaimla2025.sched.com/event/28Obz/sound-matching-an-analogue-levelling-amplifier-using-the-newton-raphson-method?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">Sound Matching an Analogue Levelling Amplifier Using the Newton-Raphson Method</a>, by Chin-Yun Yu, George Fazekas</p>\n</li>\n<li>\n<p><a href=\"https://aesaimla2025.sched.com/event/28OcW/procedural-music-generation-systems-in-games?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">Procedural Music Generation Systems in Games</a>, by Shangxuan Luo, Joshua Reiss</p>\n</li>\n<li>\n<p><a href=\"https://aesaimla2025.sched.com/event/28Obt/neutone-sdk-an-open-source-framework-for-neural-audio-processing?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">Neutone SDK: An Open Source Framework for Neural Audio Processing</a>, by Christopher Mitcheltree, Bogdan Teleaga, Andrew Fyfe, Naotake Masuda, Matthias Schäfer, Alfie Bradic, Nao Tokui</p>\n</li>\n</ul>\n<p>The following late-breaking posters from C4DM members will be presented at AIMLA:</p>\n<ul>\n<li>\n<p><a href=\"https://aesaimla2025.sched.com/event/28TJN/transformer-based-sustain-pedal-reconstruction-for-expressive-piano-performance-midi?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">Transformer-Based Sustain Pedal Reconstruction for Expressive Piano Performance MIDI</a>, by Wenhao Liu, George Fazekas, Jingjing Tang</p>\n</li>\n<li>\n<p><a href=\"https://aesaimla2025.sched.com/event/28TJ2/decoding-melodic-acoustic-features-from-neural-data?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">Decoding Melodic Acoustic Features from Neural Data</a>, by Zorka Bozilovic, Iran Roman</p>\n</li>\n<li>\n<p><a href=\"https://aesaimla2025.sched.com/event/28TJH/towards-intelligent-music-education-score-informed-transcription-and-performance-assessment?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">Towards Intelligent Music Education: Score-Informed Transcription and Performance Assessment</a>, by Jack Loth, Marikaiti Primenta, Jingjing Tang, Xavier Riley, Simon Dixon, Emmanouil Benetos</p>\n</li>\n</ul>\n<p>Last but not least, the following tutorial will be co-presented by C4DM PhD student Franco Caspe:</p>\n<ul>\n<li><a href=\"https://aesaimla2025.sched.com/event/28OaU/tutorial-real-time-neural-audio-inference?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">Real-Time Neural Audio Inference </a>, by Franco Caspe and Jatin Chowdhury</li>\n</ul>\n<p>See you in London!</p>","id":"8ca8145f-7bcc-55ae-b8a6-1a7b454f655e"},{"fields":{"slug":"/news/2025-07-29.C4DM-at_Interspeech_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#084898","images":{"fallback":{"src":"/static/fbc593813a2397e9f177cd960126bb83/869a6/Interspeech-2025.jpg","srcSet":"/static/fbc593813a2397e9f177cd960126bb83/97a19/Interspeech-2025.jpg 480w,\n/static/fbc593813a2397e9f177cd960126bb83/d4aca/Interspeech-2025.jpg 960w,\n/static/fbc593813a2397e9f177cd960126bb83/869a6/Interspeech-2025.jpg 1920w","sizes":"(min-width: 1920px) 1920px, 100vw"},"sources":[{"srcSet":"/static/fbc593813a2397e9f177cd960126bb83/21b1a/Interspeech-2025.webp 480w,\n/static/fbc593813a2397e9f177cd960126bb83/d6f60/Interspeech-2025.webp 960w,\n/static/fbc593813a2397e9f177cd960126bb83/26222/Interspeech-2025.webp 1920w","type":"image/webp","sizes":"(min-width: 1920px) 1920px, 100vw"}]},"width":1920,"height":1920}}},"title":"C4DM at Interspeech 2025","author":"admin","date":"Tue 29 Jul 2025"},"html":"<p>On 17-21 August, C4DM members will participate in <a href=\"https://www.interspeech2025.org/home\">Interspeech 2025</a>. Interspeech is the premier international conference for research on the science and technology of spoken language processing.</p>\n<p>The following papers authored/co-authored by C4DM members will be presented at Interspeech 2025:</p>\n<ul>\n<li>\n<p><a href=\"https://arxiv.org/abs/2505.23509\">Spectrotemporal Modulation: Efficient and Interpretable Feature Representation for Classifying Speech, Music, and Environmental Sounds</a> by Andrew Chang, Yike Li, Iran R. Roman, David Poeppel</p>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/abs/2506.02339\">Enhancing Lyrics Transcription on Music Mixtures with Consistency Loss</a> by Jiawen Huang, Felipe Sousa, Emir Demirel, Emmanouil Benetos, Igor Gadelha</p>\n</li>\n</ul>\n<p>See you at Interspeech!</p>","id":"6bc651c0-77cf-5031-a443-082420242949"},{"fields":{"slug":"/news/2025-09-03.C4DM-at_UKAIRS"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/0a520cc6b532ebe8e12d32e1128fe298/1ff6a/ukairs_2025.png","srcSet":"/static/0a520cc6b532ebe8e12d32e1128fe298/a5ae6/ukairs_2025.png 334w,\n/static/0a520cc6b532ebe8e12d32e1128fe298/40a65/ukairs_2025.png 668w,\n/static/0a520cc6b532ebe8e12d32e1128fe298/1ff6a/ukairs_2025.png 1336w","sizes":"(min-width: 1336px) 1336px, 100vw"},"sources":[{"srcSet":"/static/0a520cc6b532ebe8e12d32e1128fe298/c6d30/ukairs_2025.webp 334w,\n/static/0a520cc6b532ebe8e12d32e1128fe298/27806/ukairs_2025.webp 668w,\n/static/0a520cc6b532ebe8e12d32e1128fe298/93676/ukairs_2025.webp 1336w","type":"image/webp","sizes":"(min-width: 1336px) 1336px, 100vw"}]},"width":1336,"height":1336}}},"title":"C4DM at UKAIRS 2025","author":"Christos Plachouras","date":"Tue 29 Jul 2025"},"html":"<p>On 8-10 September, C4DM members will participate in the <a href=\"https://www.ukairs.ac.uk/\">UK AI Research Symposium (UKAIRS)</a> in Northumbria University, Newcastle. UKAIRS will bring together researchers from different disciplines, aiming to inform UK AI research priorities, foster interdisciplinary collaboration, and shape discourse on delivering AI for societal and economic benefit.</p>\n<p>The following works C4DM members have been involved in will be presented:</p>\n<ul>\n<li><a href=\"https://hal.science/hal-04943901v1/document\">Towards Music Industry 5.0: Perspectives on Artificial Intelligence</a>, Alexander Williams, Mathieu Barthet</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/103081/Xambo%20Sedo%20Human-machine%20agencies%20in%20live%20coding%20for%20music%20performance%202024%20Accepted.pdf?sequence=2&#x26;isAllowed=y\">Human–machine agencies in live coding for music performance</a>, Anna Xambó, Gerard Roma</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/106803\">Towards a Unified Representation Evaluation Framework Beyond Downstream Tasks</a>, Christos Plachouras, Julien Guinot, George Fazekas, Elio Quinton, Emmanouil Benetos, Johan Pauwels</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/110219/Xambo%20Soundscape-based%20music%202025%20Accepted.pdf?sequence=2\">Soundscape-based music and creative AI: Insights and promises</a>, Anna Xambó, Peter Batchelor, Luigi Marino, Gerard Roma, Mike Bell, George Xenakis</li>\n<li>Split Fine-Tuning of BERT-based Music Models in the Edge-Cloud Continuum: An Empirical Analysis, Bradley Aldous, Ahmed M. A. Sayed</li>\n<li>Composing Kernel Models with Self-Attention, Carey Bunks, Simon Dixon, Bruno Di Giorgi</li>\n</ul>\n<p>See you at UKAIRS!</p>","id":"0b82bc45-d8be-59c4-8e5c-cfef869e665a"},{"fields":{"slug":"/news/2025-07-28.AIM student to join the Alan Turing Institute in 2025-2026"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/84457edde2978607b183e69bea1840e1/cabd1/ATI_logo_black_W500px.jpg","srcSet":"/static/84457edde2978607b183e69bea1840e1/19e71/ATI_logo_black_W500px.jpg 128w,\n/static/84457edde2978607b183e69bea1840e1/68974/ATI_logo_black_W500px.jpg 256w,\n/static/84457edde2978607b183e69bea1840e1/cabd1/ATI_logo_black_W500px.jpg 511w","sizes":"(min-width: 511px) 511px, 100vw"},"sources":[{"srcSet":"/static/84457edde2978607b183e69bea1840e1/6766a/ATI_logo_black_W500px.webp 128w,\n/static/84457edde2978607b183e69bea1840e1/22bfc/ATI_logo_black_W500px.webp 256w,\n/static/84457edde2978607b183e69bea1840e1/9f973/ATI_logo_black_W500px.webp 511w","type":"image/webp","sizes":"(min-width: 511px) 511px, 100vw"}]},"width":511,"height":511}}},"title":"C4DM student to join the Alan Turing Institute in 2025-2026","author":"Admin","date":"Mon 28 Jul 2025"},"html":"<p>C4DM PhD student <a href=\"https://www.qmul.ac.uk/eecs/people/profiles/adityabhattacharjee-.html\">Aditya Bhattacharjee</a> has been awarded an <a href=\"https://www.turing.ac.uk/work-turing/studentships/enrichment\">enrichment placement</a> by the <a href=\"https://www.turing.ac.uk/\">Alan Turing Institute</a>, the UK’s national institute in artificial intelligence and data science, enabling him to join and interact with institute researchers and its community in the 2025/26 academic year. Aditya’s placement will be hosted by the Turing’s <a href=\"https://www.turing.ac.uk/research/research-programmes/fundamental-research\">Fundamental research in data science and AI</a> research programme.</p>\n<p>Congratulations to Aditya!</p>","id":"049b3ca8-72c8-5274-af77-62fb37ecb440"},{"fields":{"slug":"/news/2025-06-30.CfLBDP.AIMLA.2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#083858","images":{"fallback":{"src":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/18c0c/AIMLA-logo.jpg","srcSet":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/0fdf4/AIMLA-logo.jpg 300w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/7706b/AIMLA-logo.jpg 601w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/18c0c/AIMLA-logo.jpg 1201w","sizes":"(min-width: 1201px) 1201px, 100vw"},"sources":[{"srcSet":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/078c3/AIMLA-logo.webp 300w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/2b014/AIMLA-logo.webp 601w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/1fcf1/AIMLA-logo.webp 1201w","type":"image/webp","sizes":"(min-width: 1201px) 1201px, 100vw"}]},"width":1201,"height":1201}}},"title":"Call for Late Breaking Demo Papers: First AES International Conference on AI and Machine Learning for Audio (AIMLA 2025)","author":"Christos Plachouras","date":"Mon 30 Jun 2025"},"html":"<p>The AES International Conference on Artificial Intelligence and Machine Learning for Audio (AIMLA 2025), hosted at the Centre for Digital Music of Queen Mary University of London and taking place on Sept. 8-10, 2025 is calling for Late Breaking Demo Paper submissions.</p>\n<p>We are seeking 2-page extended abstracts showcasing prototype systems and early research results that are highly relevant to the conference theme. At least one author must register for the conference and present their work as a poster in person during the main track poster session.</p>\n<p>Submissions open on July 1, 2025 with a deadline of August 1, 2025. Submissions will be reviewed on a rolling basis.</p>\n<p>For more information on submission guidelines, templates, and technical requirements, please visit: <a href=\"https://aes2.org/contributions/2025-1st-aes-international-conference-on-artificial-intelligence-and-machine-learning-for-audio-call-for-contributions/\">https://aes2.org/contributions/2025-1st-aes-international-conference-on-artificial-intelligence-and-machine-learning-for-audio-call-for-contributions/</a></p>","id":"8db047da-7a1b-554b-b451-7b22c99c199a"},{"fields":{"slug":"/news/2025-06-28.C4dM-at_Sonar"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/cc8173d3891b7c4607cde479aee1ccce/3dc4a/sonar_2025.png","srcSet":"/static/cc8173d3891b7c4607cde479aee1ccce/7c333/sonar_2025.png 344w,\n/static/cc8173d3891b7c4607cde479aee1ccce/8e0d2/sonar_2025.png 688w,\n/static/cc8173d3891b7c4607cde479aee1ccce/3dc4a/sonar_2025.png 1376w","sizes":"(min-width: 1376px) 1376px, 100vw"},"sources":[{"srcSet":"/static/cc8173d3891b7c4607cde479aee1ccce/0f0fa/sonar_2025.webp 344w,\n/static/cc8173d3891b7c4607cde479aee1ccce/a7ca0/sonar_2025.webp 688w,\n/static/cc8173d3891b7c4607cde479aee1ccce/a8d24/sonar_2025.webp 1376w","type":"image/webp","sizes":"(min-width: 1376px) 1376px, 100vw"}]},"width":1376,"height":1376}}},"title":"C4DM at Sónar+D 2025","author":"Shuoyang Zheng","date":"Sat 28 Jun 2025"},"html":"<p>Sónar is a pioneering festival that's reflected the evolution and expansion of electronic music and digital culture since its first edition in 1994. The interactive exhibition space, <a href=\"https://sonar.es/en/activity/project-area\">Project Area at Sónar+D</a>, showcases state-of-the-art technology, innovative design, radical thinking, and cutting-edge research side-by-side in the heart of the music festival Sónar by Day.</p>\n<p>At Sónar+D Project Area, C4DM members Shuoyang Zheng and Franco Caspe joined the AI &#x26; Music exhibition area powered by S+T+ARTS to present their innovative tools for AI-driven sound creation. In addition, C4DM members Christopher Mitcheltree represented Neutone to present the cutting-edge audio plugins.</p>\n<p>Franco Caspe presented BRAVE, a timbre transfer tool that allows performers to play an AI model as an instrument, transforming timbre in real-time. Shuoyang Zheng presented Latent Terrain Synthesis, an innovative method to explore sonic landscapes dissected from the internal space of a generative AI model.</p>\n<p><a href=\"https://sonar.es/en/activity/ai-performance-playground-live\">The AI Performance Playground</a> took place between 11th and 14th June as part of Sónar+D 2025, co-organised by C4DM Senior Lecturer Anna Xambó, powered by S+T+ARTS, with support from La Salle-URL. This collaborative hacklab brought together artists, coders, musicians, DIY creators, and creative technologists to explore and deepen their use of machine learning tools, AI, and other related technologies for musical performance. C4DM member Teresa Pelinski participated in the hacklab and joined a collaborative performance at SonarÀgora - open to the general public at Sónar by Day.</p>\n<p>C4DM members Christopher Mitcheltree and Shuoyang Zheng, together with Rebecca Fiebrink (University of the Arts London) and Nao Tokui (Neutone) joined the enlightening talk panel during the hacklab with Ben Cantil (DataMind) to discuss the challenges and opportunities of being an artist using AI tools.</p>","id":"c17c66e8-67ac-5caf-a6e8-d1f066836164"}]}}}
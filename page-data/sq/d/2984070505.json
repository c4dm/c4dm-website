{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/YIFgS8she58","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/Sandler-Artificial-Neuroscience"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png","srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/2fd20/EPSRC.png 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/de391/EPSRC.png 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/d66e1/EPSRC.webp 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/e7160/EPSRC.webp 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/5f169/EPSRC.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Artificial Neuroscience: metrology and engineering for Deep Learning using Linear Algebra","author":"Prof Mark Sandler (PI), Boris Khoruzhenko (CI)","date":null,"link":"https://www.ukri.org/councils/epsrc/"},"html":"","id":"c1314000-fe3b-5175-88a2-f239d861ddad"},{"fields":{"slug":"/projects/Barthet-Netz"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Netz: A Novel XR Musical Instrument","author":"Dr Mathieu Barthet (PI)","date":null,"link":"https://iuk-business-connect.org.uk/programme/icure/"},"html":"","id":"0040b455-65cf-5ea1-a7ce-38ab70570dae"},{"fields":{"slug":"/projects/Benetos-Algorivm"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Project Maestro - AI Musical Analysis Platform","author":"Dr Emmanouil Benetos (PI), Prof Simon Dixon (CI)","date":null,"link":"https://www.ukri.org/councils/innovate-uk/"},"html":"","id":"78c348d8-d78f-5acf-8441-382bc6c4d791"},{"fields":{"slug":"/projects/Benetos-L-Acoustics"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Online speech enhancement in scenarios with low direct-to-reverberant-ratio","author":"Dr Emmanouil Benetos (PI), Dr Aidan Hogg (CI)","date":null,"link":"https://www.l-acoustics.com/"},"html":"","id":"ce65fc6c-7228-5805-a9b2-6e14e5fd5086"},{"fields":{"slug":"/projects/Benetos-Moises"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Enhancing lyrics transcription with open-source architectures and fine-tuning techniques","author":"Dr Emmanouil Benetos (PI)","date":null,"link":"https://moises.ai/"},"html":"","id":"d61977ff-9345-55a4-b85e-44caffa4a7f0"},{"fields":{"slug":"/projects/Dixon-assessment"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Music Performance Assessment and Feedback","author":"Prof Simon Dixon (PI), Dr Emmanouil Benetos (CI)","date":null,"link":null},"html":"","id":"68ee56ea-59b7-58de-a8c9-805c4aba4357"}]},"news":{"nodes":[{"fields":{"slug":"/news/2025-10-22.C4DM-Seminar_Come_Peladeau"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Côme Peladeau","author":"Admin","date":"Wed 22 Oct 2025"},"html":"<h3>C4DM Seminar: Côme Peladeau: Audio processors: estimating parameter distributions using discrete normalizing flows and DDSP</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Côme Peladeau</p>\n<p><strong>Date/time:</strong>  Wednesday, 22th October 2025, 1pm</p>\n<p><strong>Location:</strong> Online</p>\n<p><strong>Zoom Link:</strong> <a href=\"https://qmul-ac-uk.zoom.us/j/2387202947\">https://qmul-ac-uk.zoom.us/j/2387202947</a></p>\n<h2><b>Title</b>: Audio processors: estimating parameter distributions using discrete normalizing flows and DDSP</h2>\n<p><b>Abstract</b>: Audio processor, such as audio effects or synthesizers, are widely used in popular music production.</p>\n<p>Their parameters control the quality of the output sound. Multiple combinations of parameters can lead to the same sound.</p>\n<p>While recent approaches have been proposed to estimate these parameters given only the output sound, those are deterministic, i.e. they only estimate a single solution among the many possible parameter configurations.</p>\n<p>This work proposes an approach to estimate all the combinations that lead to the target sound, in the form of a probability distribution.</p>\n<p>We achieve this using differentiable signal processing and discrete normalizing flows.</p>\n<p><b>Bio</b>: Côme Peladeau graduated with a masters degree in Acoustics, Signal Processing and Computer Science applied to Music at Sorbonne Université in 2023.</p>\n<p>He is now a PhD student working with Geoffroy Peeters and Dominique Fourer at the Information Processing and Communications Laboratory (LTCI) in Telecom Paris.</p>\n<p>HI work focuses on audio effects estimation using deep learning and differentiable signal processing.</p>","id":"184cbea7-8b9a-5196-9c4d-7a5b3238c12c"},{"fields":{"slug":"/news/2025-10-06.C4DM-at_WASPAA_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#a8c8e8","images":{"fallback":{"src":"/static/6871445306f9af07141444a39ae524a8/fad00/waspaa2025.png","srcSet":"/static/6871445306f9af07141444a39ae524a8/71cba/waspaa2025.png 291w,\n/static/6871445306f9af07141444a39ae524a8/fa1e5/waspaa2025.png 582w,\n/static/6871445306f9af07141444a39ae524a8/fad00/waspaa2025.png 1163w","sizes":"(min-width: 1163px) 1163px, 100vw"},"sources":[{"srcSet":"/static/6871445306f9af07141444a39ae524a8/46435/waspaa2025.webp 291w,\n/static/6871445306f9af07141444a39ae524a8/f27b2/waspaa2025.webp 582w,\n/static/6871445306f9af07141444a39ae524a8/24d0f/waspaa2025.webp 1163w","type":"image/webp","sizes":"(min-width: 1163px) 1163px, 100vw"}]},"width":1163,"height":1163}}},"title":"C4DM at WASPAA 2025","author":"Admin","date":"Mon 06 Oct 2025"},"html":"<p></p>\n<p>On 12-15 October, several C4DM researchers will participate at the <b><a href=\"http://www.waspaa.com/\">2025 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics</a></b>, taking place at the Granlibakken Tahoe Resort near Lake Tahoe, in Tahoe City, CA, USA. WASPAA is a premier event in the field of audio signal processing, organised by the IEEE's Audio and Acoustic Signal Processing (AASP) technical committee, with a strong focus on music signal processing and computational sound scene analysis.</p>\n<p>The Centre for Digital Music (C4DM), as in previous years, will have a strong presence at WASPAA 2025.</p>\n<p>In the <b>Technical Programme</b>, the following papers are authored by C4DM members:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2507.07066\">Latent Acoustic Mapping for Direction of Arrival Estimation: A Self-Supervised Approach</a> (Adrian S. Roman, Iran R. Roman, Juan Pablo Bello)</li>\n<li><a href=\"https://arxiv.org/abs/2507.12175\">RUMAA: Repeat-Aware Unified Music Audio Analysis for Score-Performance Alignment, Transcription, and Mistake Detection</a> (Sungkyun Chang, Simon Dixon, Emmanouil Benetos)</li>\n<li>Modulation Discovery with Differentiable Digital Signal Processing (Christopher Mitcheltree, Hao Hao Tan, Joshua D. Reiss)</li>\n<li>Beyond Architecture: The Critical Impact of Inference Overlap on Music Source Separation Benchmarks (Harnick Khera, Johan Pauwels, Alan W. Archer-Boyd, Mark B. Sandler)</li>\n<li><a href=\"https://arxiv.org/abs/2505.11315\">Improving Inference-Time Optimisation for Vocal Effects Style Transfer with a Gaussian Prior</a> (Chin-Yun Yu, Marco A. Martínez-Ramírez, Junghyun Koo, Wei-Hsiang Liao, Yuki Mitsufuji, George Fazekas)</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/109071\">Self-Supervised Representation Learning with a JEPA Framework for Multi-instrument Music Transcription</a> (Mary Pilataki, Matthias Mauch, Simon Dixon)</li>\n</ul>\n<p>In the <b>Demo Session</b>, the following demos will be presented by C4DM members:</p>\n<ul>\n<li>Neural Audio Synthesis for Non-Keyboard Instruments (Franco Caspe, Andrew McPherson, Mark Sandler)</li>\n<li>PCA-DiffVox: Augmenting Vocal Effects Tweakability With a Bijective Latent Space (Chin-Yun Yu, Marco A. Martínez-Ramírez, Junghyun Koo, Wei-Hsiang Liao, Yuki Mitsufuji, George Fazekas)</li>\n</ul>\n<p>See you at WASPAA!</p>","id":"5422e2a2-1826-5148-b601-a7cb31a882f5"},{"fields":{"slug":"/news/2025-09-30.C4DM-ISMIR_Awards"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8b8b8","images":{"fallback":{"src":"/static/0d9bd1939a6f78714877cb0f2df8af8a/93c00/c4dm_ismir_2025_awards.png","srcSet":"/static/0d9bd1939a6f78714877cb0f2df8af8a/383f5/c4dm_ismir_2025_awards.png 239w,\n/static/0d9bd1939a6f78714877cb0f2df8af8a/6c9f5/c4dm_ismir_2025_awards.png 477w,\n/static/0d9bd1939a6f78714877cb0f2df8af8a/93c00/c4dm_ismir_2025_awards.png 954w","sizes":"(min-width: 954px) 954px, 100vw"},"sources":[{"srcSet":"/static/0d9bd1939a6f78714877cb0f2df8af8a/2600e/c4dm_ismir_2025_awards.webp 239w,\n/static/0d9bd1939a6f78714877cb0f2df8af8a/616b7/c4dm_ismir_2025_awards.webp 477w,\n/static/0d9bd1939a6f78714877cb0f2df8af8a/1094f/c4dm_ismir_2025_awards.webp 954w","type":"image/webp","sizes":"(min-width: 954px) 954px, 100vw"}]},"width":954,"height":954}}},"title":"Best student paper and outstanding reviewer awards at ISMIR 2025","author":"Christos Plachouras","date":"Tue 30 Sep 2025"},"html":"<p>We are delighted to share that C4DM PhD student Ben Hayes, along with C4DM academics Charalampos Saitis and George Fazekas, have received the best student paper award at the ISMIR 2025 conference. The paper \"<a href=\"https://ismir2025program.ismir.net/poster_7.html\">Audio Synthesizer Inversion in Symmetric Parameter Spaces With Approximately Equivariant Flow Matching</a>\" proposes using permutation equivariant continuous normalizing flows to handle the ill-posed problem of audio synthesizer inversion, where multiple parameter configurations can produce identical sounds due to intrinsic symmetries in synthesizer design. By explicitly modeling these symmetries, particularly permutation invariance across repeated components like oscillators and filters, the method outperforms both regression-based approaches and symmetry-naive generative models on both synthetic tasks and a real-world synthesizer (Surge XT).</p>\n<p>We are also happy to share that two C4DM PhD students, Yannis Vasilakis and Ben Hayes, were recognised as outstanding reviewers.</p>","id":"0331e8af-8b6b-52e3-a193-151c89c594fc"},{"fields":{"slug":"/news/2025-09-29.C4DM-Seminar_Ahmed_Sayed"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Ahmed Sayed","author":"Admin","date":"Mon 29 Sep 2025"},"html":"<h3>C4DM Seminar: Ahmed Sayed: Advancing Decentralized AI: Scalable, Adaptive, and Client-Centric Learning Systems</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Ahmed Sayed</p>\n<p><strong>Date/time:</strong>  Monday, 29th September 2025, 1pm</p>\n<p><strong>Location:</strong> G2, Enginerring Building, Mile End Campus, Queen Mary University of London, E1 4NS</p>\n<h2><b>Title</b>: Advancing Decentralized AI: Scalable, Adaptive, and Client-Centric Learning Systems</h2>\n<p><b>Abstract</b>: Decentralised AI systems, particularly those employing federated learning (FL), offer a promising approach to training machine learning models across distributed data sources while preserving privacy. However, they face significant challenges, including system heterogeneity, dynamic client availability, and resource constraints. Addressing these issues is crucial for the effective deployment of FL in real-world scenarios.\nIn this talk, I will discuss our recent efforts to enhance the robustness and adaptability of FL systems. I will introduce REFL, a resource-efficient FL framework that decouples the collection of participant updates from model aggregation, intelligently selecting participants based on their likelihood of future availability to maximise resource utilisation. Then I will cover the development of FLOAT, an automated tuning framework that dynamically optimises resource utilisation to meet training deadlines, mitigating stragglers and dropouts through various optimisation techniques. Additionally, QKT is presented as a framework that enables tailored knowledge acquisition to fulfil specific client needs without direct data exchange, employing a data-free masking strategy to facilitate communication-efficient query-focused knowledge transfer while refining task-specific parameters to mitigate knowledge interference and forgetting. Finally, our UKRI-EPSRC-funded project, KUber, addresses these challenges by developing a distributed knowledge delivery system to enhance FL scalability and efficiency. KUber’s architecture facilitates seamless knowledge exchange among learning entities, optimising resource utilisation and model convergence.</p>\n<p><b>Bio</b>: <a href=\"https://www.qmul.ac.uk/eecs/people/profiles/sayedahmed.html\">https://www.qmul.ac.uk/eecs/people/profiles/sayedahmed.html</a></p>","id":"4813536d-c027-591d-987d-b6c4375ba0ef"},{"fields":{"slug":"/news/2025-09-22-C4DM_AIMLA_award"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#d8c8b8","images":{"fallback":{"src":"/static/8f7a261cac09f01f2128f6e6ab7afc3f/b26d3/AIMLA-award.jpg","srcSet":"/static/8f7a261cac09f01f2128f6e6ab7afc3f/47930/AIMLA-award.jpg 400w,\n/static/8f7a261cac09f01f2128f6e6ab7afc3f/baaed/AIMLA-award.jpg 800w,\n/static/8f7a261cac09f01f2128f6e6ab7afc3f/b26d3/AIMLA-award.jpg 1600w","sizes":"(min-width: 1600px) 1600px, 100vw"},"sources":[{"srcSet":"/static/8f7a261cac09f01f2128f6e6ab7afc3f/416c3/AIMLA-award.webp 400w,\n/static/8f7a261cac09f01f2128f6e6ab7afc3f/c1587/AIMLA-award.webp 800w,\n/static/8f7a261cac09f01f2128f6e6ab7afc3f/6e0f2/AIMLA-award.webp 1600w","type":"image/webp","sizes":"(min-width: 1600px) 1600px, 100vw"}]},"width":1600,"height":1600}}},"title":"C4DM PhD student wins best paper award at AES AIMLA 2025","author":"Admin","date":"Mon 22 Sep 2025"},"html":"<p>We are delighted to share that Mary Pilataki, a PhD student at C4DM, has received the Best Paper Award at the Audio Engineering Society International Conference on Artificial Intelligence and Machine Learning for Audio (AES AIMLA) 2025.</p>\n<p>The awarded paper, “Extraction and Neural Synthesis of Low-Order Spectral Components for Head-Related Transfer Functions”, presents research carried out during her internship at PlayStation London in collaboration with Cal Armstrong and Chris Buchanan. The study explores how deep learning can separate sound colouration from spatial cues in Head-Related Transfer Functions (HRTFs), opening up new possibilities for creating more personalised and immersive 3D audio experiences.</p>\n<p>The full paper is available through the <a href=\"https://aes2.org/publications/elibrary-page/?id=22993\">AES E-Library</a>.</p>","id":"4ae7a7df-3c63-5643-8846-0c27231d9217"},{"fields":{"slug":"/news/2025-08-09.C4DM-at_ISMIR_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/1f63c29bf8c9e3effafc2164312893eb/a5b34/ismir2025logo.png","srcSet":"/static/1f63c29bf8c9e3effafc2164312893eb/13677/ismir2025logo.png 1000w,\n/static/1f63c29bf8c9e3effafc2164312893eb/f21c0/ismir2025logo.png 2000w,\n/static/1f63c29bf8c9e3effafc2164312893eb/a5b34/ismir2025logo.png 4000w","sizes":"(min-width: 4000px) 4000px, 100vw"},"sources":[{"srcSet":"/static/1f63c29bf8c9e3effafc2164312893eb/3cd29/ismir2025logo.webp 1000w,\n/static/1f63c29bf8c9e3effafc2164312893eb/62c39/ismir2025logo.webp 2000w,\n/static/1f63c29bf8c9e3effafc2164312893eb/a6ca8/ismir2025logo.webp 4000w","type":"image/webp","sizes":"(min-width: 4000px) 4000px, 100vw"}]},"width":4000,"height":4000}}},"title":"C4DM at ISMIR 2025","author":"Admin","date":"Mon 08 Sep 2025"},"html":"<p></p>\n<p>On 21-25 September 2025, several C4DM researchers will participate at the <b><a href=\"https://ismir2025.ismir.net/\">26th International Society for Music Information Retrieval Conference (ISMIR 2025)</a></b>. ISMIR is the leading conference in the field of music informatics, and is currently the <a href=\"https://scholar.google.com/citations?view_op=top_venues&#x26;hl=en&#x26;vq=hum_musicmusicology\">top-cited publication for Music &#x26; Musicology</a> (source: Google Scholar). This year ISMIR will take place onsite in Daejeon, Korea.</p>\n<p>Similar to previous years, the Centre for Digital Music will have a strong presence at ISMIR 2025.</p>\n<p><br>In the <b>Scientific Programme</b>, the following papers are authored/co-authored by C4DM members:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2506.07199\">Audio Synthesizer Inversion in Symmetric Parameter Spaces with Approximately Equivariant Flow Matching</a> (Ben Hayes, Charalampos Saitis, György Fazekas)</li>\n<li><a href=\"https://arxiv.org/abs/2506.17815\">SLAP: Siamese Language Audio Pretraining without Negative Samples for Music Understanding</a> (Julien Guinot, Alain Riou, Elio Quinton, György Fazekas)</li>\n<li><a href=\"https://arxiv.org/abs/2506.17886\">GD-Retriever: Controllable Generative Text Music Retrieval with Diffusion Models</a> (Julien Guinot, Elio Quinton, György Fazekas)</li>\n<li><a href=\"https://arxiv.org/abs/2405.18386\">Instruct-MusicGen: Unlocking Text to Music Editing for Music Language Models via Instruction Tuning</a> (Yixiao Zhang, Yukara Ikemiya, Woosung Choi, Naoki Murata, Marco A. Martínez Ramírez, Liwei Lin, Gus Xia, Wei Hsiang Liao, Yuki Mitsufuji, Simon Dixon)</li>\n<li><a href=\"https://arxiv.org/abs/2506.23869\">Scaling Self Supervised Representation Learning for Symbolic Piano Performance</a> (Louis Bradshaw, Honglu Fan, Alexander Spangher, Stella Biderman, Simon Colton)</li>\n<li>Codicodec: Unifying Continuous and Discrete Compressed Representations of Audio (Marco Pasini, Stefan Lattner, György Fazekas)</li>\n<li><a href=\"https://arxiv.org/abs/2507.08530\">MIDI-VALLE: Improving Expressive Piano Performance Synthesis through Neural Codec Language Modelling</a> (Jingjing Tang, Xin Wang, Zhe Zhang, Junichi Yamagishi, Geraint Wiggins, György Fazekas)</li>\n<li><a href=\"https://arxiv.org/abs/2506.17055\">Universal Music Representations? Evaluating Foundation Models on World Music Corpora</a> (Charilaos Papaioannou, Emmanouil Benetos, Alexandros Potamianos)</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/107959\">Perceptual Errors in Music Source Separation: Looking Beyond SDR Averages</a> (Saurjya Sarkar, Victoria Moomjian, Basil Woods, Emmanouil Benetos, Mark Sandler)</li>\n<li><a href=\"https://github.com/JackJamesLoth/GOAT-Dataset/blob/main/GOAT_paper.pdf\">GOAT: a Large Dataset of Paired Guitar Audio Recordings and Tablatures</a> (Jackson Loth, Pedro Sarmento, Saurjya Sarkar, Zixun Guo, Mathieu Barthet, Mark Sandler)</li>\n<li><a href=\"https://arxiv.org/abs/2506.12285\">CMI-Bench: a Comprehensive Benchmark for Evaluating Music Instruction Following</a> (Yinghao Ma, Siyou Li, Juntao Yu, Emmanouil Benetos, Akira Maezawa)</li>\n<li><a href=\"https://arxiv.org/abs/2507.07764\">Assessing the Alignment of Audio Representations with Timbre Similarity Ratings</a> (Haokun Tian, Stefan Lattner, Charalampos Saitis)</li>\n<li><a href=\"https://arxiv.org/abs/2507.11233\">Improving Neural Pitch Estimation with SWIPE Kernels</a> (David Marttila, Joshua D. Reiss)</li>\n<li><a href=\"https://www.arxiv.org/abs/2506.14684\">Refining Music Sample Identification with a Self Supervised Graph Neural Network</a> (Aditya Bhattacharjee, Ivan Meresman Higgs, Mark Sandler, Emmanouil Benetos)</li>\n</ul>\n<p><br>The following <b><a href=\"https://ismir2025.ismir.net/program-tutorials\">Tutorials</a></b> will be co-presented by C4DM PhD students Rodrigo Diaz and Julien Guinot:</p>\n<ul>\n<li>Differentiable Physical Modeling Sound Synthesis: Theory, Musical Application, and Programming (Jin Woo Lee, Stefan Bilbao, Rodrigo Diaz)</li>\n<li>Self-supervised Learning for Music - An Overview and New Horizons (Julien Guinot, Alain Riou, Yuexuan Kong, Marco Pasini, Gabriel Meseguer-Brocal, Stefan Lattner)</li>\n</ul>\n<p><br>The following journal papers published at <b>TISMIR</b> which are co-authored by C4DM members will be presented at the conference:</p>\n<ul>\n<li><a href=\"https://transactions.ismir.net/articles/10.5334/tismir.214\">Predicting Eurovision Song Contest Results: A Hit Song Science Approach</a> (Katarzyna Adamska, Joshua Reiss)</li>\n<li><a href=\"https://transactions.ismir.net/articles/10.5334/tismir.203\">The GigaMIDI Dataset with Features for Expressive Music Performance Detection</a> (Keon Ju Lee, Jeff Ens, Sara Adkins, Pedro Sarmento, Mathieu Barthet, Philippe Pasquier)</li>\n</ul>\n<p><br>As part of the <b>MIREX public evaluations</b>:</p>\n<ul>\n<li>C4DM PhD student Yinghao Ma is task captain for the <a href=\"https://www.music-ir.org/mirex/wiki/2025:Music_Reasoning_QA\">Music Reasoning QA</a>, <a href=\"https://www.music-ir.org/mirex/wiki/2025:Audio_Beat_Tracking\">Audio Beat Tracking</a>, and <a href=\"https://www.music-ir.org/mirex/wiki/2025:Audio_Key_Detection\">Audio Key Detection</a> tasks</li>\n<li>C4DM PhD student Huan Zhang is task captain for <a href=\"https://www.music-ir.org/mirex/wiki/2025:RenCon\">RenCon 2025: Expressive Performance Rendering Competitionk</a></li>\n</ul>\n<p>Finally, on the organisational side:</p>\n<ul>\n<li>C4DM PhD student Chin-Yun Yu is Virtual Co-Chair for the ISMIR 2025 conference.</li>\n<li>C4DM PhD student Yinghao Ma is co-organising the satellite workshop <a href=\"https://m-a-p.ai/LLM4Music/\">LLM4MA: Large Language Models for Music &#x26; Audio</a></li>\n</ul>\n<p><br>See you at Daejeon!</p>","id":"0b1307c1-c5c7-5b93-86f5-afca1e60b144"}]}}}
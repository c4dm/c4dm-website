{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/YIFgS8she58","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/Sandler-Artificial-Neuroscience"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png","srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/2fd20/EPSRC.png 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/de391/EPSRC.png 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/d66e1/EPSRC.webp 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/e7160/EPSRC.webp 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/5f169/EPSRC.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Artificial Neuroscience: metrology and engineering for Deep Learning using Linear Algebra","author":"Prof Mark Sandler (PI), Boris Khoruzhenko (CI)","date":null,"link":"https://www.ukri.org/councils/epsrc/"},"html":"","id":"c1314000-fe3b-5175-88a2-f239d861ddad"},{"fields":{"slug":"/projects/Barthet-Netz"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Netz: A Novel XR Musical Instrument","author":"Dr Mathieu Barthet (PI)","date":null,"link":"https://iuk-business-connect.org.uk/programme/icure/"},"html":"","id":"0040b455-65cf-5ea1-a7ce-38ab70570dae"},{"fields":{"slug":"/projects/Benetos-L-Acoustics"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Online speech enhancement in scenarios with low direct-to-reverberant-ratio","author":"Dr Emmanouil Benetos (PI), Dr Aidan Hogg (CI)","date":null,"link":"https://www.l-acoustics.com/"},"html":"","id":"ce65fc6c-7228-5805-a9b2-6e14e5fd5086"},{"fields":{"slug":"/projects/Benetos-Algorivm"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Project Maestro - AI Musical Analysis Platform","author":"Dr Emmanouil Benetos (PI), Prof Simon Dixon (CI)","date":null,"link":"https://www.ukri.org/councils/innovate-uk/"},"html":"","id":"78c348d8-d78f-5acf-8441-382bc6c4d791"},{"fields":{"slug":"/projects/Benetos-Moises"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Enhancing lyrics transcription with open-source architectures and fine-tuning techniques","author":"Dr Emmanouil Benetos (PI)","date":null,"link":"https://moises.ai/"},"html":"","id":"d61977ff-9345-55a4-b85e-44caffa4a7f0"},{"fields":{"slug":"/projects/Dixon-assessment"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Music Performance Assessment and Feedback","author":"Prof Simon Dixon (PI), Dr Emmanouil Benetos (CI)","date":null,"link":null},"html":"","id":"68ee56ea-59b7-58de-a8c9-805c4aba4357"}]},"news":{"nodes":[{"fields":{"slug":"/news/2025-06-24.C4DM-Seminar_Ye_Wang"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/7679529f166e79781779fa56b1619637/a687e/yewang.jpg","srcSet":"/static/7679529f166e79781779fa56b1619637/60f77/yewang.jpg 37w,\n/static/7679529f166e79781779fa56b1619637/deb6c/yewang.jpg 74w,\n/static/7679529f166e79781779fa56b1619637/a687e/yewang.jpg 148w","sizes":"(min-width: 148px) 148px, 100vw"},"sources":[{"srcSet":"/static/7679529f166e79781779fa56b1619637/7e23d/yewang.webp 37w,\n/static/7679529f166e79781779fa56b1619637/e1943/yewang.webp 74w,\n/static/7679529f166e79781779fa56b1619637/4c54c/yewang.webp 148w","type":"image/webp","sizes":"(min-width: 148px) 148px, 100vw"}]},"width":148,"height":148}}},"title":"C4DM Seminar: Ye Wang","author":"nicolaus625","date":"Tue 24 Jun 2025"},"html":"<h3>C4DM Seminar: Ye Wang: Speech and Music AI: Connecting the Dots of AI and Human Potential</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Prof. Ye Wang (National University of Singapore)</p>\n<p><strong>Date/time:</strong>  Tuesday, 24th June 2025, 11am</p>\n<p><strong>Location:</strong> GC201, Graduate Centre, Mile End Campus, QMUL</p>\n<h2><b>Title</b>: Speech and Music AI: Connecting the Dots of AI and Human Potential</h2>\n<p><b>Abstract</b>: Speech and music are traditionally studied by different research communities. In this talk, I will share some insights from my journey of connecting the dots of speech, music, AI and neuroscience for real world applications in boosting human health and potential. This decade-long exploration has shaped my signature research – Sound and Music Computing for Human Health and Potential (SMC4HHP).</p>\n<p><b>Bio</b>: Prof. Ye Wang is affiliated with the Computer Science Department at the National University of Singapore (NUS), the NUS Graduate School’s Integrative Sciences and Engineering Programme (ISEP), the Institute of Data Science (IDS), as well as the Institute for Applied Learning Sciences &#x26; Educational Technology (ALSET). He established and continues to direct the NUS Sound and Music Computing (SMC) Lab (<a href=\"https://smcnus.comp.nus.edu.sg/\">https://smcnus.comp.nus.edu.sg/</a>). Before joining NUS, he was a member of the technical staff at Nokia Research Center in Tampere, Finland for 9 years. His research sits at the intersection of Sound and Music Computing (SMC) and Human Health and Potential (HHP).</p>","id":"fc49e0d5-2fe3-5419-b19e-61f9d82a3e24"},{"fields":{"slug":"/news/2025-06-12.Mark-Sandler_presents_at_Erlangen_AI_Hub_Conference"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#585858","images":{"fallback":{"src":"/static/9eef75c01631f1a2caf17509f71a027f/d931f/Sandler-Erlangen.jpg","srcSet":"/static/9eef75c01631f1a2caf17509f71a027f/c2c05/Sandler-Erlangen.jpg 384w,\n/static/9eef75c01631f1a2caf17509f71a027f/b9792/Sandler-Erlangen.jpg 768w,\n/static/9eef75c01631f1a2caf17509f71a027f/d931f/Sandler-Erlangen.jpg 1536w","sizes":"(min-width: 1536px) 1536px, 100vw"},"sources":[{"srcSet":"/static/9eef75c01631f1a2caf17509f71a027f/6d535/Sandler-Erlangen.webp 384w,\n/static/9eef75c01631f1a2caf17509f71a027f/482be/Sandler-Erlangen.webp 768w,\n/static/9eef75c01631f1a2caf17509f71a027f/52a60/Sandler-Erlangen.webp 1536w","type":"image/webp","sizes":"(min-width: 1536px) 1536px, 100vw"}]},"width":1536,"height":1536}}},"title":"Mark Sandler presents at Erlangen AI Hub Conference","author":"Admin","date":"Thu 12 Jun 2025"},"html":"<p></p>\n<p>The <a href=\"https://erlangenhub.ox.ac.uk/hubs-major-conference-brings-together-leading-minds-at-the-intersection-of-mathematics-and-ai/\">Erlangen AI Hub Conference</a> took place on 9-11 June 2025 at Queen Mary University of London. It brought together over 100 leading minds from across the UK’s mathematical, algorithmic and computational communities to advance the application of pure mathematics in AI. It formed a key element of our exciting programme that aims to unite and revolutionise the mathematical field to unlock new and improved AI systems.</p>\n<p>At the conference, C4DM Director <a href=\"https://www.qmul.ac.uk/eecs/people/profiles/sandlermark.html\">Prof. Mark Sandler</a> presented \"The Case for Artificial Neuroscience: Holistic Rigour for Understanding and Engineering Better Deep Learning\". The talk is linked with Prof. Sandler's ongoing project funded by the EPSRC entitled <a href=\"https://gtr.ukri.org/projects?ref=EP%2FZ535448%2F1\">Artificial Neuroscience: metrology and engineering for Deep Learning using Linear Algebra</a>.</p>\n<p>The EPSRC-funded Erlangen AI Hub exists to revolutionise the application of pure mathematics to understand AI, unifying and expanding the field to unlock new, more intelligent systems - more information on the hub can be found at <a href=\"https://erlangenhub.ox.ac.uk/\">https://erlangenhub.ox.ac.uk/</a>.</p>","id":"c8d84686-13c0-5a52-838b-2badd8267e63"},{"fields":{"slug":"/news/2025-06-06.C4DM-Seminar_Tuomas_Eerola"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Tuomas Eerola","author":"Admin","date":"Fri 06 Jun 2025"},"html":"<h3>C4DM Seminar: Tuomas Eerola: Computational Recognition of Emotions in Music: A Meta-Analysis and Critical Review</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Tuomas Eerola</p>\n<p><strong>Date/time:</strong>  Friday, 6th June 2025, 11am</p>\n<p><strong>Location:</strong> G2, Engineering Building, Mile End Campus, Queen Mary University of London, E1 4NS</p>\n<p><strong>Zoom:</strong> <a href=\"https://qmul-ac-uk.zoom.us/j/2387202947\">https://qmul-ac-uk.zoom.us/j/2387202947</a></p>\n<h2><b>Title</b>: Computational Recognition of Emotions in Music: A Meta-Analysis and Critical Review</h2>\n<p><b>Abstract</b>:\nThis talk presents a meta-analysis of music emotion recognition (MER) models published between 2014 and 2024, focusing on predictions of valence, arousal, and categorical emotions. A total of 553 studies were identified, of which 96 full-text articles were assessed. This resulted in a final review of 34 studies comprising 204 distinct models. Valence and arousal were predicted with reasonable accuracy (r = 0.67 and r = 0.81, respectively), while classification models achieved an accuracy of 0.87. Across modeling approaches, linear and tree-based methods generally outperformed neural networks in regression tasks, whereas neural networks and support vector machines showed the highest performance in classification tasks. The talk will address several critical issues, including conceptual shortcomings, insufficient quality control, lack of transparency, limited dataset size, and a narrow range of predicted emotions in computational approaches to music emotion recognition.</p>\n<p><b>Bio</b>:\nTuomas Eerola is a music psychologist and Professor of Music Cognition at Durham University, UK. He is a leading researcher in the field of music cognition, using empirical experiments, theorising, and computational models to study how people engage with and process music. His work encompasses a wide range of topics, including emotions induced by music and perception of musical structure, rhythm, timbre, consonance, and emotional communication through music. He has published more than 160 papers and book chapters and is on the editorial boards of several prominent music psychology journals. His research has been funded by the Academy of Finland, AHRC (UK), ESRC (UK), and EU Horizon 2020. Eerola is the author of the book \"<a href=\"https://www.taylorfrancis.com/books/mono/10.4324/9781003293804/music-science-tuomas-eerola\">Music and Science - Guide to Empirical Research</a>\" published by Routledge in 2024.<br>\nUniversity profile: <a href=\"https://www.durham.ac.uk/staff/tuomas-eerola/\">https://www.durham.ac.uk/staff/tuomas-eerola/</a><br>\nGitHub: <a href=\"https://tuomaseerola.github.io\">https://tuomaseerola.github.io</a><br>\nMusic &#x26; Science Lab: <a href=\"https://musicscience.net\">https://musicscience.net</a></p>","id":"59c097a8-d22f-5d61-8385-d42c336a9f1f"},{"fields":{"slug":"/news/2025-06-04.C4DM-involvement_in_MIREX_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/2aa145e9a51656c8fa0d1efaa043bb46/1fa44/MIREX-logo.png","srcSet":"/static/2aa145e9a51656c8fa0d1efaa043bb46/6fcb3/MIREX-logo.png 135w,\n/static/2aa145e9a51656c8fa0d1efaa043bb46/08932/MIREX-logo.png 270w,\n/static/2aa145e9a51656c8fa0d1efaa043bb46/1fa44/MIREX-logo.png 540w","sizes":"(min-width: 540px) 540px, 100vw"},"sources":[{"srcSet":"/static/2aa145e9a51656c8fa0d1efaa043bb46/e01df/MIREX-logo.webp 135w,\n/static/2aa145e9a51656c8fa0d1efaa043bb46/ede49/MIREX-logo.webp 270w,\n/static/2aa145e9a51656c8fa0d1efaa043bb46/4cb34/MIREX-logo.webp 540w","type":"image/webp","sizes":"(min-width: 540px) 540px, 100vw"}]},"width":540,"height":540}}},"title":"C4DM involvement in MIREX 2025","author":"Admin","date":"Wed 04 Jun 2025"},"html":"<p></p>\n<p><b><a href=\"https://www.music-ir.org/mirex/wiki/MIREX_HOME\">MIREX (Music Information Retrieval Evaluation eXchange)</a></b> is a prominent evaluation platform in the field of music information retrieval. Researchers are invited to submit novel algorithms for a variety of music-related tasks and receive standardized evaluation results, with the opportunity to present posters during the annual ISMIR conference. For more details on submission, please see the following link:</p>\n<ul>\n<li>homepage: <a href=\"https://www.music-ir.org/mirex/wiki/MIREX_HOME\">https://www.music-ir.org/mirex/wiki/MIREX_HOME</a></li>\n<li>contact email: <a href=\"mailto:future-mirex@googlegroups.com\">future-mirex@googlegroups.com</a></li>\n<li>Linkedin pages: <a href=\"https://www.linkedin.com/company/future-mirex/\">https://www.linkedin.com/company/future-mirex/</a></li>\n<li>Discord group: <a href=\"https://discord.gg/vC2YWX29sC\">https://discord.gg/vC2YWX29sC</a></li>\n</ul>\n<p>THis year, C4DM PhD students <a href=\"https://nicolaus625.github.io/\">Yinghao Ma</a> and <a href=\"https://www.huanz.space/\">Huan Zhang</a> introduced new tasks to the platform, including Multimodal Music QA, Expressive Piano Performance Rendering, alongside traditional MIR challenges and emerging understanding/generation tasks. Specifically we are coordinating the following tasks:</p>\n<p><b>Music Reasoning QA</b><br>\nTask Captain: Yinghao Ma<br>\nThe MIREX 2025 Music Reasoning Question Answering (QA) Task challenges participants to develop models capable of answering natural language questions that require understanding and reasoning over musical audio. This task seeks to advance the frontier of machine music intelligence by evaluating models on their ability to reason about all kinds of music information musical structure, instrument presence, melody information, vocal content, and environmental context etc., along with knowledge in music theory and music history.\nParticipants will build systems that answer multiple-choice questions grounded in audio inputs. The task includes questions from four curated subsets (Music, Music-Speech, Sound-Music, Sound-Music-Speech) from the MMAR benchmark, and Music-subset with image caption from the OmniBench benchmark. Each question is paired with an audio clip and 2-4 different choices.</p>\n<p><b>RenCon: Expressive Piano Performance Rendering Contest</b><br>\nTask Captain: Huan Zhang<br>\nExpressive Performance Rendering (<a href=\"https://ren-con2025.vercel.app/\">https://ren-con2025.vercel.app/</a>) is a task that challenges participants to develop systems capable of rendering expressive musical performances from symbolic scores in MusicXML format. We accept system that generate symbolic (MIDI) or audio (wav) renderings, and the output shall contain human-like expressive deviation from the MusicXML score.\nSimilar to AI song contest, the evaluation of expressive rendering is subjective and requires human judges to assess. Thus, we have a two-phase competition structure: Phase 1 – Preliminary Round (Online) Submit performances of assigned and free-choice pieces. The submission period is open from May 30, 2025 to Aug 20, 2025. After the submission deadline, the preliminary round page will be finalized with the list of participants and their submissions, and the online evaluation will take place. Phase 2 – Live Contest at ISMIR (Daejeon, Korea) Top systems from preliminary round will be invited to render a surprise piece live at ISMIR, using their system in real time. The live contest is open to all ISMIR attendees, as well as the general public. The audience will be able to listen to the live performances and vote for their favorite system.</p>\n<p><b>Audio Beat Tracking</b><br>\nTask Captain: Wenye Ma &#x26; Yinghao Ma<br>\nThe aim of the automatic beat tracking task is to track each beat locations in a collection of sound files. Unlike the Audio Tempo Extraction task, which aim is to detect tempi for each file, the beat tracking task aims at detecting all beat locations in recordings. The algorithms will be evaluated in terms of their accuracy in predicting beat locations annotated by a group of listeners.</p>\n<p><b>Audio Key Detection</b><br>\nTask Captain: Wenye Ma &#x26; Yinghao Ma<br>\nAudio Key Detection aims to identify the musical key (e.g., C major, A minor) of an audio recording. This involves determining both the tonic (root pitch) and the mode (major or minor) from the audio signal.</p>","id":"3f04602d-8953-5c8d-84ca-4016b73768eb"},{"fields":{"slug":"/news/2025-05-29.C4DM-Seminar_Alberto_Bernardini"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Alberto Bernardini","author":"Admin","date":"Thu 29 May 2025"},"html":"<h3>C4DM Seminar: Alberto Bernardini:  Differentiable Physical Models of Acoustic Systems for Audio Signal Processing</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>EECS Seminar &#x26; Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Alberto Bernardini</p>\n<p><strong>Date/time:</strong>  Thursday, 29th May 2025, 11am</p>\n<p><strong>Location:</strong> GC204, Graduate Centre Building, Mile End Campus, Queen Mary University of London, E1 4NS</p>\n<p><strong>Zoom:</strong> <a href=\"https://qmul-ac-uk.zoom.us/j/9798452959\">https://qmul-ac-uk.zoom.us/j/9798452959</a></p>\n<h2><b>Title</b>: Differentiable Physical Models of Acoustic Systems for Audio Signal Processing</h2>\n<p><b>Abstract</b>:\nAccurate simulations of acoustic systems—such as analog audio circuits, transducers, and acoustic reverberant environments—are often computationally demanding, which limits their use in real-time or online audio signal processing contexts. To overcome this challenge, there is a growing need for physically motivated models that are both computationally efficient and sufficiently accurate to be embedded in audio processing pipelines. In recent years, automatic differentiation techniques have opened new possibilities by enabling the direct optimization of physical model parameters using measured data. This development has given rise to hybrid modeling approaches that combine the interpretability and robustness of physics-based models with the adaptability and accuracy of data-driven methods. In this seminar, we will explore the emerging field of differentiable physical modeling for audio applications. We will present concrete examples including models of analog audio circuits for Virtual Analog Modeling, physical models of acoustic transducers such as loudspeakers and microphones, and artificial reverberation systems based on Delay Networks. These case studies will demonstrate how differentiable models can be leveraged to bridge simulation and machine learning in the design of modern audio processing systems.</p>\n<p><b>Bio</b>:\nAlberto Bernardini received the B.S. degree in computer engineering from the University of Bologna, Bologna, Italy, in 2012, and the M.S. degree (cum laude) in computer engineering and the Ph.D. degree (cum laude) in information engineering from the Politecnico di Milano, Milan, Italy, in 2015 and 2019, respectively. From 2019 to 2021, he was a Postdoctoral Researcher with the Politecnico di Milano. He is currently an Assistant Professor with the Dipartimento di Elettronica, Informazione e Bioingegneria (DEIB), Politecnico di Milano. He authored more than 80 publications in international journals and proceedings of international conferences. He is coauthor of three international patents. His research interests mainly include audio signal processing, computational acoustics, and modeling of nonlinear systems. He was a recipient of the Dimitris N. Chorafas Award in 2019. He is an IEEE Senior Member. He is an Associate Member of the Digital Signal Processing Technical Committee and a Regular Member of the Nonlinear Circuits and Systems Technical Committee, both of the IEEE Circuits and Systems Society.  He is a Member of the EURASIP Signal and Data Analytics for Machine Learning Technical Area Committee. He was an Associate Editor for the IEEE Transactions on Circuits and Systems I: Regular Papers. He is now an Associate Editor for the IEEE/ACM Transactions on Audio, Speech, and Language Processing, the Elsevier Digital Signal Processing journal, and the EURASIP Journal on Audio, Speech, and Music Processing.</p>","id":"c69e6934-ab14-516e-a1d8-044599a65e0d"},{"fields":{"slug":"/news/2025-05-29.New-Vamp_Plugins"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#787868","images":{"fallback":{"src":"/static/c538d9becca0722fd0519bac46cfbeab/dff79/new-vamp-plugins.jpg","srcSet":"/static/c538d9becca0722fd0519bac46cfbeab/1066c/new-vamp-plugins.jpg 166w,\n/static/c538d9becca0722fd0519bac46cfbeab/2762b/new-vamp-plugins.jpg 331w,\n/static/c538d9becca0722fd0519bac46cfbeab/dff79/new-vamp-plugins.jpg 662w","sizes":"(min-width: 662px) 662px, 100vw"},"sources":[{"srcSet":"/static/c538d9becca0722fd0519bac46cfbeab/d067e/new-vamp-plugins.webp 166w,\n/static/c538d9becca0722fd0519bac46cfbeab/4c234/new-vamp-plugins.webp 331w,\n/static/c538d9becca0722fd0519bac46cfbeab/a284e/new-vamp-plugins.webp 662w","type":"image/webp","sizes":"(min-width: 662px) 662px, 100vw"}]},"width":662,"height":662}}},"title":" New Vamp plugins developed at C4DM","author":"Admin","date":"Thu 29 May 2025"},"html":"<p></p>\n<p><a href=\"https://www.linkedin.com/in/cannam/?lipi=urn%3Ali%3Apage%3Ad_flagship3_detail_base%3BigSsnSezQ9KdnODYvStwOA%3D%3D\">Chris Cannam</a>, Principal Research Software Developer at the Centre for Digital Music at QMUL has just completed a project that involved creating two new Vamp plugins from machine learning models for music audio:</p>\n<ul>\n<li>\n<p><a href=\"https://github.com/cannam/mert-vamp-plugin\">MERT Vamp Plugin</a>: a feature extractor for <a href=\"https://github.com/yizhilll/MERT\">MERT audio features</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/cannam/vamp-lossy-encoding-detector\">Vamp Lossy Encoding Detector</a>: detect whether music audio has previously been encoded via lossy compression</p>\n</li>\n</ul>\n<p>More info can be found in the below blog post: <a href=\"https://thebreakfastpost.com/2025/05/29/mert-vamp-plugin-and-a-lossy-encoding-detector/\">https://thebreakfastpost.com/2025/05/29/mert-vamp-plugin-and-a-lossy-encoding-detector/</a>\nAnd more info on Vamp plugins and how to use them can be found at: <a href=\"https://www.vamp-plugins.org/\">https://www.vamp-plugins.org/</a></p>","id":"6870f9e4-dd9e-5c63-b444-26f864e3c5c3"}]}}}
{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/YIFgS8she58","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/EPSRC-additional-skills"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png","srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/2fd20/EPSRC.png 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/de391/EPSRC.png 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/d66e1/EPSRC.webp 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/e7160/EPSRC.webp 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/5f169/EPSRC.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"EPSRC additional skills funding summer 2025","author":"Prof Akram Alomainy (PI), Prof Simon Dixon (CI), Dr Iran Roman (CI) plus 8 others","date":null,"link":"https://www.ukri.org/councils/epsrc/"},"html":"","id":"bc1e4be0-d104-5b53-8c02-61f296532555"},{"fields":{"slug":"/projects/Fazekas-Leverhulme"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/14ae7694223d03edb7966d74ea33ce18/82c11/RAEng.png","srcSet":"/static/14ae7694223d03edb7966d74ea33ce18/2fd20/RAEng.png 125w,\n/static/14ae7694223d03edb7966d74ea33ce18/de391/RAEng.png 250w,\n/static/14ae7694223d03edb7966d74ea33ce18/82c11/RAEng.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/14ae7694223d03edb7966d74ea33ce18/d66e1/RAEng.webp 125w,\n/static/14ae7694223d03edb7966d74ea33ce18/e7160/RAEng.webp 250w,\n/static/14ae7694223d03edb7966d74ea33ce18/5f169/RAEng.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Knowledge-driven Deep Learning for Music Informatics","author":"Dr George Fazekas (PI)","date":null,"link":"https://raeng.org.uk/programmes-and-prizes/programmes/uk-grants-and-prizes/support-for-research/research-awardees/leverhulme-awardees/2025-2026/dr-george-fazekas/"},"html":"","id":"f47b222e-21cf-5c90-bff2-6181b436d1aa"},{"fields":{"slug":"/projects/Fazekas-Steinberg-2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png","srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/4b686/aimcdt.png 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/d2213/aimcdt.png 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png 615w","sizes":"(min-width: 615px) 615px, 100vw"},"sources":[{"srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/b2942/aimcdt.webp 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/46581/aimcdt.webp 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/fa942/aimcdt.webp 615w","type":"image/webp","sizes":"(min-width: 615px) 615px, 100vw"}]},"width":615,"height":615}}},"title":"AI-Powered Audio Loop Generation for Assistive Music Production","author":"Dr George Fazekas (PI)","date":null,"link":null},"html":"","id":"90759bed-47aa-57d3-8ce9-0dbe6a6b40ac"},{"fields":{"slug":"/projects/Ma-Google"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Large Language Models for Multimodal Music Understanding and Ethical Audio Generation","author":"Dr Emmanouil Benetos (PI), Yinghao Ma (PhD fellow)","date":null,"link":"https://research.google/programs-and-events/phd-fellowship/"},"html":"","id":"e4f5df67-9d4e-57da-bb16-3efcb2359e70"},{"fields":{"slug":"/projects/Pauwels-Sofilab"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/0dc43ebbd856c9b632bb3367d1386a39/391db/Sofilab.png","srcSet":"/static/0dc43ebbd856c9b632bb3367d1386a39/49369/Sofilab.png 924w,\n/static/0dc43ebbd856c9b632bb3367d1386a39/f82cb/Sofilab.png 1847w,\n/static/0dc43ebbd856c9b632bb3367d1386a39/391db/Sofilab.png 3694w","sizes":"(min-width: 3694px) 3694px, 100vw"},"sources":[{"srcSet":"/static/0dc43ebbd856c9b632bb3367d1386a39/34b27/Sofilab.webp 924w,\n/static/0dc43ebbd856c9b632bb3367d1386a39/e7008/Sofilab.webp 1847w,\n/static/0dc43ebbd856c9b632bb3367d1386a39/1d94a/Sofilab.webp 3694w","type":"image/webp","sizes":"(min-width: 3694px) 3694px, 100vw"}]},"width":3694,"height":3694}}},"title":"Smart musical corpus technologies","author":"Dr Johan Pauwels (PI)","date":null,"link":"https://sofilab.art/"},"html":"","id":"5b93e208-074a-5baf-a108-023df35a29f0"},{"fields":{"slug":"/projects/Reiss-Yamaha"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Yamaha Visiting Researcher Collaboration","author":"Prof Josh Reiss (PI)","date":null,"link":"https://intelligentsoundengineering.wordpress.com/"},"html":"","id":"145db7f6-77af-5f89-b477-6c4d87952ef4"}]},"news":{"nodes":[{"fields":{"slug":"/news/2025-11-05.BMVA-Workshop"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#a8a898","images":{"fallback":{"src":"/static/f50b3f2ab8deecae63e8cdd69d401e0b/2b34e/BMVA2025.png","srcSet":"/static/f50b3f2ab8deecae63e8cdd69d401e0b/bcbe6/BMVA2025.png 245w,\n/static/f50b3f2ab8deecae63e8cdd69d401e0b/78a05/BMVA2025.png 490w,\n/static/f50b3f2ab8deecae63e8cdd69d401e0b/2b34e/BMVA2025.png 979w","sizes":"(min-width: 979px) 979px, 100vw"},"sources":[{"srcSet":"/static/f50b3f2ab8deecae63e8cdd69d401e0b/c4230/BMVA2025.webp 245w,\n/static/f50b3f2ab8deecae63e8cdd69d401e0b/a5fb7/BMVA2025.webp 490w,\n/static/f50b3f2ab8deecae63e8cdd69d401e0b/758a3/BMVA2025.webp 979w","type":"image/webp","sizes":"(min-width: 979px) 979px, 100vw"}]},"width":979,"height":979}}},"title":"The British Machine Vision Association Workshop on Multimodal LLMs","author":"admin","date":"Wed 05 Nov 2025"},"html":"<p>The <a href=\"https://www.bmva.org/meetings/25-11-05-MultimodalLargeModels.html\">British Machine Vision Association and Society for Pattern Recognition (BMVA) Workshop on Multimodal Large Models Bridging Vision, Language, and Beyond</a> was held at British Computer Society (BCS), 25 Copthall Avenue, London EC2R 7BP on November 5th, 2025.</p>\n<p>Among the selected oral presentations, C4DM PhD student <strong>Yinghao Ma</strong> (supervised by Prof. Emmanouil Benetos) presented his latest work <em>MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix</em>. MMAR is a newly released benchmark that spans 1,000 real-world audio reasoning tasks, covering speech, sound events, music, and mixed-modality scenarios. It is one of the first benchmarks to explicitly evaluate multi-step reasoning abilities in audio-language and omni-modal large models, with tasks ranging from low-level signal perception to high-level cultural understanding.</p>\n<p>The talk was part of the “Domain Applications and Human-Centric Modalities” session, alongside research on sign language translation, visual illusions, and 3D-aware facial editing. MMAR attracted interest from both academia and industry attendees, especially as multimodal reasoning becomes a key focus in the next wave of AI foundation models.</p>\n<p>The BMVA workshop featured keynote speakers from Google DeepMind, UCL, and the University of Surrey, and brought together researchers advancing the frontier of multimodal intelligence across vision, language, audio, and embodied learning.</p>\n<p>MMAR is open-source and available on arXiv: <strong>arXiv:2505.13032</strong>. The video recording of presentation is available at <a href=\"https://www.bmva.org/meetings/25-11-05-MultimodalLargeModels.html\">here</a></p>","id":"8f6388ab-107d-5585-8923-6db01cfbded3"},{"fields":{"slug":"/news/2025-11-05.C4DM-Seminar_Masataka_Goto"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Masataka Goto","author":null,"date":"Wed 05 Nov 2025"},"html":"<h3>C4DM Seminar: Masataka Goto: Advancing Music Experience Through Music Information Research</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Masataka Goto</p>\n<p><strong>Date/time:</strong>  Wednesday, 5th Nov 2025, 3.00pm</p>\n<p><strong>Location:</strong> In-person only. Mile Eng campus, Eng building, room G2</p>\n<h2><b>Title</b>: Advancing Music Experience Through Music Information Research</h2>\n<p><b>Abstract</b>:\nMusic technologies will open up new ways of enjoying music, both in terms of creating and appreciating music. In this talk, I will discuss how music information research can enrich music experiences by introducing examples of our research outcomes. For example, \"Lyric Apps\" offer a new form of lyric-driven visual art, dynamically rendering different visual content based on user interaction. After releasing \"TextAlive App API\", a web-based framework for creating lyric apps, which has received several awards including at ACM CHI 2023, we have held annual programming contests since 2020. Another example is \"Kiite Cafe\", a web service that allows users to get together virtually to listen to music. It lets users enjoy the same song simultaneously while reacting in real time, creating a shared listening experience, and has also received multiple awards. In the future, further advances in music information research will make interactions between people and music more active and enriching.</p>\n<p><b>Bio</b>:\nMasataka Goto received the Doctor of Engineering degree from Waseda University in 1998. He is currently the Senior Principal Researcher at the National Institute of Advanced Industrial Science and Technology (AIST), Japan. Over the past 33 years, he has published more than 350 papers in refereed journals and international conference proceedings and has received 76 awards, including several best paper awards, best presentation awards, the Tenth Japan Academy Medal, and the Tenth JSPS PRIZE. He has served as a committee member of over 140 scientific societies and conferences, including as the General Chair of ISMIR 2009 and 2014 and the Program Chair of ISMIR 2022.</p>","id":"ac072711-e924-5b7a-a17a-02e79c202de8"},{"fields":{"slug":"/news/2025-10-23.C4DM-student-awarded-Google-PhD-fellowship"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4ac60065bd12d76e2e9830ee1a3f805a/a89ca/yinghao.jpg","srcSet":"/static/4ac60065bd12d76e2e9830ee1a3f805a/96deb/yinghao.jpg 150w,\n/static/4ac60065bd12d76e2e9830ee1a3f805a/0fdf4/yinghao.jpg 300w,\n/static/4ac60065bd12d76e2e9830ee1a3f805a/a89ca/yinghao.jpg 600w","sizes":"(min-width: 600px) 600px, 100vw"},"sources":[{"srcSet":"/static/4ac60065bd12d76e2e9830ee1a3f805a/c65bc/yinghao.webp 150w,\n/static/4ac60065bd12d76e2e9830ee1a3f805a/078c3/yinghao.webp 300w,\n/static/4ac60065bd12d76e2e9830ee1a3f805a/6d09e/yinghao.webp 600w","type":"image/webp","sizes":"(min-width: 600px) 600px, 100vw"}]},"width":600,"height":600}}},"title":"C4DM PhD student awarded Google PhD Fellowship","author":"Admin","date":"Thu 23 Oct 2025"},"html":"<p>We are extremely proud to announce that <a href=\"https://nicolaus625.github.io/\">Yinghao Ma</a>, a PhD student in AI and Music at the Centre for Digital Music at QMUL, has been awarded the <a href=\"https://research.google/programs-and-events/phd-fellowship/recipients/?filtertab=2025\">2025 Google Fellowship in Machine Perception</a>.</p>\n<p>A Google spokesperson said: “The student nominations we received this year were exemplary in their quality, but Yinghao especially stood out and was endorsed by the research scientists and distinguished engineers within Google who participated in the review. Congratulations to Yinghao on this well-deserved recognition, it’s an honor to support such incredibly talented students.”</p>\n<p>Yinghao's PhD research focuses on advancing Large Language Models (LLMs) for music understanding and generation. Specifically, he studies how multimodal models can integrate audio, symbolic, and textual information to understand, reason about, and generate music.</p>\n<p>Together with colleagues, he developed <a href=\"https://arxiv.org/abs/2306.00107\">MERT</a>, a large-scale music audio representation model which has more than 10k monthly download in the past three years. His recent work includes developing music instruction-following datasets and benchmarks that help evaluate how well AI systems can comprehend and create music.</p>\n<p>He said: \"It's my great honour to receive the Google PhD Fellowship that recognises my research and strongly contribute to my future career. I’m deeply grateful to Google and QMUL for the support, providing good platforms for AI &#x26; music research.\"</p>\n<p>Congratulations Yinghao!</p>","id":"1ec9aa73-6b0c-5967-b4d1-2b1e075f8ebe"},{"fields":{"slug":"/news/2025-10-22.C4DM-Seminar_Come_Peladeau"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Côme Peladeau","author":"Admin","date":"Wed 22 Oct 2025"},"html":"<h3>C4DM Seminar: Côme Peladeau: Audio processors: estimating parameter distributions using discrete normalizing flows and DDSP</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Côme Peladeau</p>\n<p><strong>Date/time:</strong>  Wednesday, 22th October 2025, 1pm</p>\n<p><strong>Location:</strong> Online</p>\n<p><strong>Zoom Link:</strong> <a href=\"https://qmul-ac-uk.zoom.us/j/2387202947\">https://qmul-ac-uk.zoom.us/j/2387202947</a></p>\n<h2><b>Title</b>: Audio processors: estimating parameter distributions using discrete normalizing flows and DDSP</h2>\n<p><b>Abstract</b>: Audio processor, such as audio effects or synthesizers, are widely used in popular music production.</p>\n<p>Their parameters control the quality of the output sound. Multiple combinations of parameters can lead to the same sound.</p>\n<p>While recent approaches have been proposed to estimate these parameters given only the output sound, those are deterministic, i.e. they only estimate a single solution among the many possible parameter configurations.</p>\n<p>This work proposes an approach to estimate all the combinations that lead to the target sound, in the form of a probability distribution.</p>\n<p>We achieve this using differentiable signal processing and discrete normalizing flows.</p>\n<p><b>Bio</b>: Côme Peladeau graduated with a masters degree in Acoustics, Signal Processing and Computer Science applied to Music at Sorbonne Université in 2023.</p>\n<p>He is now a PhD student working with Geoffroy Peeters and Dominique Fourer at the Information Processing and Communications Laboratory (LTCI) in Telecom Paris.</p>\n<p>HI work focuses on audio effects estimation using deep learning and differentiable signal processing.</p>","id":"184cbea7-8b9a-5196-9c4d-7a5b3238c12c"},{"fields":{"slug":"/news/2025-10-22.C4DM-Seminar_Nithya_Shikarpur"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Nithya Shikarpur","author":"Yinghao Ma","date":"Wed 22 Oct 2025"},"html":"<h3>C4DM Seminar: Nithya Shikarpur: Towards Generative Modeling and Interactive Performance for Hindustani Music and beyond</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Nithya Shikarpur</p>\n<p><strong>Date/time:</strong>  Wednesday, 22th October 2025, 3.30pm</p>\n<p><strong>Location:</strong> Hybrid. Peter Landin building, room 4.24</p>\n<p><strong>Teams meeting info:</strong> Meeting ID: 324 267 638 748 , Passcode: 7sV9PL6k</p>\n<h2><b>Title</b>: Towards Generative Modeling and Interactive Performance for Hindustani Music and beyond</h2>\n<p><b>Abstract</b>:\nRecent advances in generative music modeling open up rich avenues for creative exploration and expression. As both a musician and researcher, my work focuses on two interconnected goals: (1) developing generative models that meaningfully engage with the musical context and aesthetics of specific traditions, and (2) designing interactive systems that foster creative collaboration between humans and generative models. First, I will introduce GaMaDHaNi, a hierarchical generative model for Hindustani vocal music.  Through a hierarchical system, modeling pitch contours first followed by spectrograms, GaMaDHaNi captures the microtonal nuances that characterize this tradition. I will further discuss the challenges of conditioning such models on musically relevant parameters such as raga. Second, I will share insights from “cat-in-loop”, a collaborative performance created with the Cat in Black ensemble and my collaborators Weilu Ge and Hugo Garcia. This work explores the creative (mis)use of VampNet, a masked audio transformer model, as a tool for embodied improvisation and human-AI co-performance. Together, these projects reflect a broader inquiry into how generative systems can engage with musical practices - not merely as data, but as living, evolving traditions of sound, gesture, and interaction.</p>\n<p><b>Bio</b>:\nNithya Shikarpur is a second-year PhD student at MIT advised by Prof. Cheng Zhi Anna Huang. She is interested in the modeling of human-AI interactive systems for music creation and creativity especially for low-resource genres of music. Earlier she was at Université de Montréal and Mila for her M.Sc. where she worked with Prof. Cheng-Zhi Anna Huang. She is also a practitioner and performer of Hindustani classical vocal music and draws on this knowledge to further her research projects.</p>","id":"d41d0c28-10bf-52e2-b1bb-bc28bf1cd627"},{"fields":{"slug":"/news/2025-10-06.C4DM-at_WASPAA_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#a8c8e8","images":{"fallback":{"src":"/static/6871445306f9af07141444a39ae524a8/fad00/waspaa2025.png","srcSet":"/static/6871445306f9af07141444a39ae524a8/71cba/waspaa2025.png 291w,\n/static/6871445306f9af07141444a39ae524a8/fa1e5/waspaa2025.png 582w,\n/static/6871445306f9af07141444a39ae524a8/fad00/waspaa2025.png 1163w","sizes":"(min-width: 1163px) 1163px, 100vw"},"sources":[{"srcSet":"/static/6871445306f9af07141444a39ae524a8/46435/waspaa2025.webp 291w,\n/static/6871445306f9af07141444a39ae524a8/f27b2/waspaa2025.webp 582w,\n/static/6871445306f9af07141444a39ae524a8/24d0f/waspaa2025.webp 1163w","type":"image/webp","sizes":"(min-width: 1163px) 1163px, 100vw"}]},"width":1163,"height":1163}}},"title":"C4DM at WASPAA 2025","author":"Admin","date":"Mon 06 Oct 2025"},"html":"<p></p>\n<p>On 12-15 October, several C4DM researchers will participate at the <b><a href=\"http://www.waspaa.com/\">2025 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics</a></b>, taking place at the Granlibakken Tahoe Resort near Lake Tahoe, in Tahoe City, CA, USA. WASPAA is a premier event in the field of audio signal processing, organised by the IEEE's Audio and Acoustic Signal Processing (AASP) technical committee, with a strong focus on music signal processing and computational sound scene analysis.</p>\n<p>The Centre for Digital Music (C4DM), as in previous years, will have a strong presence at WASPAA 2025.</p>\n<p>In the <b>Technical Programme</b>, the following papers are authored by C4DM members:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2507.07066\">Latent Acoustic Mapping for Direction of Arrival Estimation: A Self-Supervised Approach</a> (Adrian S. Roman, Iran R. Roman, Juan Pablo Bello)</li>\n<li><a href=\"https://arxiv.org/abs/2507.12175\">RUMAA: Repeat-Aware Unified Music Audio Analysis for Score-Performance Alignment, Transcription, and Mistake Detection</a> (Sungkyun Chang, Simon Dixon, Emmanouil Benetos)</li>\n<li><a href=\"https://arxiv.org/abs/2510.06204\">Modulation Discovery with Differentiable Digital Signal Processing</a> (Christopher Mitcheltree, Hao Hao Tan, Joshua D. Reiss)</li>\n<li>Beyond Architecture: The Critical Impact of Inference Overlap on Music Source Separation Benchmarks (Harnick Khera, Johan Pauwels, Alan W. Archer-Boyd, Mark B. Sandler)</li>\n<li><a href=\"https://arxiv.org/abs/2505.11315\">Improving Inference-Time Optimisation for Vocal Effects Style Transfer with a Gaussian Prior</a> (Chin-Yun Yu, Marco A. Martínez-Ramírez, Junghyun Koo, Wei-Hsiang Liao, Yuki Mitsufuji, George Fazekas)</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/109071\">Self-Supervised Representation Learning with a JEPA Framework for Multi-instrument Music Transcription</a> (Mary Pilataki, Matthias Mauch, Simon Dixon)</li>\n</ul>\n<p>In the <b>Demo Session</b>, the following demos will be presented by C4DM members:</p>\n<ul>\n<li>Neural Audio Synthesis for Non-Keyboard Instruments (Franco Caspe, Andrew McPherson, Mark Sandler)</li>\n<li>PCA-DiffVox: Augmenting Vocal Effects Tweakability With a Bijective Latent Space (Chin-Yun Yu, Marco A. Martínez-Ramírez, Junghyun Koo, Wei-Hsiang Liao, Yuki Mitsufuji, George Fazekas)</li>\n</ul>\n<p>See you at WASPAA!</p>","id":"5422e2a2-1826-5148-b601-a7cb31a882f5"}]}}}
{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/YIFgS8she58","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/Benetos-Algorivm"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Project Maestro - AI Musical Analysis Platform","author":"Dr Emmanouil Benetos (PI), Prof Simon Dixon (CI)","date":null,"link":"https://www.ukri.org/councils/innovate-uk/"},"html":"","id":"78c348d8-d78f-5acf-8441-382bc6c4d791"},{"fields":{"slug":"/projects/Benetos-L-Acoustics"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Online speech enhancement in scenarios with low direct-to-reverberant-ratio","author":"Dr Emmanouil Benetos (PI), Dr Aidan Hogg (CI)","date":null,"link":"https://www.l-acoustics.com/"},"html":"","id":"ce65fc6c-7228-5805-a9b2-6e14e5fd5086"},{"fields":{"slug":"/projects/Benetos-Moises"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Enhancing lyrics transcription with open-source architectures and fine-tuning techniques","author":"Dr Emmanouil Benetos (PI)","date":null,"link":"https://moises.ai/"},"html":"","id":"d61977ff-9345-55a4-b85e-44caffa4a7f0"},{"fields":{"slug":"/projects/Dixon-assessment"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Music Performance Assessment and Feedback","author":"Prof Simon Dixon (PI), Dr Emmanouil Benetos (CI)","date":null,"link":null},"html":"","id":"68ee56ea-59b7-58de-a8c9-805c4aba4357"},{"fields":{"slug":"/projects/Dixon-guitar-transcription"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"High Resolution Guitar Transcription","author":"Prof Simon Dixon (PI)","date":null,"link":null},"html":"","id":"4dee84b0-faa4-549a-a883-d4de0a21e205"},{"fields":{"slug":"/projects/Reiss-ProStyle"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Music Production Style Transfer (ProStyle)","author":"Prof Josh Reiss (PI)","date":null,"link":"https://www.musicweek.com/digital/read/audio-production-start-up-roex-awarded-250-000-grant-by-innovate-uk-s-ai-funding-competition/089706"},"html":"","id":"473b36de-2173-5bac-bfac-c8d0c8b27c41"}]},"news":{"nodes":[{"fields":{"slug":"/news/2024-09-09.C4DM-Seminar_Peter_Meier"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar:  Peter Meier","author":"Admin","date":"Mon 09 Sep 2024"},"html":"<h3>C4DM Seminar: Peter Meier: Real-Time Beat Tracking and Control Signal Generation for Interactive Music Applications</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nPeter Meier</p>\n<p><strong>Date/time:  Monday, 9th September 2024, 2pm</strong></p>\n<p>**Location: GC105, Graduate Centre Building, Mile End Campus, QMUL, E1 4NS **\nZoom: <a href=\"https://qmul-ac-uk.zoom.us/j/2387202947\">https://qmul-ac-uk.zoom.us/j/2387202947</a></p>\n<h2><b>Title</b>: Real-Time Beat Tracking and Control Signal Generation for Interactive Music Applications</h2>\n<p><b>Abstract</b>: In this seminar, we will explore the theory and application of a real-time beat tracking system based on the Predominant Local Pulse (PLP) method. We will start by detailing how the traditional PLP algorithm has been transformed into a real-time procedure capable of operating with zero latency. This real-time adaptation not only tracks beat positions but also provides dynamic insights such as beat context, stability, and lookahead predictions for each frame of real-time audio. Following the theoretical overview, we will demonstrate practical applications of the system's outputs. These include the generation of beat-synchronous Low Frequency Oscillators (LFOs) and confidence-based control signals for dynamic manipulation of audio effect parameters in real-time. We will showcase a prototype audio plugin integrated within a Digital Audio Workstation (DAW), highlighting its utility in mixing scenarios and live music performances. Additionally, we will present various audio examples that use our system for controlling different audio effects in sync with the beat of the music.</p>\n<p><b>Bio</b>: Peter Meier is a PhD student at the International Audio Laboratories Erlangen, a joint institution of Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU) and the Fraunhofer Institute for Integrated Circuits (IIS) in Germany. Under the supervision of Prof. Meinard Müller, his research focuses on interactive music analysis, aiming to transform offline algorithms into real-time applications for creative music making and educational music gaming. Peter holds a Master's degree in Media Technology with a specialization in Media Informatics from the Deggendorf Institute of Technology in Germany, where he also gained 15 years of experience working as an audio engineer in the university's audio labs. Outside his academic work, Peter is a passionate musician who enjoys singing, drumming, and playing guitar.</p>","id":"955d4337-37c9-5966-b315-50769f1018be"},{"fields":{"slug":"/news/2024-09-03.C4DM-Seminar_Kazunobu_KONDO"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar:  Kazunobu KONDO (Yamaha)","author":"Admin","date":"Tue 03 Sep 2024"},"html":"<h3>C4DM Seminar: Kazunobu KONDO: Introduction of Yamaha Research and Development division and global research internship program</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nKazunobu KONDO</p>\n<p><strong>Date/time:  Tuesday, 3rd September 2024, 1.30pm</strong></p>\n<p>**Location: GC601 Montagu LT, Graduate Centre Building, Mile End Campus, QMUL, E1 4NS **\nZoom: <a href=\"https://qmul-ac-uk.zoom.us/j/9798452959\">https://qmul-ac-uk.zoom.us/j/9798452959</a></p>\n<h2><b>Title</b>: Introduction of Yamaha Research and Development division and global research internship program</h2>\n<p><b>Abstract</b>: Introducing our research direction, selected research topics, facilities of Yamaha Research and Development division.\nKansei (in Japanese) for music, which is a perceptual perspective of music understanding, is one of our key research quetions.\nMusic performers listen their own sound when performing in order to describe emotional musical expression.\nTherefore, we are analyzing sounds, musical symbols and human behavior to be utilized for AI-powered products.\nFrom September, we will begin accepting for our global internship program for the summer of 2025, in this presentation, the overview of our internship program will be introduced.</p>\n<p><b>Bio</b>: Kazunobu Kondo received the B.E., M.E. and Ph.D. degrees from Nagoya University, Japan, in 1991, 1993, and 2014, respectively.\nHe joined the Electronics Development Center, Yamaha Co., Ltd. in 1993.\nHe is currently a principal engineer of Yamaha Research and Development Division.\nHis research interests include blind source separation, noise reduction, and dereverbetation.\nHe is a member of the IEICE, the Acoustical Society of Japan and the Audio Engineering Society, and an editorial board member of the Journal of the Audio Engineering Society.</p>","id":"8b30fb78-449a-5470-953b-d4ae01280258"},{"fields":{"slug":"/news/2024-08-05.C4DM-at_EUSIPCO_2024"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#782868","images":{"fallback":{"src":"/static/d080ae1a003b1df4a4449f601bb0943e/b47f1/EUSIPCO2024.jpg","srcSet":"/static/d080ae1a003b1df4a4449f601bb0943e/89572/EUSIPCO2024.jpg 159w,\n/static/d080ae1a003b1df4a4449f601bb0943e/e3831/EUSIPCO2024.jpg 317w,\n/static/d080ae1a003b1df4a4449f601bb0943e/b47f1/EUSIPCO2024.jpg 634w","sizes":"(min-width: 634px) 634px, 100vw"},"sources":[{"srcSet":"/static/d080ae1a003b1df4a4449f601bb0943e/0993c/EUSIPCO2024.webp 159w,\n/static/d080ae1a003b1df4a4449f601bb0943e/1ce72/EUSIPCO2024.webp 317w,\n/static/d080ae1a003b1df4a4449f601bb0943e/6582b/EUSIPCO2024.webp 634w","type":"image/webp","sizes":"(min-width: 634px) 634px, 100vw"}]},"width":634,"height":634}}},"title":"C4DM at EUSIPCO 2024","author":"Emmanouil Benetos","date":"Mon 05 Aug 2024"},"html":"<p>On 26-30 August, C4DM researchers will participate in the <b><a href=\"https://eusipcolyon.sciencesconf.org/\">32nd European Signal Processing Conference (EUSIPCO 2024)</a></b> in Lyon, France. EUSIPCO is the flagship conference of the European Signal Processing Society (EURASIP) and offers a comprehensive technical program addressing all the latest developments in research and technology for signal processing.</p>\n<p>Centre for Digital Music members will be presenting the following works:</p>\n<ul>\n<li>\n<p><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/97337\">Towards building an end-to-end multilingual automatic lyrics transcription model</a>, by Jiawen Huang and Emmanouil Benetos</p>\n</li>\n<li>\n<p><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/97744\">Mind the domain gap: a systematic analysis on bioacoustic sound event detection</a>, by Jinhua Liang, Ines Nolasco, Burooj Ghani, Huy Phan, Emmanouil Benetos, and Dan Stowell</p>\n</li>\n</ul>\n<p>See you at EUSIPCO!</p>","id":"8f1f8257-f06a-5e41-989d-def19c908a44"},{"fields":{"slug":"/news/2024-07-30.C4DM-Seminar_Hyon_Kim"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar:  Hyon Kim","author":"Admin","date":"Tue 30 Jul 2024"},"html":"<h3>C4DM Seminar: Hyon Kim: Score Informed Note-level MIDI Velocity Estimation and Its Transcription into Symbolics</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nHyon Kim</p>\n<p><strong>Date/time:  Tuesday, 30th July 2024, 3pm</strong></p>\n<p>**Location: G2, ENG, Mile End Campus, QMUL, E1 4NS **\nZoom: <a href=\"https://qmul-ac-uk.zoom.us/j/9798452959\">https://qmul-ac-uk.zoom.us/j/9798452959</a></p>\n<h2><b>Title</b>: Score Informed Note-level MIDI Velocity Estimation and Its Transcription into Symbolics</h2>\n<p><b>Abstract</b>: It is a well known fact that the dynamics in piano performance gives significant effect in expressiveness. Taking the polyphonic nature of the instrument into account, analysing information to form dynamics for each performed note has significant meaning to understand piano performance in a quantitative way. It is also a key element in an education context for piano learners.　\nIn this study, we developed a model for estimating MIDI velocity for each note, as one of indicators to represent loudness, with a condition of score by a Deep Neural Network (DNN) and Feature-wise Linear Modulation (FiLM) conditioning. Additionally, we have conducted research to map MIDI velocities to dynamics markings by combining performance MIDI roll and MusicXML information into one sequence.</p>\n<p><b>Bio</b>: Hyon Kim is a PhD student at Music Technology Group, Universitat Pompeu Fabra under the supervision of Prof. Xavier Serra. Currently, Hyon is a Visiting Researcher at C4DM, QMUL, working under the supervision of Dr. Emmanouil Benetos. His research interests include modeling the dynamic information of piano performance and transcribing it into MIDI roll and symbolic representations using various DNN methods in a multimodal fashion, incorporating both score and audio data.</p>","id":"de847f8c-3aaf-5694-a7da-262f0eef140d"},{"fields":{"slug":"/news/2024-07-26.CfP.AIMLA.2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#083858","images":{"fallback":{"src":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/18c0c/AIMLA-logo.jpg","srcSet":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/0fdf4/AIMLA-logo.jpg 300w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/7706b/AIMLA-logo.jpg 601w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/18c0c/AIMLA-logo.jpg 1201w","sizes":"(min-width: 1201px) 1201px, 100vw"},"sources":[{"srcSet":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/078c3/AIMLA-logo.webp 300w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/2b014/AIMLA-logo.webp 601w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/1fcf1/AIMLA-logo.webp 1201w","type":"image/webp","sizes":"(min-width: 1201px) 1201px, 100vw"}]},"width":1201,"height":1201}}},"title":"CfP: First AES International Conference on AI and Machine Learning for Audio (AIMLA 2025)","author":"Emmanouil Benetos","date":"Fri 26 Jul 2024"},"html":"<p><b>First AES International Conference on Artificial Intelligence and Machine Learning for Audio (AIMLA 2025), Queen Mary University of London, Sept. 8-10, 2025, Call for contributions</b></p>\n<p>The Audio Engineering Society and the Centre for Digital Music invite audio researchers and practitioners, from academia and industry to participate in the first AES conference dedicated to artificial intelligence and machine learning, as it applies to audio. This 3 day event, aims to bring the community together, educate, demonstrate and advance the state of the art. It will feature keynote speakers, workshops, tutorials, challenges and cutting-edge peer-reviewed research.</p>\n<p>The scope is wide - expecting attendance from all types of institutions, including academia, industry, and pure research, with diverse disciplinary perspectives - but tied together by a focus on artificial intelligence and machine learning for audio.</p>\n<p>For more information on the Calls for Papers, Special Sessions, Tutorials, and Challenges, please visit the conference website: <a href=\"https://aes2.org/events-calendar/2025-aes-international-conference-on-artificial-intelligence-and-machine-learning-for-audio/\">https://aes2.org/events-calendar/2025-aes-international-conference-on-artificial-intelligence-and-machine-learning-for-audio/</a></p>","id":"fc63f924-e0c0-5472-bd4e-6091fc586901"},{"fields":{"slug":"/news/2024-07-25.C4DM-at_IJCAI_2024"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/b1555465f2a9005accfd26895310ca00/6fc22/IJCAI-2024.png","srcSet":"/static/b1555465f2a9005accfd26895310ca00/7d8ec/IJCAI-2024.png 105w,\n/static/b1555465f2a9005accfd26895310ca00/6f644/IJCAI-2024.png 211w,\n/static/b1555465f2a9005accfd26895310ca00/6fc22/IJCAI-2024.png 421w","sizes":"(min-width: 421px) 421px, 100vw"},"sources":[{"srcSet":"/static/b1555465f2a9005accfd26895310ca00/20fdf/IJCAI-2024.webp 105w,\n/static/b1555465f2a9005accfd26895310ca00/16179/IJCAI-2024.webp 211w,\n/static/b1555465f2a9005accfd26895310ca00/e0af3/IJCAI-2024.webp 421w","type":"image/webp","sizes":"(min-width: 421px) 421px, 100vw"}]},"width":421,"height":421}}},"title":"C4DM at IJCAI 2024","author":"Emmanouil Benetos","date":"Thu 25 Jul 2024"},"html":"<p>On 3-9 August, C4DM PhD student <a href=\"https://ldzhangyx.github.io/\">Yixiao Zhang</a> will participate in the <b><a href=\"https://ijcai24.org/\">33rd International Joint Conference on Artificial Intelligence (IJCAI 2024)</a></b> taking place in Jeju, South Korea. IJCAI has remained the premier conference bringing together the international AI community in communicating the advances and celebrating the achievements of artificial intelligence research and practice.</p>\n<p>At the conference, Yixiao will present the following paper as part of the <a href=\"https://ijcai24.org/ai-arts-creativity-special-track-accepted-papers/\">AI, Arts &#x26; Creativity Special Track</a>:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2402.06178\">MusicMagus: Zero-Shot Text-to-Music Editing via Diffusion Models</a>, by Yixiao Zhang, Yukara Ikemiya, Gus Xia, Naoki Murata, Marco A. Martínez-Ramírez, Wei-Hsiang Liao, Yuki Mitsufuji, Simon Dixon</li>\n</ul>\n<p>See you in Jeju!</p>","id":"115c3260-ff2b-598c-8693-3bb28626870c"}]}}}
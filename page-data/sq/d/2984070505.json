{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/YIFgS8she58","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/EPSRC-additional-skills"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png","srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/2fd20/EPSRC.png 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/de391/EPSRC.png 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/d66e1/EPSRC.webp 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/e7160/EPSRC.webp 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/5f169/EPSRC.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"EPSRC additional skills funding summer 2025","author":"Prof Akram Alomainy (PI), Prof Simon Dixon (CI), Dr Iran Roman (CI) plus 8 others","date":null,"link":"https://www.ukri.org/councils/epsrc/"},"html":"","id":"bc1e4be0-d104-5b53-8c02-61f296532555"},{"fields":{"slug":"/projects/Fazekas-Leverhulme"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/14ae7694223d03edb7966d74ea33ce18/82c11/RAEng.png","srcSet":"/static/14ae7694223d03edb7966d74ea33ce18/2fd20/RAEng.png 125w,\n/static/14ae7694223d03edb7966d74ea33ce18/de391/RAEng.png 250w,\n/static/14ae7694223d03edb7966d74ea33ce18/82c11/RAEng.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/14ae7694223d03edb7966d74ea33ce18/d66e1/RAEng.webp 125w,\n/static/14ae7694223d03edb7966d74ea33ce18/e7160/RAEng.webp 250w,\n/static/14ae7694223d03edb7966d74ea33ce18/5f169/RAEng.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Knowledge-driven Deep Learning for Music Informatics","author":"Dr George Fazekas (PI)","date":null,"link":"https://raeng.org.uk/programmes-and-prizes/programmes/uk-grants-and-prizes/support-for-research/research-awardees/leverhulme-awardees/2025-2026/dr-george-fazekas/"},"html":"","id":"f47b222e-21cf-5c90-bff2-6181b436d1aa"},{"fields":{"slug":"/projects/Fazekas-Steinberg-2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png","srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/4b686/aimcdt.png 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/d2213/aimcdt.png 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png 615w","sizes":"(min-width: 615px) 615px, 100vw"},"sources":[{"srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/b2942/aimcdt.webp 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/46581/aimcdt.webp 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/fa942/aimcdt.webp 615w","type":"image/webp","sizes":"(min-width: 615px) 615px, 100vw"}]},"width":615,"height":615}}},"title":"AI-Powered Audio Loop Generation for Assistive Music Production","author":"Dr George Fazekas (PI)","date":null,"link":null},"html":"","id":"90759bed-47aa-57d3-8ce9-0dbe6a6b40ac"},{"fields":{"slug":"/projects/Ma-Google"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e3011e08b70d2397f6a5f53aca3eac47/b5658/Google.png","srcSet":"/static/e3011e08b70d2397f6a5f53aca3eac47/acb7c/Google.png 256w,\n/static/e3011e08b70d2397f6a5f53aca3eac47/ccc41/Google.png 512w,\n/static/e3011e08b70d2397f6a5f53aca3eac47/b5658/Google.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e3011e08b70d2397f6a5f53aca3eac47/22bfc/Google.webp 256w,\n/static/e3011e08b70d2397f6a5f53aca3eac47/d689f/Google.webp 512w,\n/static/e3011e08b70d2397f6a5f53aca3eac47/67ded/Google.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"title":"Large Language Models for Multimodal Music Understanding and Ethical Audio Generation","author":"Dr Emmanouil Benetos (PI), Yinghao Ma (PhD fellow)","date":null,"link":"https://research.google/programs-and-events/phd-fellowship/"},"html":"","id":"e4f5df67-9d4e-57da-bb16-3efcb2359e70"},{"fields":{"slug":"/projects/Pauwels-Sofilab"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6aad6623ce4e89038628dab00ebc48e0/c8f14/Sofilab.png","srcSet":"/static/6aad6623ce4e89038628dab00ebc48e0/abab1/Sofilab.png 185w,\n/static/6aad6623ce4e89038628dab00ebc48e0/96aa8/Sofilab.png 370w,\n/static/6aad6623ce4e89038628dab00ebc48e0/c8f14/Sofilab.png 739w","sizes":"(min-width: 739px) 739px, 100vw"},"sources":[{"srcSet":"/static/6aad6623ce4e89038628dab00ebc48e0/c5b6a/Sofilab.webp 185w,\n/static/6aad6623ce4e89038628dab00ebc48e0/7fd3d/Sofilab.webp 370w,\n/static/6aad6623ce4e89038628dab00ebc48e0/5bbdd/Sofilab.webp 739w","type":"image/webp","sizes":"(min-width: 739px) 739px, 100vw"}]},"width":739,"height":739}}},"title":"Smart musical corpus technologies","author":"Dr Johan Pauwels (PI)","date":null,"link":"https://sofilab.art/"},"html":"","id":"5b93e208-074a-5baf-a108-023df35a29f0"},{"fields":{"slug":"/projects/Reiss-Yamaha"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Yamaha Visiting Researcher Collaboration","author":"Prof Josh Reiss (PI)","date":null,"link":"https://intelligentsoundengineering.wordpress.com/"},"html":"","id":"145db7f6-77af-5f89-b477-6c4d87952ef4"}]},"news":{"nodes":[{"fields":{"slug":"/news/2026-01-06.Reimagining-music-videos-with-AI"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#081828","images":{"fallback":{"src":"/static/1d9355dcb7ccbd2e8acf2f603acb6fc0/74886/AutoMV.png","srcSet":"/static/1d9355dcb7ccbd2e8acf2f603acb6fc0/67a35/AutoMV.png 224w,\n/static/1d9355dcb7ccbd2e8acf2f603acb6fc0/c0546/AutoMV.png 448w,\n/static/1d9355dcb7ccbd2e8acf2f603acb6fc0/74886/AutoMV.png 895w","sizes":"(min-width: 895px) 895px, 100vw"},"sources":[{"srcSet":"/static/1d9355dcb7ccbd2e8acf2f603acb6fc0/f42a0/AutoMV.webp 224w,\n/static/1d9355dcb7ccbd2e8acf2f603acb6fc0/c48dd/AutoMV.webp 448w,\n/static/1d9355dcb7ccbd2e8acf2f603acb6fc0/31b61/AutoMV.webp 895w","type":"image/webp","sizes":"(min-width: 895px) 895px, 100vw"}]},"width":895,"height":895}}},"title":"Reimagining music videos with AI: C4DM research breaks new ground","author":"admin","date":"Tue 06 Jan 2026"},"html":"<p><a href=\"https://nicolaus625.github.io/\">Yinghao Ma</a>, a PhD candidate in the Centre for Digital Music at Queen Mary University of London, has helped develop AutoMV, the first open-source AI system capable of generating complete music videos directly from full-length songs.</p>\n<p>Music-to-video generation remains a major challenge for generative AI. While recent video models can produce visually impressive short clips, they often struggle with long-form storytelling, musical alignment, and character consistency. AutoMV addresses these limitations by introducing a multi-agent AI system designed specifically for full-length music video production.</p>\n<p>Developed through a collaboration between Queen Mary researchers and partners at Beijing University of Posts and Telecommunications, Nanjing University, Hong Kong University of Science and Technology, and the University of Manchester, AutoMV brings together expertise in music information retrieval, multimodal AI, and creative computing. The work was led by <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/ebenetos/\">Dr Emmanouil Benetos</a>, with contributions from Yinghao Ma as well as <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/coh/\">Dr. Changjae Oh</a> and <a href=\"https://noone65536.github.io/\">Chaoran Zhu</a> from the Centre for Intelligent Sensing.</p>\n<p>AutoMV works like a virtual film production team. First, it analyses a song’s musical structure, beats, and time-aligned lyrics. Then, a set of specialised AI agents—taking on roles such as screenwriter, director, and editor—collaborate to plan scenes, maintain character identity, and generate images and video clips. A final quality-control “verifier” agent checks for coherence and consistency, regenerating content where needed.</p>\n<p>This approach allows AutoMV to produce music videos that follow a song from beginning to end, maintaining narrative flow and visual identity throughout. Human expert evaluations show that AutoMV significantly outperforms existing commercial tools, narrowing the gap between AI-generated videos and professionally produced music videos.</p>\n<p>By lowering the cost of music video production from tens of thousands of pounds to roughly the cost of an API call, AutoMV has the potential to empower independent musicians, educators, and creators who previously lacked access to professional video production. As an open-source project, it also supports transparent, reproducible research and encourages community collaboration.</p>\n<p>The team is actively inviting researchers and students to contribute to the codebase, extend the benchmark, and explore future directions for long-form, multimodal AI systems.</p>\n<ul>\n<li>Code: <a href=\"https://github.com/multimodal-art-projection/AutoMV\">https://github.com/multimodal-art-projection/AutoMV</a></li>\n<li>Paper: <a href=\"https://arxiv.org/abs/2512.12196\">https://arxiv.org/abs/2512.12196</a></li>\n<li>Project website: <a href=\"https://m-a-p.ai/AutoMV/\">https://m-a-p.ai/AutoMV/</a></li>\n</ul>","id":"47d03e8e-fbc9-51ae-a11d-4169551a78c4"},{"fields":{"slug":"/news/2025-12-02.DMRN2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#e8e8f8","images":{"fallback":{"src":"/static/6a96619ca934bdc4aee6bdedb3d8a3e9/baaed/DMRN-logo.jpg","srcSet":"/static/6a96619ca934bdc4aee6bdedb3d8a3e9/dd515/DMRN-logo.jpg 200w,\n/static/6a96619ca934bdc4aee6bdedb3d8a3e9/47930/DMRN-logo.jpg 400w,\n/static/6a96619ca934bdc4aee6bdedb3d8a3e9/baaed/DMRN-logo.jpg 800w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[{"srcSet":"/static/6a96619ca934bdc4aee6bdedb3d8a3e9/2e34e/DMRN-logo.webp 200w,\n/static/6a96619ca934bdc4aee6bdedb3d8a3e9/416c3/DMRN-logo.webp 400w,\n/static/6a96619ca934bdc4aee6bdedb3d8a3e9/c1587/DMRN-logo.webp 800w","type":"image/webp","sizes":"(min-width: 800px) 800px, 100vw"}]},"width":800,"height":800}}},"title":"DMRN+20: Digital Music Research Network One-day Workshop 2025","author":"Admin","date":"Tue 02 Dec 2025"},"html":"<p><b>DMRN+20: Digital Music Research Network 1-Day Workshop 2025</b></p>\n<p><b>King’s College London (in person only)</b><br>\n<b>Tue 16 December 2025</b><br>\n<b><a href=\"https://www.qmul.ac.uk/dmrn/dmrn20/\"><a href=\"https://www.qmul.ac.uk/dmrn/dmrn20/\">https://www.qmul.ac.uk/dmrn/dmrn20/</a></a></b></p>\n<p>The 2020 Digital Music Research Network workshop (DMRN+20) marks the 20th edition of DMRN. Moreover, this annual gathering has a pre-history under other names, making it an institution that’s more like 25 years old! This year, we celebrate that history and the ever-growing presence of music computing in and around London. As well as the usual offering (top-notch research and friendly social atmosphere), we’ll also consider the past and future of DMRN, and the possible roles for a regionally organised network of this kind in the few (perhaps even 25) years. This event is hosted at KCL in collaboration with – but outside of – C4DM for the first time in many years, and takes the theme “Collaboration, Coordination, and Community”.</p>\n<p><b>Theme: “Collaboration, Coordination, and Community”.</b></p>\n<p>This years’ theme is “Collaboration, Coordination, and Community”. You may like to include a nod to this theme in your submission (this is optional!) and/or in your chats with others at the event. For example, you might like to discuss research communities at both local (e.g., DMRN) and global (e.g., ISMIR) scales. Likewise, you might give thought to the wider music scholarship communities with which we sometime have less interaction than we ought (e.g., ICMPC). And what about the much wider communities of musicians (professional and amateur) in London and beyond? What does successful collaborative, coordinated, and community-oriented work looks like, and what might DMRN’s role be?</p>\n<p>For further information, visit: <a href=\"https://www.qmul.ac.uk/dmrn/dmrn20/\">https://www.qmul.ac.uk/dmrn/dmrn20/</a></p>\n<p>Enquiries: Alvaro Bort (<a href=\"mailto:a.bort@qmul.ac.uk\">a.bort@qmul.ac.uk</a>)</p>","id":"e799d1ed-b2b6-5065-868e-edc40ed3782a"},{"fields":{"slug":"/news/2025-11-20.C4DM-Seminar_Eloi_Moliner"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Eloi Moliner","author":null,"date":"Thu 20 Nov 2025"},"html":"<h3>C4DM Seminar: Eloi Moliner: Diffusion Models for Audio Effects: From Blind Estimation to Automatic Mixing</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Eloi Moliner</p>\n<p><strong>Date/time:</strong>  Thursday, 20th Nov 2025, 11 am</p>\n<p><strong>Location:</strong> GC203, Graduate Centre, Mile End Campus, Queen Mary University of London</p>\n<p><a href=\"https://teams.microsoft.com/l/meetup-join/19%3ameeting_ZDcxODQ2ZTctN2NkYy00MDExLWEyOWMtNzRkMjlmMGUxMThh%40thread.v2/0?context=%7b%22Tid%22%3a%22569df091-b013-40e3-86ee-bd9cb9e25814%22%2c%22Oid%22%3a%22cad9cb9a-abc2-4aac-ab51-682e8a852753%22%7d\">Teams meeting link</a></p>\n<h2><b>Title</b>: Diffusion Models for Audio Effects: From Blind Estimation to Automatic Mixing</h2>\n<p><b>Abstract</b>:\nThis talk presents a series of recent works exploring the use of diffusion models in the analysis and generation of audio effects. In the first part, I will focus on blind and unsupervised effect estimation, where diffusion models are employed as powerful data-driven priors for recovering clean or unprocessed signals from their altered counterparts, while simultaneously estimating the parameters of a model of the underlying audio effect. In the second part, I will discuss a recent work on automatic music mixing, introducing MEGAMI (Multitrack Embedding Generative Auto MIxing)—a generative framework that models the distribution of professional mixes directly in a multitrack effect embedding space.</p>\n<p><b>Bio</b>:\nEloi Moliner received his Ph.D. degree from the Acoustics Lab of Aalto University, Espoo, Finland, in 2025. He previuosly obtained his M.Sc. degree in Telecommunications Engineering in 2021 and his B.Sc. degree in Telecommunications Technologies and Services Engineering in 2018, both from the Polytechnic University of Catalonia, Spain. He received the Best Student Paper Awards at IEEE ICASSP 2023, IWAENC 2024, and AES AIMLA 2025. His research interests include generative models for audio, audio restoration and enhancement, and audio signal processing.</p>","id":"e7b942ad-e4fe-5988-87fe-7bad707e1b48"},{"fields":{"slug":"/news/2025-11-17.C4DM-at_NeurIPS_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/a20d19fb2ed10997b9bc578028722b0a/baaed/NeurIPS-2025.jpg","srcSet":"/static/a20d19fb2ed10997b9bc578028722b0a/dd515/NeurIPS-2025.jpg 200w,\n/static/a20d19fb2ed10997b9bc578028722b0a/47930/NeurIPS-2025.jpg 400w,\n/static/a20d19fb2ed10997b9bc578028722b0a/baaed/NeurIPS-2025.jpg 800w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[{"srcSet":"/static/a20d19fb2ed10997b9bc578028722b0a/2e34e/NeurIPS-2025.webp 200w,\n/static/a20d19fb2ed10997b9bc578028722b0a/416c3/NeurIPS-2025.webp 400w,\n/static/a20d19fb2ed10997b9bc578028722b0a/c1587/NeurIPS-2025.webp 800w","type":"image/webp","sizes":"(min-width: 800px) 800px, 100vw"}]},"width":800,"height":800}}},"title":"C4DM at NeurIPS 2025","author":"Emmanouil Benetos","date":"Mon 17 Nov 2025"},"html":"<p>On 2-7 December, C4DM researchers will participate at the <a href=\"https://neurips.cc/\">39th Annual Conference on Neural Information Processing Systems (NeurIPS 2025)</a>, taking place in San Diego, USA. NeurIPS is a prestigious annual academic conference and non-profit foundation that fosters the exchange of research in artificial intelligence (AI), machine learning (ML), and computational neuroscience.</p>\n<p>The following papers from C4DM members will be presented at the <b>Datasets and Benchmarks track</b> of NeurIPS 2025:</p>\n<ul>\n<li>\n<p><a href=\"https://openreview.net/forum?id=SSF4qgsNYE\">OmniBench: Towards The Future of Universal Omni-Language Models</a> by Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, King Zhu, Hangyu Guo, Yiming Liang, Jiaheng Liu, Zekun Moore Wang, Jian Yang, Siwei Wu, Xingwei Qu, Jinjie Shi, Xinyue Zhang, Zhenzhu Yang, Yidan WEN, Yanghai Wang, Shihao Li, Zhaoxiang Zhang, Ruibo Liu, Emmanouil Benetos, Wenhao Huang, Chenghua Lin</p>\n</li>\n<li>\n<p><a href=\"https://openreview.net/forum?id=fgmrBJemlQ\">MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix</a> by Ziyang Ma, Yinghao Ma, Yanqiao Zhu, Chen Yang, Yi-Wen Chao, Ruiyang Xu, Wenxi Chen, Yuanzhe Chen, Zhuo Chen, Jian Cong, Kai Li, Keliang Li, Siyou Li, Xinfeng Li, Xiquan Li, Zheng Lian, Yuzhe Liang, Minghao Liu, Zhikang Niu, tianrui wang, Yuping Wang, Yuxuan Wang, Yihao Wu, Guanrou Yang, Jianwei Yu, Ruibin Yuan, Zhisheng Zheng, Ziya Zhou, Haina Zhu, Wei Xue, Emmanouil Benetos, Kai Yu, EngSiong Chng, Xie Chen</p>\n</li>\n</ul>\n<p>The following paper will be presented at the <b>Creative AI track</b> of NeurIPS 2025:</p>\n<ul>\n<li><a href=\"https://openreview.net/forum?id=3yeBer3J5z\">The Ghost in the Keys: A Disklavier Demo for Human-AI Musical Co-Creativity</a> by Louis Bradshaw, Alexander Spangher, Stella Biderman, Simon Colton</li>\n</ul>\n<p>The following papers will be presented at the <b><a href=\"https://aiformusicworkshop.github.io/\">NeurIPS 2025 Workshop on AI for Music</a></b>:</p>\n<ul>\n<li>\n<p><a href=\"https://openreview.net/pdf?id=yL8BrlEqHQ\">The Ghost in the Keys: A Disklavier Demo for Human-AI Musical Co-Creativity</a> by Louis Bradshaw, Alexander Spangher, Stella Biderman, Simon Colton</p>\n</li>\n<li>\n<p><a href=\"https://openreview.net/pdf?id=rXUKO0ysUy\">Perceptually Aligning Representations of Music via Noise-Augmented Autoencoders</a> by Mathias Rose Bjare, Giorgia Cantisani, Marco Pasini, Stefan Lattner, Gerhard Widmer</p>\n</li>\n<li>\n<p><a href=\"https://openreview.net/pdf?id=pbCcvZdHyG\">Evaluating Multimodal Large Language Models on Core Music Perception Tasks</a> by Brandon James Carone, Iran R Roman, Pablo Ripollés</p>\n</li>\n<li>\n<p><a href=\"https://openreview.net/pdf?id=NG187AZ71W\">Advancing Multi-Instrument Music Transcription: Results from the 2025 AMT Challenge</a> by Ojas Chaturvedi, Kayshav Bhardwaj, Tanay Gondil, Benjamin Shiue-Hal Chou, Kristen Yeon-Ji Yun, Yung-Hsiang Lu, Yujia Yan, Sungkyun Chang</p>\n</li>\n</ul>\n<p>Finally, the following paper will be presented at the <a href=\"https://differentiable-systems.github.io/workshop-eurips-2025/#schedule\">Differentiable Systems and Scientific Machine Learning workshop</a> of EurIPS:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2511.14390\">Accelerating Automatic Differentiation of Direct Form Digital Filters</a> by Chin-Yun Yu, George Fazekas</li>\n</ul>\n<p>See you all at NeurIPS!</p>","id":"cadafe6e-e591-5ae2-8b62-52de8a24c21a"},{"fields":{"slug":"/news/2025-11-17.PhD-call-2026"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/f64b6/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/d59af/c4dm.png 431w,\n/static/c138fbce66e709a3f503405435de2f2c/325b2/c4dm.png 862w,\n/static/c138fbce66e709a3f503405435de2f2c/f64b6/c4dm.png 1723w","sizes":"(min-width: 1723px) 1723px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/4b79d/c4dm.webp 431w,\n/static/c138fbce66e709a3f503405435de2f2c/562bc/c4dm.webp 862w,\n/static/c138fbce66e709a3f503405435de2f2c/c4c1b/c4dm.webp 1723w","type":"image/webp","sizes":"(min-width: 1723px) 1723px, 100vw"}]},"width":1723,"height":1723}}},"title":"PhD Studentships at the Centre for Digital Music - Autumn 2026 start","author":"Admin","date":"Mon 17 Nov 2025"},"html":"<p>The Centre for Digital Music at Queen Mary University of London is inviting applications for PhD study for Autumn 2026 start across various funding schemes. Below are suggested PhD topics offered by academics; interested applicants can apply for a PhD under one of those topics, or can propose their own topic. In all cases, prospective applicants are strongly encouraged to contact academics at C4DM to informally discuss prospective research topics.</p>\n<p>Opportunities include internally and externally funded positions for PhD projects to start in Autumn 2026. It is also possible to apply as a self-funded student or with funding from another source. Studentship opportunities include:</p>\n<ul>\n<li>\n<p><a href=\"https://www.qmul.ac.uk/eecs/phd/phd-studentships/neural-dynamics-of-perceptually-aligned-artificial-intelligence/#d.en.1556147\">One UK home PhD studentship</a> (Autumn 2026 start, UK home applicants, deadline 9 January 2026)</p>\n</li>\n<li>\n<p><a href=\"https://www.qmul.ac.uk/scholarships/items/uk-bame-phd-studentships-1.html\">S&#x26;E Doctoral Research Studentships for Underrepresented Groups</a> (UK home applicants, Autumn 2026 start, 3 positions funded across the Faculty of Science &#x26; Engineering, deadline 28 January 2026 5pm)</p>\n</li>\n<li>\n<p><a href=\"https://www.qmul.ac.uk/eecs/phd/phd-studentships/csc-phd-studentships-in-electronic-engineering-and-computer-science/\">CSC PhD Studentships in Electronic Engineering and Computer Science</a> (Autumn 2026 start, Chinese applicants, up to 5 nominations allocated for the Centre for Digital Music, deadline 28 January 2026 5pm)</p>\n</li>\n<li>\n<p>National (country of origin) PhD Study Abroad Schemes (Autumn 2026 start, any nationality)</p>\n</li>\n</ul>\n<p>Each funding scheme has a dedicated application process and requirements. Detailed information and application links can be found on the respective funding scheme pages, following the above links.</p>\n<hr>\n<p><strong>Understanding Neural Audio – and Building it Better</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cfcs/people/msandler/\">Mark Sandler</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, International PhD Funding Scheme</p>\n<p>As Deep Learning models for Audio and Music have got ever more complex, so our ability to fully understand them has diminished. In this research, we explore how weight matrices evolve during training and how activations evolve during both training and inference. One of the key tools for this is Linear Algebra, especially Matrix and Tensor decomposition techniques. But Matrix and Tensor decomposition not only leads us to new insights, it also leads to new ways to build Deep Learning models. In particular, we have developed a new approach called Sum of Rank One (SoRO) layers, where a fully connected layer is replaced with a sum of small, rank-1 matrices which can be very efficiently implemented. Preliminary work has shown that these layers not only learn quicker, they also learn better and potentially with less data.</p>\n<p>Students joining this project – which is in collaboration with our School of Mathematical Sciences – will get the opportunity either to study training in existing Neural Audio models (e.g. for sound synthesis or for sound source separation) or to explore novel models that incorporate SoRO layers, or both.</p>\n<p>Another possible aspect to this work is to develop ways to implement convolutional layers in the SoRO formulation and explore the changing learning dynamics of CNNS. Further potential avenues for study are Attention Heads in Transformers and the autoencoders of Diffusion Models.</p>\n<p>Applicants should develop their own particular interest within this framework and explain it in their Research Proposal.</p>\n<hr>\n<p><strong>Differentiable Physics Neural Modelling of Strings, Membranes and Plates</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cfcs/people/msandler/\">Mark Sandler</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, International PhD Funding Scheme</p>\n<p>This roject will investigate a new architecture for data-driven sound generation that builds on recent advances in Neural Audio sound generation using Differential Digital Signal Processing (DDSP) and a new differentiable approach to more conventional physical modelling (Diaz Fernandez et al., 2024). This is what we call Differentiable Physics Neural Modelling (DPNM). This delivers efficient, interpretable sound generation by combining the best of physical modelling and data-driven approaches. It falls into the broader category of Model Based Deep Learning (Shlezinger et al., 2021).</p>\n<p>In recent years, significant research attention has been paid to Physics-informed Neural Networks (PINNs) (Raissi et al., 2019) in a variety of fields and it is natural to explore their use in physical modelling of sounding objects, such as plucked strings, wooden panels, membranes etc. However, extensive research at QMUL team has shown that the levels of accuracy that are sufficient in conventional physics are not accurate enough for studio-quality sound generation over adequately long periods. Preliminary work suggests that DPNM can overcome these short-comings, making this an exciting topic for PhD research.</p>\n<p>The student should have a background that includes understanding of Deep Learning and ODEs/PDEs.</p>\n<p>Diaz Fernandez, R., De La Vega, M. C., &#x26; Sandler, M. (2024). Towards Efficient Modelling of String Dynamics: A Comparison of State Space and Koopman based Deep Learning Methods. International Conference on Digital Audio Effects, DAFx.</p>\n<p>Shlezinger, N., Whang, J., Eldar, Y. C., &#x26; Dimakis, A. G. (2021). Model-Based Deep Learning: Key Approaches and Design Guidelines. 2021 IEEE Data Science and Learning Workshop (DSLW), 1-6. 10.1109/DSLW51110.2021.9523403</p>\n<hr>\n<p><strong>AI Models of Music Understanding</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/sdixon/\">Simon Dixon</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, CSC PhD Studentships, International PhD Funding Scheme</p>\n<p>Music information retrieval (MIR) applies computing and engineering technologies to musical data to satisfy users' information needs. This topic involves the application of artificial intelligence technologies to the processing of music, either in audio or symbolic (score, MIDI) form. The application could be e.g. for software to enhance the listening experience, for music education, for musical practice or for the scientific study of music. Examples of topics of particular interest are automatic transcription of multi-instrumental music, providing feedback to music learners, incorporation of musical knowledge into data-driven deep learning approaches, and tracing the transmission of musical styles, ideas or influences across time or locations.</p>\n<p>It is intentional that this topic description is very general, but it is expected that applicants choose your own specific project within this broad area of research, according to your interests and experience. The research proposal should define the scope of the project, the relationship to the state of the art, the data and methods that you plan to use, and the expected outputs and means of evaluation.</p>\n<hr>\n<p><strong>Bridging Musical Intelligence and Machine Learning: Integrating Domain Knowledge into Music and Audio Representation Learning</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/gfazekas\">George Fazekas</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, CSC PhD Studentships, International PhD Funding Scheme</p>\n<p>Audio and music representation learning seeks to transform raw data into latent representations for downstream tasks such as classification, recommendation, retrieval and generation. While recent advances in deep learning, especially contrastive, self-supervised and diffusion-based approaches have achieved impressive results, most remain purely data-driven and neglect domain-specific musical structures like rhythm, melody, harmony, metrical hierarchy or genre-style traits.</p>\n<p>This PhD project will explore ways to embed theoretical and structural knowledge into modern representation learning pipelines to enhance interpretability, controllability and performance. For example, incorporating symbolic or other structured representations, inductive biases, well-known principles exploited in classic DSP algorithms, or ontological constraints, the research aims to bridge the gap between data-driven models and the structured understanding of music and audio.</p>\n<p>Potential directions include hybrid models that combine deep audio and symbolic embeddings, graph-based or relational learning of musical structure, and explainable methods for music analysis, production or generation. The project will also engage with principles of Ethical and Responsible AI: reducing data bias, improving transparency and supporting fair attribution of authorship.</p>\n<p>Examples of relevant works include but not limited to:</p>\n<p>Guinot, Quinton, Fazekas: “Semi-Supervised Contrastive Learning of Musical Representations”, ISMIR-2024</p>\n<p>Yu, Fazekas: “Singing voice synthesis using differentiable LPC and glottal-flow-inspired wavetables”, ISMIR-2023</p>\n<p>Agarwal, Wang, Richard: F-StrIPE: Fast Structure-Informed Positional Encoding for Symbolic Music Generation, ICASSP-2025</p>\n<hr>\n<p><strong>Assistive technologies for music making, production or listening using Generative AI</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/gfazekas\">George Fazekas</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, CSC PhD Studentships, International PhD Funding Scheme</p>\n<p>Applications are invited for a PhD exploring how generative AI can power new forms of assistive technology that support music creation, performance, production, and listening. As AI systems become increasingly capable of modelling human emotion, intention, and creative context, they open opportunities to help people engage with music in more intuitive, expressive and enriched ways.</p>\n<p>This research will investigate 1) how AI can support, for example, musicians in composing or performing through intelligent accompaniment, adaptive sound design, or personalised production tools, or alternatively 2) how listeners can benefit from systems that shape or generate music in response to emotional states, physiological data, or broader wellbeing needs. Drawing inspiration from recent work on music-based self-regulation, the project may explore how generative models might interpret multimodal signals such as facial expression, movement or physiological data, and respond with music that supports focus, enhances creativity, comfort or helps stress recovery.</p>\n<p>Methodologically, the PhD may incorporate advances in deep generative audio models, foundational music models, multimodal learning, affective computing, reinforcement learning and interactive human-AI co creation systems. The expected outcomes include novel AI-driven tools that make music creation more accessible, enhance creative workflows, or offer evidence-based benefits for listeners’ emotional and mental wellbeing, contributing both new technologies and new understanding of human–music interaction in the era of generative AI.</p>\n<p>Relevant references include but are not limited to:</p>\n<p>Herremans et al.: “A Functional Taxonomy of Music Generation Systems” ACM Computing Surveys, 2017</p>\n<p>Liyanarachchi et al.: “A Survey on Multimodal Music Emotion Recognition” arXiv, 2025</p>\n<p>Strano et al., “STAGE: Stemmed Accompaniment Generation through Prefix-Based Conditioning”, ISMIR 2025</p>\n<hr>\n<p><strong>Automated machine learning for music understanding</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/ebenetos/\">Emmanouil Benetos</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, CSC PhD Studentships, International PhD Funding Scheme</p>\n<p>The field of music information retrieval (MIR) has been growing for more than 20 years, with re-cent advances in deep learning having revolutionised the way machines can make sense of music data. At the same time, research in the field is still constrained by laborious tasks involving data preparation, feature extraction, model selection, architecture optimisation, hyperparameter optimisa-tion, and transfer learning, to name but a few. Some of the model and experimental design choices made by MIR researchers also reflect their own biases.</p>\n<p>Inspired by recent developments in machine learning and automation, this PhD project will investi-gate and develop automated machine learning methods which can be applied at any stage in the MIR pipeline as to build music understanding models ready for deployment across a wide range of tasks. This project will also compare the automated decisions made on every step in the MIR pipe-line, as compared with manual model design choices made by researchers. The successful candidate will investigate, propose and develop novel deep learning methods for automating music under-standing, resulting in models that can accelerate MIR research and contribute to the democratisation of AI.</p>\n<hr>\n<p><strong>Sonification techniques for understanding hidden processes of LLMs</strong></p>\n<p>Supervisors: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/axambosedo/\">Anna Xambó</a> and <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/csaitis/\">Charalampos Saitis</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, CSC PhD Studentships, International PhD Funding Scheme</p>\n<p>Large language models (LLMs) are a type of artificial intelligence program that can recognise and generate text, which are trained on huge sets of data with a complex network of hidden processes. This PhD topic explores sonification techniques of LLMs for a better understanding of the way they process the information. Can we treat LLM engines such as ChatGPT as a musical instrument and listen to its internal processes? Can sonification techniques help us to hear and see how the information is processed? Compared to vinyl records or tape recordings, what is the acoustic signature, and what are the artefacts that are distinctive of this new medium? This work will contribute to addressing an important challenge in AI: making the inner workings and hidden knowledge of models more interpretable for people.</p>\n<p>Keywords: sonification, large language models (LLMs), explainable AI</p>\n<hr>\n<p><strong>Audio-visual sensing for machine intelligence</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/lwang/\">Lin Wang</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, International PhD Funding Scheme</p>\n<p>The project aims to develop novel audio-visual signal processing and machine learning algorithms that help improve machine intelligence and autonomy in an unknown environment, and to understand human behaviours interacting with robots. The project will investigate the application of AI algorithms for audio-visual scene analysis in real-life environments. One example is to employ multimodal sensors e.g. microphones and cameras, for analysing various sources and events present in the acoustic environment. Tasks to be considered include audio-visual source separation, localization/tracking, audio-visual event detection/recognition, audio-visual scene understanding.</p>\n<hr>\n<p><strong>Interpretable AI for Sound Event Detection and Classification</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/lwang/\">Lin Wang</a> and <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/ebenetos/\">Emmanouil Benetos</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, CSC PhD Studentships, International PhD Funding Scheme</p>\n<p>Deep-learning models have revolutionized state-of-the-art technologies for environmental sound recognition motivated by their applications in healthcare, smart homes, or urban planning. However, most of the systems used for these applications are based on black boxes and, therefore, cannot be inspected, so the rationale behind their decisions is obscure. Despite recent advances, there is still a lack of research in interpretable machine learning in the audio domain. Applicants are invited to develop ideas to reduce this gap by proposing interpretable deep-learning models for automatic sound event detection and classification in real-life environments.</p>\n<hr>\n<p><strong>Using machine learning to enhance simulation of sound phenomena</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/jreiss/\">Josh Reiss</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, International PhD Funding Scheme</p>\n<p>Physical models of sound generating phenomena are widely used in digital musical instruments, noise and vibration modelling, and sound effects. They can be incredibly high quality, but they also often have a large number of free parameters that may not be specified just from an understanding of the phenomenon.</p>\n<p>Machine learning from sample libraries could be the key to improving the physical models and speeding up the design process. Not only can optimisation approaches be used to select parameter values such that the output of the model matches samples, the accuracy of such an approach will give us insight into the limitations of a model. It also provides the opportunity to explore the overall performance of different physical modelling approaches, and to find out whether a model can be generalised to cover a large number of sounds, with a relatively small number of exposed parameters.</p>\n<p>This work will explore such approaches. It will build on recent high impact research from the team in relation to optimisation of sound effect synthesis models. Existing physical models will be used, with parameter optimisation based on gradient descent. Performance will be compared against recent neural synthesis approaches, that often provide high quality synthesis but lack a physical basis. It will also seek to measure the extent to which entire sample libraries could be replaced by a small number of physical models with parameters set to match the samples in the library.</p>\n<p>The student will have the opportunity to work closely with research engineers from the start-up company Nemisindo, though will also have the freedom to take the work in promising new directions. Publishing research in premier venues will be encouraged.</p>\n<p>The project can be tailored to the skills of the researcher, and has the potential for high impact.</p>\n<hr>\n<p><strong>Intelligent audio production for the hearing impaired</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/jreiss/\">Josh Reiss</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, International PhD Funding Scheme</p>\n<p>This project will explore new approaches to audio production to address hearing loss, a growing concern with an aging population. The overall goal is to investigate, implement and validate original strategies for mixing audio content such that it can be delivered with improved perceptual quality for hearing impaired people.</p>\n<p>Music content is typically recorded as multitracks, with different sound sources on different tracks. Similarly, soundtracks for television and radio content typically have dialogue, sound effects and music mixed together with normal-hearing listeners in mind. But a hearing impairment may result in this final mix sounding muddy and cluttered. The research team here have made strong advances on simulating hearing loss, understanding how to mix for hearing loss, and attempting to automatically deliver enhanced mixes for hearing loss. But these initial steps identified many unresolved issues and challenges. Why do hearing loss simulators differ from real world hearing loss, and how can this be corrected? How should hearing loss simulators be evaluated and how should they be used in the music production process? What is the best approach to mix audio content to address hearing loss? These questions will be investigated in this project.</p>\n<p>The project can be tailored to the skills of the researcher, and has the potential for high impact.</p>\n<hr>\n<p><strong>Neural Dynamics of Perceptually Aligned Artificial Intelligence</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/iroman/\">Iran Roman</a></p>\n<p>Eligible funding schemes: Fully-funded UK Home studentship (fees and London stipend)</p>\n<p>The brain easily makes sense of complex perceptual tasks, while sophisticated AI systems still struggle. This PhD project aims to bridge computational neuroscience, machine learning, and multimodal perception to build AI that perceives the world more like living organisms. Current AI often relies on statistical shortcuts, not genuine understanding. This project will draw on Neural Resonance Theory to replicate perceptual alignment, where biological networks resonate and synchronize to embody perceptual structure. The project will investigate how principles of oscillation, resonance, and attunement can be embedded in neural networks. The successful candidate will develop new theories and algorithms. Potential applications include multimodal self-supervised learning, neurodynamical models, and embodied, interactive AI systems that can understand and anticipate actions in real-time.</p>\n<hr>\n<p><strong>Deep neural modelling of music and speech perception</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/chcc/people/mpearce\">Marcus Pearce</a> and <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/iroman/\">Iran Roman</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, CSC PhD Studentships, International PhD Funding Scheme</p>\n<p>Evidence suggests speech and music perception depend on cognitive models acquired through implicit statistical learning. While deep neural networks (DNNs) use analogous mechanisms for generating music and language, it is unknown if they truly simulate human perception. This project will develop novel neural network architectures to simulate speech and music perception. The project will use existing probabilistic methods both as a benchmark and as a tool for interpreting the abstract representations learned by the DNNs. Models will be tested through iterative comparison with behavioural and neural data from human psychological experiments. The successful candidate will also investigate cross-cultural comparisons and the psychological relationships between speech and music. The project's outcome will be a computational understanding of the psychology of human cultural learning in auditory perception.</p>\n<hr>\n<p><strong>Neural Resonance and the Perception of Timbre: A Neurodynamic Modeling Approach</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/csaitis/\">Charalampos Saitis</a> and <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/iroman/\">Iran Roman</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, International PhD Funding Schemes</p>\n<p>The perception of timbre is central to auditory recognition, yet its neurodynamic basis remains underexplored compared to pitch or rhythm. Neural Resonance Theory (NRT) posits that musical experience arises from brain-body dynamics entraining to structured sound, resulting in stable, pattern-forming oscillatory activity. Recent models of cochlear and brainstem activity using nonlinear resonator networks provide biologically grounded support for this view. This project investigates how networks of nonlinear oscillators and recurrent neural networks (RNNs) respond to stimuli varying only in timbre (e.g., sinusoids, violin, voice), aiming to identify differential resonance patterns attributable to spectral characteristics alone. In parallel, RNNs trained on pitch tasks will be analyzed to determine whether their emergent internal dynamics replicate resonance phenomena. Finally, EEG recordings from human listeners will be used to detect entrainment signatures matching model predictions. This interdisciplinary approach offers a novel application of NRT to timbre perception, bridging biologically inspired modeling, machine learning, and empirical neuroscience.</p>","id":"f318f0d9-d5ff-5115-93cf-a18a2e6dd83f"},{"fields":{"slug":"/news/2025-11-05.BMVA-Workshop"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#a8a898","images":{"fallback":{"src":"/static/f50b3f2ab8deecae63e8cdd69d401e0b/2b34e/BMVA2025.png","srcSet":"/static/f50b3f2ab8deecae63e8cdd69d401e0b/bcbe6/BMVA2025.png 245w,\n/static/f50b3f2ab8deecae63e8cdd69d401e0b/78a05/BMVA2025.png 490w,\n/static/f50b3f2ab8deecae63e8cdd69d401e0b/2b34e/BMVA2025.png 979w","sizes":"(min-width: 979px) 979px, 100vw"},"sources":[{"srcSet":"/static/f50b3f2ab8deecae63e8cdd69d401e0b/c4230/BMVA2025.webp 245w,\n/static/f50b3f2ab8deecae63e8cdd69d401e0b/a5fb7/BMVA2025.webp 490w,\n/static/f50b3f2ab8deecae63e8cdd69d401e0b/758a3/BMVA2025.webp 979w","type":"image/webp","sizes":"(min-width: 979px) 979px, 100vw"}]},"width":979,"height":979}}},"title":"The British Machine Vision Association Workshop on Multimodal LLMs","author":"admin","date":"Wed 05 Nov 2025"},"html":"<p>The <a href=\"https://www.bmva.org/meetings/25-11-05-MultimodalLargeModels.html\">British Machine Vision Association and Society for Pattern Recognition (BMVA) Workshop on Multimodal Large Models Bridging Vision, Language, and Beyond</a> was held at British Computer Society (BCS), 25 Copthall Avenue, London EC2R 7BP on November 5th, 2025.</p>\n<p>Among the selected oral presentations, C4DM PhD student <strong>Yinghao Ma</strong> (supervised by Prof. Emmanouil Benetos) presented his latest work <em>MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix</em>. MMAR is a newly released benchmark that spans 1,000 real-world audio reasoning tasks, covering speech, sound events, music, and mixed-modality scenarios. It is one of the first benchmarks to explicitly evaluate multi-step reasoning abilities in audio-language and omni-modal large models, with tasks ranging from low-level signal perception to high-level cultural understanding.</p>\n<p>The talk was part of the “Domain Applications and Human-Centric Modalities” session, alongside research on sign language translation, visual illusions, and 3D-aware facial editing. MMAR attracted interest from both academia and industry attendees, especially as multimodal reasoning becomes a key focus in the next wave of AI foundation models.</p>\n<p>The BMVA workshop featured keynote speakers from Google DeepMind, UCL, and the University of Surrey, and brought together researchers advancing the frontier of multimodal intelligence across vision, language, audio, and embodied learning.</p>\n<p>MMAR is open-source and available on arXiv: <strong>arXiv:2505.13032</strong>. The video recording of presentation is available at <a href=\"https://www.bmva.org/meetings/25-11-05-MultimodalLargeModels.html\">here</a></p>","id":"8f6388ab-107d-5585-8923-6db01cfbded3"}]}}}
{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/YIFgS8she58","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/Sandler-Artificial-Neuroscience"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png","srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/2fd20/EPSRC.png 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/de391/EPSRC.png 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/d66e1/EPSRC.webp 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/e7160/EPSRC.webp 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/5f169/EPSRC.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Artificial Neuroscience: metrology and engineering for Deep Learning using Linear Algebra","author":"Prof Mark Sandler (PI), Boris Khoruzhenko (CI)","date":null,"link":"https://www.ukri.org/councils/epsrc/"},"html":"","id":"c1314000-fe3b-5175-88a2-f239d861ddad"},{"fields":{"slug":"/projects/Barthet-Netz"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Netz: A Novel XR Musical Instrument","author":"Dr Mathieu Barthet (PI)","date":null,"link":"https://iuk-business-connect.org.uk/programme/icure/"},"html":"","id":"0040b455-65cf-5ea1-a7ce-38ab70570dae"},{"fields":{"slug":"/projects/Benetos-Algorivm"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Project Maestro - AI Musical Analysis Platform","author":"Dr Emmanouil Benetos (PI), Prof Simon Dixon (CI)","date":null,"link":"https://www.ukri.org/councils/innovate-uk/"},"html":"","id":"78c348d8-d78f-5acf-8441-382bc6c4d791"},{"fields":{"slug":"/projects/Benetos-L-Acoustics"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Online speech enhancement in scenarios with low direct-to-reverberant-ratio","author":"Dr Emmanouil Benetos (PI), Dr Aidan Hogg (CI)","date":null,"link":"https://www.l-acoustics.com/"},"html":"","id":"ce65fc6c-7228-5805-a9b2-6e14e5fd5086"},{"fields":{"slug":"/projects/Benetos-Moises"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Enhancing lyrics transcription with open-source architectures and fine-tuning techniques","author":"Dr Emmanouil Benetos (PI)","date":null,"link":"https://moises.ai/"},"html":"","id":"d61977ff-9345-55a4-b85e-44caffa4a7f0"},{"fields":{"slug":"/projects/Dixon-assessment"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Music Performance Assessment and Feedback","author":"Prof Simon Dixon (PI), Dr Emmanouil Benetos (CI)","date":null,"link":null},"html":"","id":"68ee56ea-59b7-58de-a8c9-805c4aba4357"}]},"news":{"nodes":[{"fields":{"slug":"/news/2025-02-11.C4DM-Seminar_audio.md"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Connecting two continents: Immersive Audio Research in Korea Advanced Institute of Science and Technology (KAIST) and Rochester Institute of Technology (RIT)","author":"nicolaus625","date":"Tue 11 Mar 2025"},"html":"<hr>\n<h3>C4DM Seminar: Connecting two continents: Immersive Audio Research in Korea Advanced Institute of Science and Technology (KAIST) and Rochester Institute of Technology (RIT)</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Dr. Sungyoung Kim</p>\n<p><strong>Date/time:</strong> Tuesday 11th March, 13:30-14:30</p>\n<p><strong>Location:</strong> G2, Engineering Building, Mile End Campus, QMUL, E1 4NS</p>\n<h2><b>Title</b>: Connecting two continents: Immersive Audio Research in Korea Advanced Institute of Science and Technology (KAIST) and Rochester Institute of Technology (RIT)</h2>\n<p><b>Abstract</b>:\nImmersive audio is experiencing a rapid boom across various fields, driven by AI advancements that open new possibilities in areas such as auditory attention and aural heritage preservation. This presentation explores the researcher's journey across Japan, the U.S., and Korea, highlighting how auditory immersion has fostered interdisciplinary collaborations and generated new insights. Key projects include aural heritage preservation of world heritage sites in Peru and traditional temples in Korea, neural decoding of a listener’s spatial attention, and the development of a game-based auditory training program for hard-of-hearing listeners. The presentation will also introduce a new collaborative initiative aimed at integrating sound recording, music, neuroscience, and AI to further explore the transformative potential of immersive audio in future research and applications.</p>\n<p><b>Bio</b>:\nSungyoung Kim received a B.S. degree from Sogang University, Korea, and Master of Music and Ph.D. from McGill University, Canada. Currently, he works for the Korea Advanced Institute of Science and Technology (KAIST) and Rochester Institute of Technology (RIT) as an associate professor. His research interests are rendering and perceptual evaluation of spatial audio, digital preservation of aural heritage, and auditory training for hearing rehabilitation. He leads the Applied and Innovative Research for Immersive Audio (AIRIS) laboratory (<a href=\"https://airislab.kaist.ac.kr/\">https://airislab.kaist.ac.kr/</a> ).</p>","id":"6b34adf2-d954-59a0-88b1-6a1b6932314c"},{"fields":{"slug":"/news/2025-02-10.C4DM-Seminar_Pipa.md"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Pipa Performative Score Dataset Construction based on Intelligent Installation and Computer Vision","author":"nicolaus625","date":"Mon 10 Feb 2025"},"html":"<hr>\n<h3>C4DM Seminar: Pipa Performative Score Dataset Construction based on Intelligent Installation and Computer Vision</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nDr. Rongfeng Li</p>\n<p><strong>Date/time:  Monday, 10 February, 14:30</strong></p>\n<p>**Location: G2, Engineering Building, Mile End Campus, QMUL, E1 4NS **\nZoom: not available</p>\n<h2><b>Title</b>: Pipa Performative Score Dataset Construction based on Intelligent Installation and Computer Vision</h2>\n<p><b>Abstract</b>:\nThis study addresses the limitations of AI models in analyzing traditional music, particularly in Western classical, by focusing on the intertwined composition and performance aspects of Chinese music. Using the Pipa, a Chinese string instrument, an Arduino-based system captures real-time finger techniques and string touches. Computer vision algorithms complement this by analyzing performance videos to extract note movements and playing techniques. This approach has resulted in an hour of Pipa performance data being converted into a detailed performative score, enriching AI’s capability in music digitization. These intelligent devices and recognition algorithms are instrumental for future AI-driven analysis of traditional music performances.</p>\n<p>NB: the pipa is a Chinese instrument that is similar to a lute.</p>\n<p><b>Bio</b>:\nLi Rongfeng: Associate Professor, Master's Supervisor of the School of Digital Media and Design Art, Beijing University of Posts and Telecommunications, a member of the Art and Artificial Intelligence Committee of the Chinese Association for Artificial Intelligence, and a member of the Future Science and Technology Committee of the Chinese Musical Instrument Association. In 2012, he graduated from the Artificial Intelligence Group of the Institute of Network and Information Systems, School of Information Science and Technology, Peking University, with a doctorate degree. His research fields include music technology, machine learning, etc. He has hosted the research work of the “Machine Learning-based Automatic Translation of Chinese Gong Chi Music” project funded by the Ministry of Education’s Social Science Fund and results were published onhttp://jiugong.chimusic.net. The \"Tianxia Musical Instrument\" project he cooperated with the China Conservatory of Music and the \"AI Solfeggio\" project in cooperation with the Central University For Nationalities have been online.</p>","id":"e52f4341-a22e-5bff-a059-bab295c5259e"},{"fields":{"slug":"/news/2025-02-05.C4DM-at_AAAI_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#083878","images":{"fallback":{"src":"/static/a8d43a863464c2bb2992bb17a1b99162/3042c/AAAI2025.png","srcSet":"/static/a8d43a863464c2bb2992bb17a1b99162/2e067/AAAI2025.png 151w,\n/static/a8d43a863464c2bb2992bb17a1b99162/6d16b/AAAI2025.png 302w,\n/static/a8d43a863464c2bb2992bb17a1b99162/3042c/AAAI2025.png 603w","sizes":"(min-width: 603px) 603px, 100vw"},"sources":[{"srcSet":"/static/a8d43a863464c2bb2992bb17a1b99162/43f11/AAAI2025.webp 151w,\n/static/a8d43a863464c2bb2992bb17a1b99162/5a5e0/AAAI2025.webp 302w,\n/static/a8d43a863464c2bb2992bb17a1b99162/9e027/AAAI2025.webp 603w","type":"image/webp","sizes":"(min-width: 603px) 603px, 100vw"}]},"width":603,"height":603}}},"title":"C4DM at AAAI 2025","author":"Admin","date":"Wed 05 Feb 2025"},"html":"<p>From the 25th February to 4th March 2025, two C4DM researchers will participate at the 39th Annual AAAI Conference on Artificial Intelligence (AAAI 2025), including the one day workshop on Artificial Intelligence for Music. AAAI is one of the leading conferences on artificial intelligence. This year AAAI will take place onsite in Philadelphia (PA, USA).</p>\n<p>The following works were authored/coauthored by C4DM PhD students and academic staff:</p>\n<p>-- <a href=\"https://arxiv.org/abs/2412.16526\">\"Text2MIDI: Generating Symbolic Music from Captions\"</a> by Keshav Bhandari, Abhinaba Roy, Kyra Wang, Geeta Puri, Simon Colton, and Dorien Herremans</p>\n<p>-- <a href=\"https://www.researchgate.net/publication/388274224_Towards_Music_Industry_50_Perspectives_on_Artificial_Intelligence\">\"Towards Music Industry 5.0: Perspectives on Artificial Intelligence\"</a> by Alexander Williams and Mathieu Barthet (AAAI workshop on Artificial Intelligence for Music)</p>\n<p>See you at AAAI!</p>","id":"78f98a34-e64e-5335-8724-ab394b03f0ff"},{"fields":{"slug":"/news/2025-24-01.C4DM-Seminar_Rodrigo_Constanzo"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM & AIL Seminar: Rodrigo Constanzo","author":"Admin","date":"Fri 24 Jan 2025"},"html":"<h3>C4DM &#x26; AIL Seminar: Rodrigo Constanzo: Hitting Laptops With Drumsticks: Approaches to Performing With Drums and Electronics</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Imperial College London, Dyson School of Design Engineering</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<h4>Augmented Instruments Lab Open Seminars</h4>\n<p><strong>Seminar by:</strong><br>\nRodrigo Constanzo (Royal Nothern College of Music)</p>\n<p><strong>Date/time:  Friday, 24th January 2025, 11am</strong></p>\n<p>**Location: Boardroom (Ground floor, opposite side to the café), Dyson School of Design Engineering, Imperial College London (South Kensington Campus) **</p>\n<p>YouTube Link: <a href=\"https://www.youtube.com/live/UlILHqq3aXs?si=hdisHl9UqtgZMq9d\">https://www.youtube.com/live/UlILHqq3aXs?si=hdisHl9UqtgZMq9d</a></p>\n<h2><b>Title</b>: Hitting Laptops With Drumsticks: Approaches to Performing With Drums and Electronics</h2>\n<p><b>Abstract</b>: Rodrigo Constanzo will present some of his recent work using drums and electronics covering a range of topics including audio analysis, meta-instrument design, machine learning, and absolute position tracking on a drum head.</p>\n<p><b>Bio</b>: Rodrigo Constanzo makes art. He thinks this is an important thing to do. The art he makes is generally smeared in time, in the form of music. He improvises and acts as an antennae to the beauty, electricity, and endless surprise that is living a crazy life. He composes and tries to create new sounds, interactions and behaviors that he find interesting and challenging. He performs his own music and the music of others on a variety of instruments, with close friends. He believes in magic. He believes in sharing things. He believes in teaching. He believes in openness. He loves sweet jams, and avoids speaking with a complicated vocabulary as much as possible. He tries to live as presently and honestly as possible.</p>","id":"e41171ce-74fd-5b55-80e4-cb9c07f1ed40"},{"fields":{"slug":"/news/2025-01-16.C4DM-Seminar_Pieter-Jan_Maes"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar:  Prof. Dr. Pieter-Jan Maes","author":"Admin","date":"Thu 16 Jan 2025"},"html":"<h3>C4DM Seminar: Prof. Dr. Pieter-Jan Maes: Embodied music interaction in eXtended Reality with virtual avatars and agents</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nProf. Dr. Pieter-Jan Maes</p>\n<p><strong>Date/time:  Thursday, 16th January 2025, 1pm</strong></p>\n<p>**Location: G2, Engineering Building, Mile End Campus, QMUL, E1 4NS **</p>\n<h2><b>Title</b>: Embodied music interaction in eXtended Reality with virtual avatars and agents</h2>\n<p><b>Abstract</b>: In my talk, I will present how eXtended Reality (XR) enhances embodied music interaction, particularly in remote music performance and collaboration with computational agents. At IPEM, we investigate ‘virtual embodiment,’ a concept that extends ‘naturalistic’ embodied music interaction into virtual (social) spaces. I'll present a series of experiments and creative-artistic applications on virtual embodiment in the domain of (remote) music performance, education, and dance.</p>\n<p><b>Bio</b>: My research is situated in the field of systematic musicology, focusing on embodied and social interaction with music. This research requires a cross-over of fundamental music research – including theory formation and experimental research – and music application development. Links: <a href=\"https://research.flw.ugent.be/en/pieterjan.maes\">https://research.flw.ugent.be/en/pieterjan.maes</a>, <a href=\"https://asil.ugent.be/team/ipem/\">https://asil.ugent.be/team/ipem/</a>.</p>","id":"37f48e08-7a5c-5286-9e48-9ad2e8cdcba5"},{"fields":{"slug":"/news/2024-03-12.C4DM-Seminar_Manvi_agarwal"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Manvi Agarwal","author":"admin","date":"Tue 03 Dec 2024"},"html":"<h3>C4DM Seminar: Manvi Agarwal: Fast Structure-informed Positional Encoding for Music Generation</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nManvi Agarwal (Télécom Paris)</p>\n<p><strong>Date/time:  Tuesday, 3rd December 2024, 2pm</strong></p>\n<p>**Location: Room G2, Engineering Building, Mile End Campus, QMUL, E1 4NS **\nZoom: <a href=\"https://qmul-ac-uk.zoom.us/j/2387202947\">https://qmul-ac-uk.zoom.us/j/2387202947</a></p>\n<h2><b>Title</b>: Fast Structure-informed Positional Encoding for Music Generation</h2>\n<p><b>Abstract</b>:  Music generated by deep learning methods often suffers from a lack of coherence and long-term organization. Over the years, several solutions have been proposed to help generative music architectures capture multi-scale hierarchical structure, which is a distinctive feature of music signals. The focus of my talk is the use of musically-relevant structural information to improve music Transformers. Specifically, I will present structure-informed positional encoding as a way to achieve superior music generation performance with low resource requirements. I will put forward two perspectives - an empirical approach exploring different designs for incorporating structural information in positional encoding and a theoretical approach using kernel approximations for improving the generative performance and computational complexity of such designs. In this way, I hope to underline the strengths of well-designed priors in dealing with some of the challenges facing music generation systems.</p>\n<p><b>Bio</b>: Manvi Agarwal is doing her PhD under the supervision of Dr. Changhong Wang and Prof. Gaël Richard in the ADASP (Audio Data Analysis and Signal Processing) group, Télécom Paris, Institut Polytechnique de Paris, France, supported by the ERC-funded Hi-Audio project. Her PhD research looks at how inductive biases can be introduced into Transformers to improve their modelling capabilities on music data. Broadly, she is interested in how sequence-based learning works and how our understanding of this process in different deep learning architectures can help us make these architectures perform better, especially in low-resource settings.</p>","id":"3d7650a4-9982-5956-8b1c-5b6641c3f20c"}]}}}
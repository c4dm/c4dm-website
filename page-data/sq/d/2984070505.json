{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/YIFgS8she58","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/EPSRC-additional-skills"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png","srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/2fd20/EPSRC.png 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/de391/EPSRC.png 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/d66e1/EPSRC.webp 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/e7160/EPSRC.webp 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/5f169/EPSRC.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"EPSRC additional skills funding summer 2025","author":"Prof Akram Alomainy (PI), Prof Simon Dixon (CI), Dr Iran Roman (CI) plus 8 others","date":null,"link":"https://www.ukri.org/councils/epsrc/"},"html":"","id":"bc1e4be0-d104-5b53-8c02-61f296532555"},{"fields":{"slug":"/projects/Fazekas-Leverhulme"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/14ae7694223d03edb7966d74ea33ce18/82c11/RAEng.png","srcSet":"/static/14ae7694223d03edb7966d74ea33ce18/2fd20/RAEng.png 125w,\n/static/14ae7694223d03edb7966d74ea33ce18/de391/RAEng.png 250w,\n/static/14ae7694223d03edb7966d74ea33ce18/82c11/RAEng.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/14ae7694223d03edb7966d74ea33ce18/d66e1/RAEng.webp 125w,\n/static/14ae7694223d03edb7966d74ea33ce18/e7160/RAEng.webp 250w,\n/static/14ae7694223d03edb7966d74ea33ce18/5f169/RAEng.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Knowledge-driven Deep Learning for Music Informatics","author":"Dr George Fazekas (PI)","date":null,"link":"https://raeng.org.uk/programmes-and-prizes/programmes/uk-grants-and-prizes/support-for-research/research-awardees/leverhulme-awardees/2025-2026/dr-george-fazekas/"},"html":"","id":"f47b222e-21cf-5c90-bff2-6181b436d1aa"},{"fields":{"slug":"/projects/Fazekas-Steinberg-2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png","srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/4b686/aimcdt.png 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/d2213/aimcdt.png 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png 615w","sizes":"(min-width: 615px) 615px, 100vw"},"sources":[{"srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/b2942/aimcdt.webp 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/46581/aimcdt.webp 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/fa942/aimcdt.webp 615w","type":"image/webp","sizes":"(min-width: 615px) 615px, 100vw"}]},"width":615,"height":615}}},"title":"AI-Powered Audio Loop Generation for Assistive Music Production","author":"Dr George Fazekas (PI)","date":null,"link":null},"html":"","id":"90759bed-47aa-57d3-8ce9-0dbe6a6b40ac"},{"fields":{"slug":"/projects/Ma-Google"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e3011e08b70d2397f6a5f53aca3eac47/b5658/Google.png","srcSet":"/static/e3011e08b70d2397f6a5f53aca3eac47/acb7c/Google.png 256w,\n/static/e3011e08b70d2397f6a5f53aca3eac47/ccc41/Google.png 512w,\n/static/e3011e08b70d2397f6a5f53aca3eac47/b5658/Google.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e3011e08b70d2397f6a5f53aca3eac47/22bfc/Google.webp 256w,\n/static/e3011e08b70d2397f6a5f53aca3eac47/d689f/Google.webp 512w,\n/static/e3011e08b70d2397f6a5f53aca3eac47/67ded/Google.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"title":"Large Language Models for Multimodal Music Understanding and Ethical Audio Generation","author":"Dr Emmanouil Benetos (PI), Yinghao Ma (PhD fellow)","date":null,"link":"https://research.google/programs-and-events/phd-fellowship/"},"html":"","id":"e4f5df67-9d4e-57da-bb16-3efcb2359e70"},{"fields":{"slug":"/projects/Pauwels-Sofilab"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6aad6623ce4e89038628dab00ebc48e0/c8f14/Sofilab.png","srcSet":"/static/6aad6623ce4e89038628dab00ebc48e0/abab1/Sofilab.png 185w,\n/static/6aad6623ce4e89038628dab00ebc48e0/96aa8/Sofilab.png 370w,\n/static/6aad6623ce4e89038628dab00ebc48e0/c8f14/Sofilab.png 739w","sizes":"(min-width: 739px) 739px, 100vw"},"sources":[{"srcSet":"/static/6aad6623ce4e89038628dab00ebc48e0/c5b6a/Sofilab.webp 185w,\n/static/6aad6623ce4e89038628dab00ebc48e0/7fd3d/Sofilab.webp 370w,\n/static/6aad6623ce4e89038628dab00ebc48e0/5bbdd/Sofilab.webp 739w","type":"image/webp","sizes":"(min-width: 739px) 739px, 100vw"}]},"width":739,"height":739}}},"title":"Smart musical corpus technologies","author":"Dr Johan Pauwels (PI)","date":null,"link":"https://sofilab.art/"},"html":"","id":"5b93e208-074a-5baf-a108-023df35a29f0"},{"fields":{"slug":"/projects/Reiss-Yamaha"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Yamaha Visiting Researcher Collaboration","author":"Prof Josh Reiss (PI)","date":null,"link":"https://intelligentsoundengineering.wordpress.com/"},"html":"","id":"145db7f6-77af-5f89-b477-6c4d87952ef4"}]},"news":{"nodes":[{"fields":{"slug":"/news/2025-11-20.C4DM-Seminar_Eloi_Moliner"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Eloi Moliner","author":null,"date":"Thu 20 Nov 2025"},"html":"<h3>C4DM Seminar: Eloi Moliner: Diffusion Models for Audio Effects: From Blind Estimation to Automatic Mixing</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Eloi Moliner</p>\n<p><strong>Date/time:</strong>  Thursday, 20th Nov 2025, 11 am</p>\n<p><strong>Location:</strong> GC203, Graduate Centre, Mile End Campus, Queen Mary University of London</p>\n<p><a href=\"https://teams.microsoft.com/l/meetup-join/19%3ameeting_ZDcxODQ2ZTctN2NkYy00MDExLWEyOWMtNzRkMjlmMGUxMThh%40thread.v2/0?context=%7b%22Tid%22%3a%22569df091-b013-40e3-86ee-bd9cb9e25814%22%2c%22Oid%22%3a%22cad9cb9a-abc2-4aac-ab51-682e8a852753%22%7d\">Teams meeting link</a></p>\n<h2><b>Title</b>: Diffusion Models for Audio Effects: From Blind Estimation to Automatic Mixing</h2>\n<p><b>Abstract</b>:\nThis talk presents a series of recent works exploring the use of diffusion models in the analysis and generation of audio effects. In the first part, I will focus on blind and unsupervised effect estimation, where diffusion models are employed as powerful data-driven priors for recovering clean or unprocessed signals from their altered counterparts, while simultaneously estimating the parameters of a model of the underlying audio effect. In the second part, I will discuss a recent work on automatic music mixing, introducing MEGAMI (Multitrack Embedding Generative Auto MIxing)—a generative framework that models the distribution of professional mixes directly in a multitrack effect embedding space.</p>\n<p><b>Bio</b>:\nEloi Moliner received his Ph.D. degree from the Acoustics Lab of Aalto University, Espoo, Finland, in 2025. He previuosly obtained his M.Sc. degree in Telecommunications Engineering in 2021 and his B.Sc. degree in Telecommunications Technologies and Services Engineering in 2018, both from the Polytechnic University of Catalonia, Spain. He received the Best Student Paper Awards at IEEE ICASSP 2023, IWAENC 2024, and AES AIMLA 2025. His research interests include generative models for audio, audio restoration and enhancement, and audio signal processing.</p>","id":"e7b942ad-e4fe-5988-87fe-7bad707e1b48"},{"fields":{"slug":"/news/2025-11-17.C4DM-at_NeurIPS_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/a20d19fb2ed10997b9bc578028722b0a/baaed/NeurIPS-2025.jpg","srcSet":"/static/a20d19fb2ed10997b9bc578028722b0a/dd515/NeurIPS-2025.jpg 200w,\n/static/a20d19fb2ed10997b9bc578028722b0a/47930/NeurIPS-2025.jpg 400w,\n/static/a20d19fb2ed10997b9bc578028722b0a/baaed/NeurIPS-2025.jpg 800w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[{"srcSet":"/static/a20d19fb2ed10997b9bc578028722b0a/2e34e/NeurIPS-2025.webp 200w,\n/static/a20d19fb2ed10997b9bc578028722b0a/416c3/NeurIPS-2025.webp 400w,\n/static/a20d19fb2ed10997b9bc578028722b0a/c1587/NeurIPS-2025.webp 800w","type":"image/webp","sizes":"(min-width: 800px) 800px, 100vw"}]},"width":800,"height":800}}},"title":"C4DM at NeurIPS 2025","author":"Emmanouil Benetos","date":"Mon 17 Nov 2025"},"html":"<p>On 2-7 December, C4DM researchers will participate at the <a href=\"https://neurips.cc/\">39th Annual Conference on Neural Information Processing Systems (NeurIPS 2025)</a>, taking place in San Diego, USA. NeurIPS is a prestigious annual academic conference and non-profit foundation that fosters the exchange of research in artificial intelligence (AI), machine learning (ML), and computational neuroscience.</p>\n<p>The following papers from C4DM members will be presented at the <b>Datasets and Benchmarks track</b> of NeurIPS 2025:</p>\n<ul>\n<li>OmniBench: Towards The Future of Universal Omni-Language Models</li>\n</ul>\n<p>Yizhi LI, Ge Zhang, Yinghao Ma, Ruibin Yuan, King Zhu, Hangyu Guo, Yiming Liang, Jiaheng Liu, Zekun Moore Wang, Jian Yang, Siwei Wu, Xingwei Qu, Jinjie Shi, Xinyue Zhang, Zhenzhu Yang, Yidan WEN, Yanghai Wang, Shihao Li, Zhaoxiang Zhang, Ruibo Liu, Emmanouil Benetos, Wenhao Huang, Chenghua Lin\n<a href=\"https://openreview.net/forum?id=SSF4qgsNYE\">https://openreview.net/forum?id=SSF4qgsNYE</a></p>\n<ul>\n<li>MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix</li>\n</ul>\n<p>Ziyang Ma, Yinghao Ma, Yanqiao Zhu, Chen Yang, Yi-Wen Chao, Ruiyang Xu, Wenxi Chen, Yuanzhe Chen, Zhuo Chen, Jian Cong, Kai Li, Keliang Li, Siyou Li, Xinfeng Li, Xiquan Li, Zheng Lian, Yuzhe Liang, Minghao Liu, Zhikang Niu, tianrui wang, Yuping Wang, Yuxuan Wang, Yihao Wu, Guanrou Yang, Jianwei Yu, Ruibin Yuan, Zhisheng Zheng, Ziya Zhou, Haina Zhu, Wei Xue, Emmanouil Benetos, Kai Yu, EngSiong Chng, Xie Chen\n<a href=\"https://openreview.net/forum?id=fgmrBJemlQ\">https://openreview.net/forum?id=fgmrBJemlQ</a></p>\n<p>The following paper will be presented at the <b>Creative AI track</b> of NeurIPS 2025:</p>\n<ul>\n<li>The Ghost in the Keys: A Disklavier Demo for Human-AI Musical Co-Creativity</li>\n</ul>\n<p>Louis Bradshaw, Alexander Spangher, Stella Biderman, Simon Colton\n<a href=\"https://openreview.net/forum?id=3yeBer3J5z\">https://openreview.net/forum?id=3yeBer3J5z</a></p>\n<p>See you all at NeurIPS!</p>","id":"cadafe6e-e591-5ae2-8b62-52de8a24c21a"},{"fields":{"slug":"/news/2025-11-05.BMVA-Workshop"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#a8a898","images":{"fallback":{"src":"/static/f50b3f2ab8deecae63e8cdd69d401e0b/2b34e/BMVA2025.png","srcSet":"/static/f50b3f2ab8deecae63e8cdd69d401e0b/bcbe6/BMVA2025.png 245w,\n/static/f50b3f2ab8deecae63e8cdd69d401e0b/78a05/BMVA2025.png 490w,\n/static/f50b3f2ab8deecae63e8cdd69d401e0b/2b34e/BMVA2025.png 979w","sizes":"(min-width: 979px) 979px, 100vw"},"sources":[{"srcSet":"/static/f50b3f2ab8deecae63e8cdd69d401e0b/c4230/BMVA2025.webp 245w,\n/static/f50b3f2ab8deecae63e8cdd69d401e0b/a5fb7/BMVA2025.webp 490w,\n/static/f50b3f2ab8deecae63e8cdd69d401e0b/758a3/BMVA2025.webp 979w","type":"image/webp","sizes":"(min-width: 979px) 979px, 100vw"}]},"width":979,"height":979}}},"title":"The British Machine Vision Association Workshop on Multimodal LLMs","author":"admin","date":"Wed 05 Nov 2025"},"html":"<p>The <a href=\"https://www.bmva.org/meetings/25-11-05-MultimodalLargeModels.html\">British Machine Vision Association and Society for Pattern Recognition (BMVA) Workshop on Multimodal Large Models Bridging Vision, Language, and Beyond</a> was held at British Computer Society (BCS), 25 Copthall Avenue, London EC2R 7BP on November 5th, 2025.</p>\n<p>Among the selected oral presentations, C4DM PhD student <strong>Yinghao Ma</strong> (supervised by Prof. Emmanouil Benetos) presented his latest work <em>MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix</em>. MMAR is a newly released benchmark that spans 1,000 real-world audio reasoning tasks, covering speech, sound events, music, and mixed-modality scenarios. It is one of the first benchmarks to explicitly evaluate multi-step reasoning abilities in audio-language and omni-modal large models, with tasks ranging from low-level signal perception to high-level cultural understanding.</p>\n<p>The talk was part of the “Domain Applications and Human-Centric Modalities” session, alongside research on sign language translation, visual illusions, and 3D-aware facial editing. MMAR attracted interest from both academia and industry attendees, especially as multimodal reasoning becomes a key focus in the next wave of AI foundation models.</p>\n<p>The BMVA workshop featured keynote speakers from Google DeepMind, UCL, and the University of Surrey, and brought together researchers advancing the frontier of multimodal intelligence across vision, language, audio, and embodied learning.</p>\n<p>MMAR is open-source and available on arXiv: <strong>arXiv:2505.13032</strong>. The video recording of presentation is available at <a href=\"https://www.bmva.org/meetings/25-11-05-MultimodalLargeModels.html\">here</a></p>","id":"8f6388ab-107d-5585-8923-6db01cfbded3"},{"fields":{"slug":"/news/2025-11-05.C4DM-Seminar_Masataka_Goto"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Masataka Goto","author":null,"date":"Wed 05 Nov 2025"},"html":"<h3>C4DM Seminar: Masataka Goto: Advancing Music Experience Through Music Information Research</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Masataka Goto</p>\n<p><strong>Date/time:</strong>  Wednesday, 5th Nov 2025, 3.00pm</p>\n<p><strong>Location:</strong> In-person only. Mile Eng campus, Eng building, room G2</p>\n<h2><b>Title</b>: Advancing Music Experience Through Music Information Research</h2>\n<p><b>Abstract</b>:\nMusic technologies will open up new ways of enjoying music, both in terms of creating and appreciating music. In this talk, I will discuss how music information research can enrich music experiences by introducing examples of our research outcomes. For example, \"Lyric Apps\" offer a new form of lyric-driven visual art, dynamically rendering different visual content based on user interaction. After releasing \"TextAlive App API\", a web-based framework for creating lyric apps, which has received several awards including at ACM CHI 2023, we have held annual programming contests since 2020. Another example is \"Kiite Cafe\", a web service that allows users to get together virtually to listen to music. It lets users enjoy the same song simultaneously while reacting in real time, creating a shared listening experience, and has also received multiple awards. In the future, further advances in music information research will make interactions between people and music more active and enriching.</p>\n<p><b>Bio</b>:\nMasataka Goto received the Doctor of Engineering degree from Waseda University in 1998. He is currently the Senior Principal Researcher at the National Institute of Advanced Industrial Science and Technology (AIST), Japan. Over the past 33 years, he has published more than 350 papers in refereed journals and international conference proceedings and has received 76 awards, including several best paper awards, best presentation awards, the Tenth Japan Academy Medal, and the Tenth JSPS PRIZE. He has served as a committee member of over 140 scientific societies and conferences, including as the General Chair of ISMIR 2009 and 2014 and the Program Chair of ISMIR 2022.</p>","id":"ac072711-e924-5b7a-a17a-02e79c202de8"},{"fields":{"slug":"/news/2025-10-23.C4DM-student-awarded-Google-PhD-fellowship"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4ac60065bd12d76e2e9830ee1a3f805a/a89ca/yinghao.jpg","srcSet":"/static/4ac60065bd12d76e2e9830ee1a3f805a/96deb/yinghao.jpg 150w,\n/static/4ac60065bd12d76e2e9830ee1a3f805a/0fdf4/yinghao.jpg 300w,\n/static/4ac60065bd12d76e2e9830ee1a3f805a/a89ca/yinghao.jpg 600w","sizes":"(min-width: 600px) 600px, 100vw"},"sources":[{"srcSet":"/static/4ac60065bd12d76e2e9830ee1a3f805a/c65bc/yinghao.webp 150w,\n/static/4ac60065bd12d76e2e9830ee1a3f805a/078c3/yinghao.webp 300w,\n/static/4ac60065bd12d76e2e9830ee1a3f805a/6d09e/yinghao.webp 600w","type":"image/webp","sizes":"(min-width: 600px) 600px, 100vw"}]},"width":600,"height":600}}},"title":"C4DM PhD student awarded Google PhD Fellowship","author":"Admin","date":"Thu 23 Oct 2025"},"html":"<p>We are extremely proud to announce that <a href=\"https://nicolaus625.github.io/\">Yinghao Ma</a>, a PhD student in AI and Music at the Centre for Digital Music at QMUL, has been awarded the <a href=\"https://research.google/programs-and-events/phd-fellowship/recipients/?filtertab=2025\">2025 Google Fellowship in Machine Perception</a>.</p>\n<p>A Google spokesperson said: “The student nominations we received this year were exemplary in their quality, but Yinghao especially stood out and was endorsed by the research scientists and distinguished engineers within Google who participated in the review. Congratulations to Yinghao on this well-deserved recognition, it’s an honor to support such incredibly talented students.”</p>\n<p>Yinghao's PhD research focuses on advancing Large Language Models (LLMs) for music understanding and generation. Specifically, he studies how multimodal models can integrate audio, symbolic, and textual information to understand, reason about, and generate music.</p>\n<p>Together with colleagues, he developed <a href=\"https://arxiv.org/abs/2306.00107\">MERT</a>, a large-scale music audio representation model which has more than 10k monthly download in the past three years. His recent work includes developing music instruction-following datasets and benchmarks that help evaluate how well AI systems can comprehend and create music.</p>\n<p>He said: \"It's my great honour to receive the Google PhD Fellowship that recognises my research and strongly contribute to my future career. I’m deeply grateful to Google and QMUL for the support, providing good platforms for AI &#x26; music research.\"</p>\n<p>Congratulations Yinghao!</p>","id":"1ec9aa73-6b0c-5967-b4d1-2b1e075f8ebe"},{"fields":{"slug":"/news/2025-10-22.C4DM-Seminar_Come_Peladeau"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Côme Peladeau","author":"Admin","date":"Wed 22 Oct 2025"},"html":"<h3>C4DM Seminar: Côme Peladeau: Audio processors: estimating parameter distributions using discrete normalizing flows and DDSP</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Côme Peladeau</p>\n<p><strong>Date/time:</strong>  Wednesday, 22th October 2025, 1pm</p>\n<p><strong>Location:</strong> Online</p>\n<p><strong>Zoom Link:</strong> <a href=\"https://qmul-ac-uk.zoom.us/j/2387202947\">https://qmul-ac-uk.zoom.us/j/2387202947</a></p>\n<h2><b>Title</b>: Audio processors: estimating parameter distributions using discrete normalizing flows and DDSP</h2>\n<p><b>Abstract</b>: Audio processor, such as audio effects or synthesizers, are widely used in popular music production.</p>\n<p>Their parameters control the quality of the output sound. Multiple combinations of parameters can lead to the same sound.</p>\n<p>While recent approaches have been proposed to estimate these parameters given only the output sound, those are deterministic, i.e. they only estimate a single solution among the many possible parameter configurations.</p>\n<p>This work proposes an approach to estimate all the combinations that lead to the target sound, in the form of a probability distribution.</p>\n<p>We achieve this using differentiable signal processing and discrete normalizing flows.</p>\n<p><b>Bio</b>: Côme Peladeau graduated with a masters degree in Acoustics, Signal Processing and Computer Science applied to Music at Sorbonne Université in 2023.</p>\n<p>He is now a PhD student working with Geoffroy Peeters and Dominique Fourer at the Information Processing and Communications Laboratory (LTCI) in Telecom Paris.</p>\n<p>HI work focuses on audio effects estimation using deep learning and differentiable signal processing.</p>","id":"184cbea7-8b9a-5196-9c4d-7a5b3238c12c"}]}}}
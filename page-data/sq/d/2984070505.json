{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/YIFgS8she58","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/Sandler-Artificial-Neuroscience"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png","srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/2fd20/EPSRC.png 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/de391/EPSRC.png 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/d66e1/EPSRC.webp 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/e7160/EPSRC.webp 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/5f169/EPSRC.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Artificial Neuroscience: metrology and engineering for Deep Learning using Linear Algebra","author":"Prof Mark Sandler (PI), Boris Khoruzhenko (CI)","date":null,"link":"https://www.ukri.org/councils/epsrc/"},"html":"","id":"c1314000-fe3b-5175-88a2-f239d861ddad"},{"fields":{"slug":"/projects/Barthet-Netz"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Netz: A Novel XR Musical Instrument","author":"Dr Mathieu Barthet (PI)","date":null,"link":"https://iuk-business-connect.org.uk/programme/icure/"},"html":"","id":"0040b455-65cf-5ea1-a7ce-38ab70570dae"},{"fields":{"slug":"/projects/Benetos-Algorivm"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Project Maestro - AI Musical Analysis Platform","author":"Dr Emmanouil Benetos (PI), Prof Simon Dixon (CI)","date":null,"link":"https://www.ukri.org/councils/innovate-uk/"},"html":"","id":"78c348d8-d78f-5acf-8441-382bc6c4d791"},{"fields":{"slug":"/projects/Benetos-L-Acoustics"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Online speech enhancement in scenarios with low direct-to-reverberant-ratio","author":"Dr Emmanouil Benetos (PI), Dr Aidan Hogg (CI)","date":null,"link":"https://www.l-acoustics.com/"},"html":"","id":"ce65fc6c-7228-5805-a9b2-6e14e5fd5086"},{"fields":{"slug":"/projects/Benetos-Moises"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Enhancing lyrics transcription with open-source architectures and fine-tuning techniques","author":"Dr Emmanouil Benetos (PI)","date":null,"link":"https://moises.ai/"},"html":"","id":"d61977ff-9345-55a4-b85e-44caffa4a7f0"},{"fields":{"slug":"/projects/Dixon-assessment"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Music Performance Assessment and Feedback","author":"Prof Simon Dixon (PI), Dr Emmanouil Benetos (CI)","date":null,"link":null},"html":"","id":"68ee56ea-59b7-58de-a8c9-805c4aba4357"}]},"news":{"nodes":[{"fields":{"slug":"/news/2025-06-06.C4DM-Seminar_Tuomas_Eerola"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Tuomas Eerola","author":"Admin","date":"Fri 06 Jun 2025"},"html":"<h3>C4DM Seminar: Tuomas Eerola: Computational Recognition of Emotions in Music: A Meta-Analysis and Critical Review</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Tuomas Eerola</p>\n<p><strong>Date/time:</strong>  Friday, 6th June 2025, 11am</p>\n<p><strong>Location:</strong> To be updated</p>\n<p><strong>Zoom:</strong> <a href=\"https://qmul-ac-uk.zoom.us/j/2387202947\">https://qmul-ac-uk.zoom.us/j/2387202947</a></p>\n<h2><b>Title</b>: Computational Recognition of Emotions in Music: A Meta-Analysis and Critical Review</h2>\n<p><b>Abstract</b>:\nThis talk presents a meta-analysis of music emotion recognition (MER) models published between 2014 and 2024, focusing on predictions of valence, arousal, and categorical emotions. A total of 553 studies were identified, of which 96 full-text articles were assessed. This resulted in a final review of 34 studies comprising 204 distinct models. Valence and arousal were predicted with reasonable accuracy (r = 0.67 and r = 0.81, respectively), while classification models achieved an accuracy of 0.87. Across modeling approaches, linear and tree-based methods generally outperformed neural networks in regression tasks, whereas neural networks and support vector machines showed the highest performance in classification tasks. The talk will address several critical issues, including conceptual shortcomings, insufficient quality control, lack of transparency, limited dataset size, and a narrow range of predicted emotions in computational approaches to music emotion recognition.</p>\n<p><b>Bio</b>:\nTuomas Eerola is a music psychologist and Professor of Music Cognition at Durham University, UK. He is a leading researcher in the field of music cognition, using empirical experiments, theorising, and computational models to study how people engage with and process music. His work encompasses a wide range of topics, including emotions induced by music and perception of musical structure, rhythm, timbre, consonance, and emotional communication through music. He has published more than 160 papers and book chapters and is on the editorial boards of several prominent music psychology journals. His research has been funded by the Academy of Finland, AHRC (UK), ESRC (UK), and EU Horizon 2020. Eerola is the author of the book \"<a href=\"https://www.taylorfrancis.com/books/mono/10.4324/9781003293804/music-science-tuomas-eerola\">Music and Science - Guide to Empirical Research</a>\" published by Routledge in 2024.<br>\nUniversity profile: <a href=\"https://www.durham.ac.uk/staff/tuomas-eerola/\">https://www.durham.ac.uk/staff/tuomas-eerola/</a><br>\nGitHub: <a href=\"https://tuomaseerola.github.io\">https://tuomaseerola.github.io</a><br>\nMusic &#x26; Science Lab: <a href=\"https://musicscience.net\">https://musicscience.net</a></p>","id":"59c097a8-d22f-5d61-8385-d42c336a9f1f"},{"fields":{"slug":"/news/2025-05-21.C4DM-Seminar_Domestic_Data_Streamers"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Domestic Data Streamers","author":"Admin","date":"Wed 21 May 2025"},"html":"<h3>C4DM Seminar: Domestic Data Streamers: Fighting Indifference Towards Data</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>EECS Seminar &#x26; Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Domestic Data Streamers</p>\n<p><strong>Date/time:</strong>  Wednesday, 21st May 2025, 12:30pm</p>\n<p><strong>Location:</strong> G2, Engineering Building, Mile End Campus, QMUL, E1 4NS</p>\n<p><strong>Zoom:</strong> <a href=\"https://qmul-ac-uk.zoom.us/j/9066549084\">https://qmul-ac-uk.zoom.us/j/9066549084</a></p>\n<h2><b>Title</b>: Fighting Indifference Towards Data</h2>\n<p><b>Abstract</b>:\nNumbers, data, and statistics are often seen as objective truths, but they can be misleading. They show just a slice of the whole reality and fail to capture the emotional weight of today’s challenges. So, how do we cultivate empathy when the problems we face are so interconnected, overwhelming, and global in scale? In this talk, we will explore some humble experiments done to overcome this lack of empathy through art, technology, and participatory experiences by Domestic Data Streamers.</p>\n<p><b>Bio</b>:\nDomestic Data Streamers is a research and design studio partnering up with organisations to build change through data, community, and arts. Domestic Data Streamers was founded on the 28th of September of 2013 with a simple idea: That the world couldn’t be understood without numbers, but it wouldn’t be understood with numbers alone. We believe that any meaningful interchange of information between people needs to carry emotions and experiences to create knowledge or change. Since then, we have had the chance to bring this idea all over the world, from schools to prisons, from churches to corporate headquarters, and even to the United Nations General Assembly.<br>\n<a href=\"https://www.domesticstreamers.com/\">https://www.domesticstreamers.com/</a></p>","id":"d1de7d9c-7ef9-5b94-8f03-3b2085ccf9b4"},{"fields":{"slug":"/news/2025-05-29.C4DM-Seminar_Alberto_Bernardini"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Alberto Bernardini","author":"Admin","date":"Wed 21 May 2025"},"html":"<h3>C4DM Seminar: Alberto Bernardini:  Differentiable Physical Models of Acoustic Systems for Audio Signal Processing</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>EECS Seminar &#x26; Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Alberto Bernardini</p>\n<p><strong>Date/time:</strong>  Thursday, 29th May 2025, 11am</p>\n<p><strong>Location:</strong> GC204, Graduate Centre Building, Mile End Campus, Queen Mary University of London, E1 4NS</p>\n<p><strong>Zoom:</strong> <a href=\"https://qmul-ac-uk.zoom.us/j/9798452959\">https://qmul-ac-uk.zoom.us/j/9798452959</a></p>\n<h2><b>Title</b>: Differentiable Physical Models of Acoustic Systems for Audio Signal Processing</h2>\n<p><b>Abstract</b>:\nAccurate simulations of acoustic systems—such as analog audio circuits, transducers, and acoustic reverberant environments—are often computationally demanding, which limits their use in real-time or online audio signal processing contexts. To overcome this challenge, there is a growing need for physically motivated models that are both computationally efficient and sufficiently accurate to be embedded in audio processing pipelines. In recent years, automatic differentiation techniques have opened new possibilities by enabling the direct optimization of physical model parameters using measured data. This development has given rise to hybrid modeling approaches that combine the interpretability and robustness of physics-based models with the adaptability and accuracy of data-driven methods. In this seminar, we will explore the emerging field of differentiable physical modeling for audio applications. We will present concrete examples including models of analog audio circuits for Virtual Analog Modeling, physical models of acoustic transducers such as loudspeakers and microphones, and artificial reverberation systems based on Delay Networks. These case studies will demonstrate how differentiable models can be leveraged to bridge simulation and machine learning in the design of modern audio processing systems.</p>\n<p><b>Bio</b>:\nAlberto Bernardini received the B.S. degree in computer engineering from the University of Bologna, Bologna, Italy, in 2012, and the M.S. degree (cum laude) in computer engineering and the Ph.D. degree (cum laude) in information engineering from the Politecnico di Milano, Milan, Italy, in 2015 and 2019, respectively. From 2019 to 2021, he was a Postdoctoral Researcher with the Politecnico di Milano. He is currently an Assistant Professor with the Dipartimento di Elettronica, Informazione e Bioingegneria (DEIB), Politecnico di Milano. He authored more than 80 publications in international journals and proceedings of international conferences. He is coauthor of three international patents. His research interests mainly include audio signal processing, computational acoustics, and modeling of nonlinear systems. He was a recipient of the Dimitris N. Chorafas Award in 2019. He is an IEEE Senior Member. He is an Associate Member of the Digital Signal Processing Technical Committee and a Regular Member of the Nonlinear Circuits and Systems Technical Committee, both of the IEEE Circuits and Systems Society.  He is a Member of the EURASIP Signal and Data Analytics for Machine Learning Technical Area Committee. He was an Associate Editor for the IEEE Transactions on Circuits and Systems I: Regular Papers. He is now an Associate Editor for the IEEE/ACM Transactions on Audio, Speech, and Language Processing, the Elsevier Digital Signal Processing journal, and the EURASIP Journal on Audio, Speech, and Music Processing.</p>","id":"c69e6934-ab14-516e-a1d8-044599a65e0d"},{"fields":{"slug":"/news/2025-05-12.C4DM-Paper_Award_EvoMUSART"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/b77da60f8aff9865d0a75e25f6644e22/13677/evomusart_best_paper_2025.png","srcSet":"/static/b77da60f8aff9865d0a75e25f6644e22/de391/evomusart_best_paper_2025.png 250w,\n/static/b77da60f8aff9865d0a75e25f6644e22/82c11/evomusart_best_paper_2025.png 500w,\n/static/b77da60f8aff9865d0a75e25f6644e22/13677/evomusart_best_paper_2025.png 1000w","sizes":"(min-width: 1000px) 1000px, 100vw"},"sources":[{"srcSet":"/static/b77da60f8aff9865d0a75e25f6644e22/e7160/evomusart_best_paper_2025.webp 250w,\n/static/b77da60f8aff9865d0a75e25f6644e22/5f169/evomusart_best_paper_2025.webp 500w,\n/static/b77da60f8aff9865d0a75e25f6644e22/3cd29/evomusart_best_paper_2025.webp 1000w","type":"image/webp","sizes":"(min-width: 1000px) 1000px, 100vw"}]},"width":1000,"height":1000}}},"title":"C4DM Paper Award at EvoMUSART 2025","author":"Admin","date":"Mon 12 May 2025"},"html":"<p></p>\n<p>The <a href=\"https://www.evostar.org/2025/evomusart/\">14th International Conference on Artificial Intelligence in Music, Sound, Art and Design (EvoMUSART)</a>, part of Evostar, took place in Trieste, Italy, between 23 and 25 April 2025.</p>\n<p>We are pleased to announce that the following paper authored by C4DM members received the best paper award!</p>\n<p><a href=\"https://arxiv.org/abs/2501.17759\">Yin-Yang: Developing Motifs With Long-Term Structure And Controllability</a>, by Keshav Bhandari, Geraint A. Wiggins, Simon Colton</p>\n<p>Yin-Yang is a neuro-symbolic framework that combines three transformer models to generate structured melodies with coherent long-term development, while allowing user control over musical themes and variations.</p>","id":"ee233f18-26bd-57e5-a96e-0ddd0d31228c"},{"fields":{"slug":"/news/2025-05-08.C4DM-Seminar_Ville_Pulkki"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"EECS & C4DM Seminar: Prof Ville Pulkki","author":"Admin","date":"Thu 08 May 2025"},"html":"<h3>EECS &#x26; C4DM Seminar: Prof Ville Pulkki: Superhearing, Laser Beams, and Windy Yelling: Spatial Audio Oddities Unleashed!</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>EECS Seminar &#x26; Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Prof Ville Pulkki</p>\n<p><strong>Date/time:</strong>  Thursday, 8th May 2025, 2pm</p>\n<p><strong>Location:</strong> 3.02, Peter Landin Building, Mile End Campus, QMUL, E1 4NS</p>\n<p><strong>Zoom:</strong> <a href=\"https://qmul-ac-uk.zoom.us/j/2387202947\">https://qmul-ac-uk.zoom.us/j/2387202947</a></p>\n<h2><b>Title</b>: Superhearing, Laser Beams, and Windy Yelling: Spatial Audio Oddities Unleashed!</h2>\n<p><b>Abstract</b>:\nProf Pulkki will be providing a broad summary his career defining topics:</p>\n<ul>\n<li>Virtual source positioning over multichannel loudspeaker setups</li>\n<li>Reproduction of spatial sound with techniques taking into account bottlenecks in human spatial hearing</li>\n<li>Why do people think that it is hard to yell against the wind, although it is a physical fact that human radiates more sound when yelling upwind than downwind.</li>\n<li>How a request for a small and powerful impulsive source that could be placed inside a violin led to development of impulse response measurements using focused pulsed laser beams.</li>\n<li>Spatial superhearing technologies, where inaudible wave or radiation fields are made audible and localizable to the user. For example, ultrasonic superhearing allows to hear bats flying around, echolocating superhearing enables blind people to perceive reflections of ultrasonic clicks from surrounding environment, and underwater superhearing allows divers to better avoid hazardous boats.</li>\n</ul>\n<p><b>Bio</b>:\nProf Ville Pulkki (Aalto University, Acoustics lab) has been active in the field of acoustics for 30 years, and a professor at Aalto University for 10 years. His doctoral thesis (and project work before that) focused on a technique for positioning virtual sources over multichannel loudspeaker arrays and delved also on perceptual side of the matter, both with subjective tests and binaural auditory models. After the PhD he used the gained knowledge on the resolution of human directional hearing to develop a parametric time-frequency-domain technique for reproduction of sound fields, a version of which has also been standardized recently. In addition to spatial audio, prof Pulkki has done research and teaching on communication acoustics. He co-authored the textbook Communication Acoustics: An Introduction to Speech, Audio and Psychoacoustics (John Wiley &#x26; Sons 2015).</p>\n<p><b>Video</b>: <a href=\"https://www.youtube.com/watch?v=eykZsfMpzXs\">https://www.youtube.com/watch?v=eykZsfMpzXs</a></p>","id":"9a6d8be9-3a39-5eb6-93ea-500691a6d254"},{"fields":{"slug":"/news/2025-04-14.C4DM-at_ICLR_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e1b6924083744f1e06cfc014164defcc/928b9/ICLR2025.png","srcSet":"/static/e1b6924083744f1e06cfc014164defcc/b0268/ICLR2025.png 118w,\n/static/e1b6924083744f1e06cfc014164defcc/f1af1/ICLR2025.png 236w,\n/static/e1b6924083744f1e06cfc014164defcc/928b9/ICLR2025.png 472w","sizes":"(min-width: 472px) 472px, 100vw"},"sources":[{"srcSet":"/static/e1b6924083744f1e06cfc014164defcc/2c82d/ICLR2025.webp 118w,\n/static/e1b6924083744f1e06cfc014164defcc/9bbe7/ICLR2025.webp 236w,\n/static/e1b6924083744f1e06cfc014164defcc/b4137/ICLR2025.webp 472w","type":"image/webp","sizes":"(min-width: 472px) 472px, 100vw"}]},"width":472,"height":472}}},"title":"C4DM at ICLR 2025","author":"Emmanouil Benetos","date":"Mon 14 Apr 2025"},"html":"<p>On 24-28 April, C4DM researchers will participate at the <b><a href=\"https://iclr.cc/\">Thirteenth International Conference on Learning Representations (ICLR 2025)</a></b>, taking place in Singapore. ICLR is the premier gathering of professionals dedicated to the advancement of the branch of artificial intelligence called representation learning, but generally referred to as deep learning.</p>\n<p>C4DM members will be presenting the following papers at the main track of ICLR 2025:</p>\n<ul>\n<li>\n<p><a href=\"https://openreview.net/forum?id=X5hrhgndxW\">Aria-MIDI: A Dataset of MIDI Files for Symbolic Music Modeling</a>, by Louis Bradshaw, Simon Colton</p>\n</li>\n<li>\n<p><a href=\"https://openreview.net/forum?id=iAK9oHp4Zz\">MuPT: A Generative Symbolic Music Pretrained Transformer</a>, by Xingwei Qu, Yuelin Bai, Yinghao Ma, Ziya Zhou, Ka Man Lo, Jiaheng Liu, Ruibin Yuan, Lejun Min, Xueling Liu, Tianyu Zhang, Xinrun Du, Shuyue Guo, Yiming Liang, Yizhi Li, Shangda Wu, Junting Zhou, Tianyu Zheng, Ziyang Ma, Fengze Han, Wei Xue, Gus Xia, Emmanouil Benetos, Xiang Yue, Chenghua Lin, Xu Tan, Wenhao Huang, Jie Fu, Ge Zhang</p>\n</li>\n</ul>\n<p>See you all at ICLR!</p>","id":"367fb186-1809-5f9e-8c89-a644e24c1ee3"}]}}}
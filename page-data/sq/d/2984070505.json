{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/YIFgS8she58","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/Sandler-Artificial-Neuroscience"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png","srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/2fd20/EPSRC.png 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/de391/EPSRC.png 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/d66e1/EPSRC.webp 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/e7160/EPSRC.webp 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/5f169/EPSRC.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Artificial Neuroscience: metrology and engineering for Deep Learning using Linear Algebra","author":"Prof Mark Sandler (PI), Boris Khoruzhenko (CI)","date":null,"link":"https://www.ukri.org/councils/epsrc/"},"html":"","id":"c1314000-fe3b-5175-88a2-f239d861ddad"},{"fields":{"slug":"/projects/Barthet-Netz"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Netz: A Novel XR Musical Instrument","author":"Dr Mathieu Barthet (PI)","date":null,"link":"https://iuk-business-connect.org.uk/programme/icure/"},"html":"","id":"0040b455-65cf-5ea1-a7ce-38ab70570dae"},{"fields":{"slug":"/projects/Benetos-Algorivm"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Project Maestro - AI Musical Analysis Platform","author":"Dr Emmanouil Benetos (PI), Prof Simon Dixon (CI)","date":null,"link":"https://www.ukri.org/councils/innovate-uk/"},"html":"","id":"78c348d8-d78f-5acf-8441-382bc6c4d791"},{"fields":{"slug":"/projects/Benetos-L-Acoustics"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Online speech enhancement in scenarios with low direct-to-reverberant-ratio","author":"Dr Emmanouil Benetos (PI), Dr Aidan Hogg (CI)","date":null,"link":"https://www.l-acoustics.com/"},"html":"","id":"ce65fc6c-7228-5805-a9b2-6e14e5fd5086"},{"fields":{"slug":"/projects/Benetos-Moises"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Enhancing lyrics transcription with open-source architectures and fine-tuning techniques","author":"Dr Emmanouil Benetos (PI)","date":null,"link":"https://moises.ai/"},"html":"","id":"d61977ff-9345-55a4-b85e-44caffa4a7f0"},{"fields":{"slug":"/projects/Dixon-assessment"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Music Performance Assessment and Feedback","author":"Prof Simon Dixon (PI), Dr Emmanouil Benetos (CI)","date":null,"link":null},"html":"","id":"68ee56ea-59b7-58de-a8c9-805c4aba4357"}]},"news":{"nodes":[{"fields":{"slug":"/news/2025-05-21.C4DM-Seminar_Domestic_Data_Streamers"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Domestic Data Streamers","author":"Admin","date":"Wed 21 May 2025"},"html":"<h3>C4DM Seminar: Domestic Data Streamers: Fighting Indifference Towards Data</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>EECS Seminar &#x26; Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Domestic Data Streamers</p>\n<p><strong>Date/time:</strong>  Wednesday, 21st May 2025, 12:30pm</p>\n<p><strong>Location:</strong> G2, Engineering Building, Mile End Campus, QMUL, E1 4NS</p>\n<p><strong>Zoom:</strong> <a href=\"https://qmul-ac-uk.zoom.us/j/9066549084\">https://qmul-ac-uk.zoom.us/j/9066549084</a></p>\n<h2><b>Title</b>: Fighting Indifference Towards Data</h2>\n<p><b>Abstract</b>:\nNumbers, data, and statistics are often seen as objective truths, but they can be misleading. They show just a slice of the whole reality and fail to capture the emotional weight of today’s challenges. So, how do we cultivate empathy when the problems we face are so interconnected, overwhelming, and global in scale? In this talk, we will explore some humble experiments done to overcome this lack of empathy through art, technology, and participatory experiences by Domestic Data Streamers.</p>\n<p><b>Bio</b>:\nDomestic Data Streamers is a research and design studio partnering up with organisations to build change through data, community, and arts. Domestic Data Streamers was founded on the 28th of September of 2013 with a simple idea: That the world couldn’t be understood without numbers, but it wouldn’t be understood with numbers alone. We believe that any meaningful interchange of information between people needs to carry emotions and experiences to create knowledge or change. Since then, we have had the chance to bring this idea all over the world, from schools to prisons, from churches to corporate headquarters, and even to the United Nations General Assembly.<br>\n<a href=\"https://www.domesticstreamers.com/\">https://www.domesticstreamers.com/</a></p>","id":"d1de7d9c-7ef9-5b94-8f03-3b2085ccf9b4"},{"fields":{"slug":"/news/2025-05-12.C4DM-Paper_Award_EvoMUSART"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/b77da60f8aff9865d0a75e25f6644e22/13677/evomusart_best_paper_2025.png","srcSet":"/static/b77da60f8aff9865d0a75e25f6644e22/de391/evomusart_best_paper_2025.png 250w,\n/static/b77da60f8aff9865d0a75e25f6644e22/82c11/evomusart_best_paper_2025.png 500w,\n/static/b77da60f8aff9865d0a75e25f6644e22/13677/evomusart_best_paper_2025.png 1000w","sizes":"(min-width: 1000px) 1000px, 100vw"},"sources":[{"srcSet":"/static/b77da60f8aff9865d0a75e25f6644e22/e7160/evomusart_best_paper_2025.webp 250w,\n/static/b77da60f8aff9865d0a75e25f6644e22/5f169/evomusart_best_paper_2025.webp 500w,\n/static/b77da60f8aff9865d0a75e25f6644e22/3cd29/evomusart_best_paper_2025.webp 1000w","type":"image/webp","sizes":"(min-width: 1000px) 1000px, 100vw"}]},"width":1000,"height":1000}}},"title":"C4DM Paper Award at EvoMUSART 2025","author":"Admin","date":"Mon 12 May 2025"},"html":"<p></p>\n<p>The <a href=\"https://www.evostar.org/2025/evomusart/\">14th International Conference on Artificial Intelligence in Music, Sound, Art and Design (EvoMUSART)</a>, part of Evostar, took place in Trieste, Italy, between 23 and 25 April 2025.</p>\n<p>We are pleased to announce that the following paper authored by C4DM members received the best paper award!</p>\n<p><a href=\"https://arxiv.org/abs/2501.17759\">Yin-Yang: Developing Motifs With Long-Term Structure And Controllability</a>, by Keshav Bhandari, Geraint A. Wiggins, Simon Colton</p>\n<p>Yin-Yang is a neuro-symbolic framework that combines three transformer models to generate structured melodies with coherent long-term development, while allowing user control over musical themes and variations.</p>","id":"ee233f18-26bd-57e5-a96e-0ddd0d31228c"},{"fields":{"slug":"/news/2025-05-08.C4DM-Seminar_Ville_Pulkki"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"EECS & C4DM Seminar: Prof Ville Pulkki","author":"Admin","date":"Thu 08 May 2025"},"html":"<h3>EECS &#x26; C4DM Seminar: Prof Ville Pulkki: Superhearing, Laser Beams, and Windy Yelling: Spatial Audio Oddities Unleashed!</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>EECS Seminar &#x26; Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Prof Ville Pulkki</p>\n<p><strong>Date/time:</strong>  Thursday, 8th May 2025, 2pm</p>\n<p><strong>Location:</strong> 3.02, Peter Landin Building, Mile End Campus, QMUL, E1 4NS</p>\n<p><strong>Zoom:</strong> <a href=\"https://qmul-ac-uk.zoom.us/j/2387202947\">https://qmul-ac-uk.zoom.us/j/2387202947</a></p>\n<h2><b>Title</b>: Superhearing, Laser Beams, and Windy Yelling: Spatial Audio Oddities Unleashed!</h2>\n<p><b>Abstract</b>:\nProf Pulkki will be providing a broad summary his career defining topics:</p>\n<ul>\n<li>Virtual source positioning over multichannel loudspeaker setups</li>\n<li>Reproduction of spatial sound with techniques taking into account bottlenecks in human spatial hearing</li>\n<li>Why do people think that it is hard to yell against the wind, although it is a physical fact that human radiates more sound when yelling upwind than downwind.</li>\n<li>How a request for a small and powerful impulsive source that could be placed inside a violin led to development of impulse response measurements using focused pulsed laser beams.</li>\n<li>Spatial superhearing technologies, where inaudible wave or radiation fields are made audible and localizable to the user. For example, ultrasonic superhearing allows to hear bats flying around, echolocating superhearing enables blind people to perceive reflections of ultrasonic clicks from surrounding environment, and underwater superhearing allows divers to better avoid hazardous boats.</li>\n</ul>\n<p><b>Bio</b>:\nProf Ville Pulkki (Aalto University, Acoustics lab) has been active in the field of acoustics for 30 years, and a professor at Aalto University for 10 years. His doctoral thesis (and project work before that) focused on a technique for positioning virtual sources over multichannel loudspeaker arrays and delved also on perceptual side of the matter, both with subjective tests and binaural auditory models. After the PhD he used the gained knowledge on the resolution of human directional hearing to develop a parametric time-frequency-domain technique for reproduction of sound fields, a version of which has also been standardized recently. In addition to spatial audio, prof Pulkki has done research and teaching on communication acoustics. He co-authored the textbook Communication Acoustics: An Introduction to Speech, Audio and Psychoacoustics (John Wiley &#x26; Sons 2015).</p>\n<p><b>Video</b>: <a href=\"https://www.youtube.com/watch?v=eykZsfMpzXs\">https://www.youtube.com/watch?v=eykZsfMpzXs</a></p>","id":"9a6d8be9-3a39-5eb6-93ea-500691a6d254"},{"fields":{"slug":"/news/2025-04-14.C4DM-at_ICLR_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e1b6924083744f1e06cfc014164defcc/928b9/ICLR2025.png","srcSet":"/static/e1b6924083744f1e06cfc014164defcc/b0268/ICLR2025.png 118w,\n/static/e1b6924083744f1e06cfc014164defcc/f1af1/ICLR2025.png 236w,\n/static/e1b6924083744f1e06cfc014164defcc/928b9/ICLR2025.png 472w","sizes":"(min-width: 472px) 472px, 100vw"},"sources":[{"srcSet":"/static/e1b6924083744f1e06cfc014164defcc/2c82d/ICLR2025.webp 118w,\n/static/e1b6924083744f1e06cfc014164defcc/9bbe7/ICLR2025.webp 236w,\n/static/e1b6924083744f1e06cfc014164defcc/b4137/ICLR2025.webp 472w","type":"image/webp","sizes":"(min-width: 472px) 472px, 100vw"}]},"width":472,"height":472}}},"title":"C4DM at ICLR 2025","author":"Emmanouil Benetos","date":"Mon 14 Apr 2025"},"html":"<p>On 24-28 April, C4DM researchers will participate at the <b><a href=\"https://iclr.cc/\">Thirteenth International Conference on Learning Representations (ICLR 2025)</a></b>, taking place in Singapore. ICLR is the premier gathering of professionals dedicated to the advancement of the branch of artificial intelligence called representation learning, but generally referred to as deep learning.</p>\n<p>C4DM members will be presenting the following papers at the main track of ICLR 2025:</p>\n<ul>\n<li>\n<p><a href=\"https://openreview.net/forum?id=X5hrhgndxW\">Aria-MIDI: A Dataset of MIDI Files for Symbolic Music Modeling</a>, by Louis Bradshaw, Simon Colton</p>\n</li>\n<li>\n<p><a href=\"https://openreview.net/forum?id=iAK9oHp4Zz\">MuPT: A Generative Symbolic Music Pretrained Transformer</a>, by Xingwei Qu, Yuelin Bai, Yinghao Ma, Ziya Zhou, Ka Man Lo, Jiaheng Liu, Ruibin Yuan, Lejun Min, Xueling Liu, Tianyu Zhang, Xinrun Du, Shuyue Guo, Yiming Liang, Yizhi Li, Shangda Wu, Junting Zhou, Tianyu Zheng, Ziyang Ma, Fengze Han, Wei Xue, Gus Xia, Emmanouil Benetos, Xiang Yue, Chenghua Lin, Xu Tan, Wenhao Huang, Jie Fu, Ge Zhang</p>\n</li>\n</ul>\n<p>See you all at ICLR!</p>","id":"367fb186-1809-5f9e-8c89-a644e24c1ee3"},{"fields":{"slug":"/news/2025-04-09.C4DM-Seminar_Alexander_Hawkins"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#282828","images":{"fallback":{"src":"/static/d8ef9a548b6614090478565b12a693cb/7e194/alexander-hawkins.jpg","srcSet":"/static/d8ef9a548b6614090478565b12a693cb/cd18a/alexander-hawkins.jpg 450w,\n/static/d8ef9a548b6614090478565b12a693cb/0a45a/alexander-hawkins.jpg 900w,\n/static/d8ef9a548b6614090478565b12a693cb/7e194/alexander-hawkins.jpg 1800w","sizes":"(min-width: 1800px) 1800px, 100vw"},"sources":[{"srcSet":"/static/d8ef9a548b6614090478565b12a693cb/2890f/alexander-hawkins.webp 450w,\n/static/d8ef9a548b6614090478565b12a693cb/3987a/alexander-hawkins.webp 900w,\n/static/d8ef9a548b6614090478565b12a693cb/b46b0/alexander-hawkins.webp 1800w","type":"image/webp","sizes":"(min-width: 1800px) 1800px, 100vw"}]},"width":1800,"height":1800}}},"title":"C4DM & AIL Seminar: Alexander Hawkins","author":"Admin","date":"Wed 09 Apr 2025"},"html":"<h3>C4DM &#x26; AIL Seminar: Alexander Hawkins: Some Questions from the Field: A Performer in Conversation with New Instruments</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Imperial College London, Dyson School of Design Engineering</h4>\n<h4>Centre for Digital Music Seminar Series &#x26; Augmented Instruments Lab Open Seminars</h4>\n<p><strong>Seminar by:</strong> <a href=\"https://alexanderhawkinsmusic.com/\">Alexander Hawkins</a></p>\n<p><strong>Date/time:</strong>  Wednesday, 9th April 2025, 12:30pm</p>\n<p><strong>Location:</strong> Performance Lab, Engineering Building, Mile End Campus, QMUL, E1 4NS</p>\n<h2><b>Title</b>: Some Questions from the Field: A Performer in Conversation with New Instruments</h2>\n<p><b>Abstract</b>:\nAlthough I have only ever worked as a professional musician, I did obtain a PhD in law before beginning my career. This was in a somewhat liminal area of the subject: my university called it ‘law’, but others might have called it ‘sociology’, others ‘criminology’, and so on. I also taught many ‘law’ undergraduates during my doctoral studies: but the philosophers, historians, economists, statisticians, and others would all have recognised much of the territory as their own. An ‘augmented instruments’ group may well bear out something similar, in terms of a variety of disciplinary approaches, and we are probably all familiar with the fascinating insights this variegation can yield, as well as the frustrating moments of somehow ‘talking past’ each other.</p>\n<p>In my musical career, there is possibly something similar going on. I am fortunate to work regularly with artists of wildly different aesthetics, in contexts which are sometimes fully improvised, and sometimes fully notated; which sometimes look like ‘jazz’, sometimes like ‘classical’ musics, sometimes like ‘traditional’ musics, sometimes like ‘electronic’ musics, and so on. ‘Idiom’ and ‘innovation’ are therefore ideas on which I reflect a great deal. I would like to think that I spend a lot of time chasing sounds which I’ve never heard, but at the same time, have to acknowledge my instrumentalist’s obsession with control and technique.</p>\n<p>In this seminar, I would like to take a ‘liminal’ stance, and think out loud about what augmented instruments might mean in the context of my own practice as a composer-performer: as a way of teasing out some conceptual ideas, as well, perhaps, as some of the insecurities and questions of the musician who is fascinated by, but not necessarily fluent in, the possibilities.</p>\n<p><b>Bio</b>:\nAlexander Hawkins is a composer, pianist, organist, and bandleader who is ‘unlike anything else in modern creative music’. Regarded as one of his generation’s most innovative thinkers, his own unique soundworld is shaped by a profound fascination with composition and structure, alongside a love of chance and open forms.</p>\n<p>His writing has been said to represent ‘a fundamental reassertion of composition within improvised music’, and his voice one of the ‘most vividly distinctive...in modern jazz’. As a pianist, he has been described as ‘remarkable...possessing staggering technical ability and a fecund imagination.’ Concerning his organ playing, critic Brian Morton recently commented that ‘[t]he most interesting Hammond player of the last decade and more, [Hawkins] has already extended what can be done on the instrument.’</p>\n<p>Hawkins is a frequent solo performer, and also appears in groupings ranging from duo through to large ensembles. He can be heard live and on record with a vast array of contemporary leaders of all generations, including the likes of Anthony Braxton, Joe McPhee, Nicole Mitchell, Jonny Greenwood, and many others. He has been widely commissioned, by the likes of the BBC, and festivals such as the London and Berlin Jazz Festivals. He was named ‘Instrumentalist of the Year’ in the 2016 Parliamentary Jazz Awards. In 2018, he was elected a fellow of the Civitella Ranieri.</p>","id":"e0138643-cc04-531e-88fe-2199cf0b9b35"},{"fields":{"slug":"/news/2025-04-09.brain-turns_sound_into_music"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/a71fd6e7f5a7df7ab88ff0f1853f09d8/f4e58/brain-turns-sound-into-music.png","srcSet":"/static/a71fd6e7f5a7df7ab88ff0f1853f09d8/cbc40/brain-turns-sound-into-music.png 260w,\n/static/a71fd6e7f5a7df7ab88ff0f1853f09d8/e1568/brain-turns-sound-into-music.png 521w,\n/static/a71fd6e7f5a7df7ab88ff0f1853f09d8/f4e58/brain-turns-sound-into-music.png 1041w","sizes":"(min-width: 1041px) 1041px, 100vw"},"sources":[{"srcSet":"/static/a71fd6e7f5a7df7ab88ff0f1853f09d8/0fcc6/brain-turns-sound-into-music.webp 260w,\n/static/a71fd6e7f5a7df7ab88ff0f1853f09d8/05f5c/brain-turns-sound-into-music.webp 521w,\n/static/a71fd6e7f5a7df7ab88ff0f1853f09d8/04d6c/brain-turns-sound-into-music.webp 1041w","type":"image/webp","sizes":"(min-width: 1041px) 1041px, 100vw"}]},"width":1041,"height":1041}}},"title":"Groundbreaking research reveals how the brain turns sound into music","author":"Emmanouil Benetos","date":"Wed 09 Apr 2025"},"html":"<p><a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/iroman/\">Dr. Iran R. Roman</a>, Lecturer of Artificial Intelligence at the School of Electronic Engineering and Computer Science, and a group of external collaborators have revealed a groundbreaking theory explaining how the brain transforms sound into the human experience of music.</p>\n<p>Read the full story at: <a href=\"https://www.qmul.ac.uk/eecs/news-and-events/news/items/groundbreaking-research-reveals-how-the-brain-turns-sound-into-music.html\">https://www.qmul.ac.uk/eecs/news-and-events/news/items/groundbreaking-research-reveals-how-the-brain-turns-sound-into-music.html</a></p>","id":"36b76dc6-a54b-5ecd-b724-cd2632e4ee08"}]}}}
{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/YIFgS8she58","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/Benetos-Algorivm"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Project Maestro - AI Musical Analysis Platform","author":"Dr Emmanouil Benetos (PI), Prof Simon Dixon (CI)","date":null,"link":"https://www.ukri.org/councils/innovate-uk/"},"html":"","id":"78c348d8-d78f-5acf-8441-382bc6c4d791"},{"fields":{"slug":"/projects/Benetos-L-Acoustics"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Online speech enhancement in scenarios with low direct-to-reverberant-ratio","author":"Dr Emmanouil Benetos (PI), Dr Aidan Hogg (CI)","date":null,"link":"https://www.l-acoustics.com/"},"html":"","id":"ce65fc6c-7228-5805-a9b2-6e14e5fd5086"},{"fields":{"slug":"/projects/Benetos-Moises"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Enhancing lyrics transcription with open-source architectures and fine-tuning techniques","author":"Dr Emmanouil Benetos (PI)","date":null,"link":"https://moises.ai/"},"html":"","id":"d61977ff-9345-55a4-b85e-44caffa4a7f0"},{"fields":{"slug":"/projects/Dixon-assessment"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Music Performance Assessment and Feedback","author":"Prof Simon Dixon (PI), Dr Emmanouil Benetos (CI)","date":null,"link":null},"html":"","id":"68ee56ea-59b7-58de-a8c9-805c4aba4357"},{"fields":{"slug":"/projects/Dixon-guitar-transcription"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"High Resolution Guitar Transcription","author":"Prof Simon Dixon (PI)","date":null,"link":null},"html":"","id":"4dee84b0-faa4-549a-a883-d4de0a21e205"},{"fields":{"slug":"/projects/Fazekas-SmartEQ-AIM"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png","srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/4b686/aimcdt.png 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/d2213/aimcdt.png 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png 615w","sizes":"(min-width: 615px) 615px, 100vw"},"sources":[{"srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/b2942/aimcdt.webp 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/46581/aimcdt.webp 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/fa942/aimcdt.webp 615w","type":"image/webp","sizes":"(min-width: 615px) 615px, 100vw"}]},"width":615,"height":615}}},"title":"Smart EQ: Personalising Audio with Context-aware AI using Listener Preferences and Psychological Factors","author":"Dr György Fazekas (PI), Dr Charalampos Saitis (CI)","date":null,"link":null},"html":"","id":"8df63b90-0094-5360-a9c8-6d3c758d558e"}]},"news":{"nodes":[{"fields":{"slug":"/news/2024-03-12.C4DM-Seminar_Manvi_agarwal"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Manvi Agarwal","author":"admin","date":"Tue 03 Dec 2024"},"html":"<h3>C4DM Seminar: Manvi Agarwal: Fast Structure-informed Positional Encoding for Music Generation</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nManvi Agarwal (Télécom Paris)</p>\n<p><strong>Date/time:  Tuesday, 3rd December 2024, 2pm</strong></p>\n<p>**Location: Room G2, Engineering Building, Mile End Campus, QMUL, E1 4NS **\nZoom: <a href=\"https://qmul-ac-uk.zoom.us/j/2387202947\">https://qmul-ac-uk.zoom.us/j/2387202947</a></p>\n<h2><b>Title</b>: Fast Structure-informed Positional Encoding for Music Generation</h2>\n<p><b>Abstract</b>:  Music generated by deep learning methods often suffers from a lack of coherence and long-term organization. Over the years, several solutions have been proposed to help generative music architectures capture multi-scale hierarchical structure, which is a distinctive feature of music signals. The focus of my talk is the use of musically-relevant structural information to improve music Transformers. Specifically, I will present structure-informed positional encoding as a way to achieve superior music generation performance with low resource requirements. I will put forward two perspectives - an empirical approach exploring different designs for incorporating structural information in positional encoding and a theoretical approach using kernel approximations for improving the generative performance and computational complexity of such designs. In this way, I hope to underline the strengths of well-designed priors in dealing with some of the challenges facing music generation systems.</p>\n<p><b>Bio</b>: Manvi Agarwal is doing her PhD under the supervision of Dr. Changhong Wang and Prof. Gaël Richard in the ADASP (Audio Data Analysis and Signal Processing) group, Télécom Paris, Institut Polytechnique de Paris, France, supported by the ERC-funded Hi-Audio project. Her PhD research looks at how inductive biases can be introduced into Transformers to improve their modelling capabilities on music data. Broadly, she is interested in how sequence-based learning works and how our understanding of this process in different deep learning architectures can help us make these architectures perform better, especially in low-resource settings.</p>","id":"3d7650a4-9982-5956-8b1c-5b6641c3f20c"},{"fields":{"slug":"/news/2024-11-16.ISMIR-2024_paper_awards"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/23c0c3bffae6b50f360808e214b5c26b/f054e/ismir2024logo.png","srcSet":"/static/23c0c3bffae6b50f360808e214b5c26b/a49f4/ismir2024logo.png 188w,\n/static/23c0c3bffae6b50f360808e214b5c26b/98376/ismir2024logo.png 375w,\n/static/23c0c3bffae6b50f360808e214b5c26b/f054e/ismir2024logo.png 750w","sizes":"(min-width: 750px) 750px, 100vw"},"sources":[{"srcSet":"/static/23c0c3bffae6b50f360808e214b5c26b/e6874/ismir2024logo.webp 188w,\n/static/23c0c3bffae6b50f360808e214b5c26b/e3305/ismir2024logo.webp 375w,\n/static/23c0c3bffae6b50f360808e214b5c26b/4f03f/ismir2024logo.webp 750w","type":"image/webp","sizes":"(min-width: 750px) 750px, 100vw"}]},"width":750,"height":750}}},"title":"ISMIR 2024 paper awards for C4DM","author":"Admin","date":"Sat 16 Nov 2024"},"html":"<p></p>\n<p>The <b><a href=\"https://ismir2024.ismir.net/\">25th International Society for Music Information Retrieval Conference (ISMIR 2024)</a></b> has just taken place, and we are pleased to announce that the following works authored or coauthored by C4DM members received best paper awards at the conference:</p>\n<ul>\n<li>\n<p><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/98705\">MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models</a> (Benno Weck, Ilaria Manco, Emmanouil Benetos, Elio Quinton, George Fazekas, Dmitry Bogdanov)</p>\n</li>\n<li>\n<p><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/98593\">ST-ITO: Controlling audio effects for style transfer with inference-time optimization</a> (Christian J. Steinmetz, Shubhr Singh, Marco Comunità, Ilias Ibnyahya, Shanxin Yuan, Emmanouil Benetos, Joshua D. Reiss)</p>\n</li>\n</ul>\n<p>Congratulations to all! You can explore C4DM's contributions for ISMIR 2024 at: <a href=\"https://www.c4dm.eecs.qmul.ac.uk/news/2024-10-28.C4DM-at_ISMIR_2024/\">https://www.c4dm.eecs.qmul.ac.uk/news/2024-10-28.C4DM-at_ISMIR_2024/</a></p>","id":"0253ca26-a995-5854-80d7-a713b2b6fa3a"},{"fields":{"slug":"/news/2024-11-12.PhD-call-2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/f64b6/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/d59af/c4dm.png 431w,\n/static/c138fbce66e709a3f503405435de2f2c/325b2/c4dm.png 862w,\n/static/c138fbce66e709a3f503405435de2f2c/f64b6/c4dm.png 1723w","sizes":"(min-width: 1723px) 1723px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/4b79d/c4dm.webp 431w,\n/static/c138fbce66e709a3f503405435de2f2c/562bc/c4dm.webp 862w,\n/static/c138fbce66e709a3f503405435de2f2c/c4c1b/c4dm.webp 1723w","type":"image/webp","sizes":"(min-width: 1723px) 1723px, 100vw"}]},"width":1723,"height":1723}}},"title":"PhD Studentships at the Centre for Digital Music - Autumn 2025 start","author":"Admin","date":"Tue 12 Nov 2024"},"html":"<p>The Centre for Digital Music at Queen Mary University of London is inviting applications for PhD study for Autumn 2025 start across various funding schemes. Below are suggested PhD topics offered by academics; interested applicants can apply for a PhD under one of those topics, or can propose their own topic. In all cases, prospective applicants are strongly encouraged to contact academics at C4DM to informally discuss prospective research topics.</p>\n<p>Opportunities include internally and externally funded positions for PhD projects to start in Autumn 2025. It is also possible to apply as a self-funded student or with funding from another source. Applicants can apply for a 3-year PhD degree in Computer Science or Electronic Engineering, or for a 4-year <a href=\"https://www.aim.qmul.ac.uk/apply/\">PhD in AI and Music</a>. Studentship opportunities include:</p>\n<ul>\n<li>\n<p>One industry funded PhD position in collaboration with Steinberg Media Technologies GmbH (applicants from all nationalities, Autumn 2025 start)</p>\n</li>\n<li>\n<p><a href=\"https://www.seresearch.qmul.ac.uk/content/phdstudents/files/S%26E%20UnderRep%20studentships%202025_Public.pdf\">S&#x26;E Doctoral Research Studentships for Underrepresented Groups</a> (UK home applicants, Autumn 2025 start, 6 positions funded across the Faculty of Science &#x26; Engineering)</p>\n</li>\n<li>\n<p><a href=\"https://www.qmul.ac.uk/eecs/phd/phd-studentships/csc-phd-studentships-in-electronic-engineering-and-computer-science/\">CSC PhD Studentships in Electronic Engineering and Computer Science</a> (Autumn 2025 start, Chinese applicants, up to 8 nominations allocated for the Centre for Digital Music)</p>\n</li>\n<li>\n<p><a href=\"https://www.qmul.ac.uk/postgraduate/research/funding_phd/studentships/\">International PhD Funding Schemes</a> (Autumn 2025 start, numerous international funding agencies)</p>\n</li>\n</ul>\n<hr>\n<p><strong>AI Models of Music Understanding</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/sdixon/\">Simon Dixon</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, CSC PhD Studentships, International PhD Funding Scheme</p>\n<p>Music information retrieval (MIR) applies computing and engineering technologies to musical data to satisfy users' information needs. This topic involves the application of artificial intelligence technologies to the processing of music, either in audio or symbolic (score, MIDI) form. The application could be e.g. for software to enhance the listening experience, for music education, for musical practice or for the scientific study of music. Examples of topics of particular interest are automatic transcription of multi-instrumental music, providing feedback to music learners, incorporation of musical knowledge into data-driven deep learning approaches, and tracing the transmission of musical styles, ideas or influences across time or locations.</p>\n<p>It is intentional that this topic description is very general, but it is expected that applicants choose your own specific project within this broad area of research, according to your interests and experience. The research proposal should define the scope of the project, the relationship to the state of the art, the data and methods that you plan to use, and the expected outputs and means of evaluation.</p>\n<hr>\n<p><strong>AI-Powered Audio Loop Generation for Assistive Music Production</strong> (in collaboration with Steinberg Media Technologies GmbH)</p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/gfazekas\">George Fazekas</a></p>\n<p>Eligible funding schemes: Industry funded PhD topic in collaboration with Steinberg Media Technologies GmbH (applicants from all nationalities are eligible)</p>\n<p>This research explores the use of controllable deep learning models for generating high-quality audio loops tailored to musicians' needs. By focusing on audio tokenisation and representation learning techniques, the project aims to create reusable loops, such as drum patterns, basslines and synth textures, that seamlessly integrate into music production workflows. Unlike tools that generate full compositions, this approach priorities modular, user-customisable components, enabling artists to adapt loops for specific creative goals. The work also emphasises real-time usability, with plans to integrate the model into digital audio workstations (DAWs). By advancing tokenisation methods and intuitive controls, the research seeks to enhance AI's role in modern music production. There is scope to explore different tokenisation techniques and different modelling approaches including, transformers, diffusion and consistency models, as well as retrieval augmented generation. Key challenges include ensuring high audio quality across diverse loop types, balancing customisable controls with user-friendly simplicity, and optimising the model for low-latency, efficient performance in real-time DAW environments. The research should also include elements concerning the evaluation of audio and musical qualities of the generated output and the usability/controllability of the model.</p>\n<hr>\n<p><strong>Audio-visual sensing for machine intelligence</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/lwang/\">Lin Wang</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, CSC PhD Studentships, International PhD Funding Scheme</p>\n<p>The project aims to develop novel audio-visual signal processing and machine learning algorithms that help improve machine intelligence and autonomy in an unknown environment, and to understand human behaviours interacting with robots. The project will investigate the application of AI algorithms for audio-visual scene analysis in real-life environments. One example is to employ multimodal sensors e.g. microphones and cameras, for analysing various sources and events present in the acoustic environment. Tasks to be considered include audio-visual source separation, localization/tracking, audio-visual event detection/recognition, audio-visual scene understanding.  </p>\n<hr>\n<p><strong>Automated machine learning for music understanding</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/ebenetos/\">Emmanouil Benetos</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, CSC PhD Studentships, International PhD Funding Scheme</p>\n<p>The field of music information retrieval (MIR) has been growing for more than 20 years, with recent advances in deep learning having revolutionised the way machines can make sense of music data. At the same time, research in the field is still constrained by laborious tasks involving data preparation, feature extraction, model selection, architecture optimisation, hyperparameter optimisation, and transfer learning, to name but a few. Some of the model and experimental design choices made by MIR researchers also reflect their own biases.</p>\n<p>Inspired by recent developments in machine learning and automation, this PhD project will investigate and develop automated machine learning methods which can be applied at any stage in the MIR pipeline as to build music understanding models ready for deployment across a wide range of tasks. This project will also compare the automated decisions made on every step in the MIR pipeline, as compared with manual model design choices made by researchers. The successful candidate will investigate, propose and develop novel deep learning methods for automating music understanding, resulting in models that can accelerate MIR research and contribute to the democratisation of AI.</p>\n<hr>\n<p><strong>Dynamical Systems Analysis and Hebbian Learning for Advanced Time-Series Processing</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/iroman/\">Iran R. Roman</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, International PhD Funding Scheme</p>\n<p>This research aims to advance neural networks for time-series processing by applying dynamical systems theory and Hebbian learning, with a focus on emulating biological mechanisms that recognize and retain temporal patterns. We intend to develop efficient, adaptable architectures that minimize data dependency, utilizing low-dimensional circuits derived from dynamic analyses of large-scale neural activities. By converting complex neural states into simpler mathematical forms, we enhance both the efficiency and adaptability of processing time-series data.</p>\n<p>The PhD project will develop state-of-the-art neural network models for applications such as musical rhythm, speech processing, and time-series forecasting. Using dynamical systems theory, we will dissect these models to understand the underlying dynamics that facilitate synchronization and pattern generation, identifying essential lower-dimensional circuits. Comparative analysis with biological data from humans and primates will be used to inform the design of biologically inspired models.</p>\n<p>Additionally, the PhD student will implement Hebbian learning to create networks capable of few-shot and continual learning, thereby reducing the dependency on extensive datasets. This strategy will lead to robust, data-efficient models that offer deeper insights into both artificial and biological time-series processing mechanisms.</p>\n<hr>\n<p><strong>Exploiting Domain-Knowledge in Music Representation Learning</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/gfazekas\">George Fazekas</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, CSC PhD Studentships, International PhD Funding Scheme</p>\n<p>The field of music representation learning aims to transform complex musical data into latent representations that are useful for tasks such as music classification, mood detection, music recommendation or generation. Despite recent advances in deep learning, many models rely purely on data-driven approaches and overlook domain-specific musical structures such as rhythm, melody and harmony.</p>\n<p>This PhD project will investigate the integration of domain knowledge into music representation learning to enhance model interpretability and performance. Embedding music theoretical knowledge, structural hierarchies or genre-specific knowledge, the research should improve learning efficiency and provide richer representations that are more explainable and interpretable. The research has the option to explore various techniques, including incorporating symbolic representations, develop new methodologies for better utilisation of inductive biases, or leveraging musical ontologies to bridge the gap between data-driven models and the structured knowledge inherent in music theory.</p>\n<p>There is flexibility in the approach taken, but the candidate should identify and outline a specific method within music analysis, production or generation. Special attention should be devoted to Ethical AI, i.e., it is expected that the proposed approach will not only improve music representation but allow for the reduction data biases or improve attribution of authorship to respect copyright.</p>\n<hr>\n<p><strong>Generative sound-based music</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/axambosedo/\">Anna Xambó</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, CSC PhD Studentships, International PhD Funding Scheme</p>\n<p>This PhD research explores the potential of generative techniques in sound-based music, where sound itself—rather than traditional musical notes—serves as the core building block of composition. By utilising generative learning procedures, the study will develop systems capable of creating novel soundscapes and site-specific sound art experiences. It is particularly relevant for students with expertise in computing and music, as it combines advanced algorithmic design with artistic sound manipulation. Through the integration of neural networks and sound synthesis methods, this research will examine how machines can generate, transform, and structure sounds into cohesive musical works from a human-centred perspective. This approach contributes to various fields, including acoustic ecology, sound design, interactive music systems, and human-computer interaction.</p>\n<hr>\n<p><strong>Interpretable AI for Sound Event Detection and Classification</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/lwang/\">Lin Wang</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, CSC PhD Studentships, International PhD Funding Scheme</p>\n<p>Deep-learning models have revolutionized state-of-the-art technologies for environmental sound recognition motivated by their applications in healthcare, smart homes, or urban planning. However, most of the systems used for these applications are based on black boxes and, therefore, cannot be inspected, so the rationale behind their decisions is obscure. Despite recent advances, there is still a lack of research in interpretable machine learning in the audio domain. Applicants are invited to develop ideas to reduce this gap by proposing interpretable deep-learning models for automatic sound event detection and classification in real-life environments.</p>\n<hr>\n<p><strong>Machine learning models for musical timbre</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/csaitis\">Charalampos Saitis</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, CSC PhD Studentships, International PhD Funding Scheme</p>\n<p>Music information retrieval tasks related to timbre (e.g., instrument identification, playing technique detection) have historically been under-researched, partly due to lack of available -and annotated- data, including a lack of community consensus around instrument and technique taxonomies. In the context of music similarity, which extends to the topic of timbre similarity, metric learning methods are commonly used to learn distances from human judgements. There is extensive work on using metric learning with hand-crafted features, but such representations can be limiting. Conversely, deep metric learning methods attempt to learn distances directly from data, promising a viable alternative. Despite some limited adoption of deep metric learning for specific music similarity tasks, related efforts to learn timbre similarity, or automatically construct taxonomical structures for timbre, are currently lacking. This project will investigate, propose, and develop machine learning models, including curating a new sizable dataset, that can learn discriminative representations of timbre through supervised, semi-supervised, and self-supervised learning paradigms of similarity and categorisation. Such models will enable a wide range of applications for computational music understanding (e.g., foundation models for music) and generation/creativity (e.g., neural audio synthesis). Candidates should have experience in at least one of the following: music informatics, machine listening, metric learning.</p>\n<hr>\n<p><strong>Scalable Acoustic Imaging Using Sparse Microphone Arrays for Embedded Devices</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/iroman/\">Iran R. Roman</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, International PhD Funding Scheme</p>\n<p>This project will fundamentally transform existing acoustic imaging technologies by developing scalable and adaptable machine learning algorithms aimed at delivering precise spatial sound representations, while requiring minimal hardware. The project focuses on harnessing the potential of embedded microphone arrays, using as few as two or four channels, to create efficient algorithms integrated directly into chips or mobile devices. These algorithms will be designed to accurately localize sound sources and decode semantic information from the sounds, such as identifying the type of sound-producing entity.</p>\n<p>The project will entail the development of efficient machine learning models that effectively process both simulated and real sound recordings to not only pinpoint the exact location of sound sources but also extract rich semantic content. This capability will enable compact and versatile acoustic cameras. These cameras will be integrated into various devices, such as smartphones, AR glasses, and security doorbells, enhancing functionalities such as video object tracking by localizing sounds outside the visual field, improving automatic speech recognition systems by providing spatial audio cues to differentiate speakers, and augmenting reality applications by synchronizing virtual sound sources with physical environments.</p>\n<p>Students will engage in rigorous algorithmic design, leveraging both theoretical and practical aspects of acoustic signal processing, machine learning, and spatial audio techniques. Comparative analyses with existing technologies will help in fine-tuning the algorithms to achieve high accuracy and efficiency. This research aims to pave the way for next-generation multimodal technologies that enhance the sensory capabilities of everyday devices through advanced sound processing.</p>\n<hr>\n<p><strong>Sound-based DIY approaches to creative AI</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/axambosedo/\">Anna Xambó</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, CSC PhD Studentships, International PhD Funding Scheme</p>\n<p>I am also open to discussing other projects related to creative AI and DIY projects that aim to improve societal aspects of unprivileged communities through the use of sound-based music and acoustic ecology systems.</p>\n<hr>\n<p><strong>Understanding Neural Audio Models with Artificial Intelligence and Linear Algebra</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/msandler/\">Mark Sandler</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, CSC PhD Studentships, International PhD Funding Scheme</p>\n<p>Since ~2016 most research in Digital Music and Digital Audio has adopted Deep Learning techniques. These have brought performance improvements in applications like Music Source Separation, Automatic Music Transcription and so on. This is good, but on the downside, the models get larger, they consume increasingly large amounts of power for training and inference, require more data and become less understandable and explainable. These issues underpin the research in this PhD.</p>\n<p>A fundamental building block in DL is Matrix (or Linear) Algebra. Through training, each each layer’s weight matrix is progressively modified to reduce the training error. By examining these matrices during training, DL models can be compactly engineered to learn faster and more efficiently.</p>\n<p>Research will start by exploring the learning dynamics of established Music Source Separation models. Using this knowledge, we can intelligently prune the models, using Low Rank approximations of weight matrices. We will explore what happens when Low Rank is imposed as a training constraint. Is the model better trained? Is it easier and cheaper to train? Next, the work shifts either to other Neural Audio applications, or to applying Mechanistic Interpretability, which reveals the hidden, innermost structures that emerge in trained Neural Networks.</p>\n<hr>\n<p><strong>Using machine learning to enhance simulation of sound phenomena</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/jreiss/\">Josh Reiss</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, CSC PhD Studentships, International PhD Funding Scheme</p>\n<p>Physical and signal-based models of sound generating phenomena are widely used in noise and vibration modelling, sound effects, and digital musical instruments. This project will explore machine learning from sample libraries for improving the models and their design process.</p>\n<p>Not only can optimisation approaches be used to select parameter values such that the output of the model matches samples, the accuracy of such an approach will give us insight into the limitations of a model. It also provides the opportunity to explore the overall performance of different modelling approaches, and to find out whether a model can be generalised to cover a large number of sounds, with a relatively small number of exposed parameters.</p>\n<p>Existing models will be used, with parameter optimisation based on gradient descent. Performance will be compared against recent neural synthesis approaches that often provide high quality synthesis but lack intuitive controls or a physical basis. It will also seek to measure the extent to which entire sample libraries could be replaced by a small number of models with parameters set to match the samples in the library.</p>\n<p>The project can be tailored to the skills of the researcher, and has the potential for high impact.</p>","id":"5518a436-20d3-52fe-aea2-d289e222dce7"},{"fields":{"slug":"/news/2024-11-07.new-study_music_gendering"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/5fab8e440cce116b3b1f14f79fa31217/24f4c/gendered_toys.jpg","srcSet":"/static/5fab8e440cce116b3b1f14f79fa31217/68974/gendered_toys.jpg 256w,\n/static/5fab8e440cce116b3b1f14f79fa31217/3c367/gendered_toys.jpg 512w,\n/static/5fab8e440cce116b3b1f14f79fa31217/24f4c/gendered_toys.jpg 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/5fab8e440cce116b3b1f14f79fa31217/22bfc/gendered_toys.webp 256w,\n/static/5fab8e440cce116b3b1f14f79fa31217/d689f/gendered_toys.webp 512w,\n/static/5fab8e440cce116b3b1f14f79fa31217/67ded/gendered_toys.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"title":"Melodious or distorted? How music reinforces gender stereotypes in kids' toys commercials","author":"Charalampos Saitis","date":"Thu 07 Nov 2024"},"html":"<p>The music in toy commercials might be shaping how children see themselves and the world! A <a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0311876\">new study</a>, published in PLOS ONE, from Queen Mary University of London reveals that sound in toy ads isn’t just background noise — it’s actively reinforcing gender stereotypes.</p>\n<p>AIM PhD student Luca Marinelli, supervised by Dr Charalampos Saitis (<a href=\"https://www.c4dm.eecs.qmul.ac.uk/\">C4DM</a> - <a href=\"https://comma.eecs.qmul.ac.uk/\">COMMA</a>) and in collaboration with Prof Petra Lucht (Center for Interdisciplinary Women’s and Gender Studies at TU Berlin) found that ads targeting boys often feature intense, abrasive sounds, while those for girls use softer, harmonious music. These choices subtly reinforce ideas of \"masculine\" and \"feminine\" play from an early age.</p>\n<p>Read more about the research and see the data firsthand:\n<a href=\"https://www.qmul.ac.uk/media/news/2024/se/new-study-sheds-light-on-the-role-of-sound-and-music-in-gendered-toy-marketing.html\">https://www.qmul.ac.uk/media/news/2024/se/new-study-sheds-light-on-the-role-of-sound-and-music-in-gendered-toy-marketing.html</a></p>\n<p>(<a href=\"https://www.flickr.com/photos/49399132@N00/8297358676\">image source</a> and <a href=\"https://creativecommons.org/licenses/by-nc-nd/2.0/\">license details</a>)</p>","id":"988e8e34-9faa-54ea-adb0-544b52e0a46a"},{"fields":{"slug":"/news/2024-10-28.C4DM-at_ISMIR_2024"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/23c0c3bffae6b50f360808e214b5c26b/f054e/ismir2024logo.png","srcSet":"/static/23c0c3bffae6b50f360808e214b5c26b/a49f4/ismir2024logo.png 188w,\n/static/23c0c3bffae6b50f360808e214b5c26b/98376/ismir2024logo.png 375w,\n/static/23c0c3bffae6b50f360808e214b5c26b/f054e/ismir2024logo.png 750w","sizes":"(min-width: 750px) 750px, 100vw"},"sources":[{"srcSet":"/static/23c0c3bffae6b50f360808e214b5c26b/e6874/ismir2024logo.webp 188w,\n/static/23c0c3bffae6b50f360808e214b5c26b/e3305/ismir2024logo.webp 375w,\n/static/23c0c3bffae6b50f360808e214b5c26b/4f03f/ismir2024logo.webp 750w","type":"image/webp","sizes":"(min-width: 750px) 750px, 100vw"}]},"width":750,"height":750}}},"title":"C4DM at ISMIR 2024","author":"Emmanouil Benetos","date":"Mon 28 Oct 2024"},"html":"<p></p>\n<p>On 10-14 November 2024, several C4DM researchers will participate at the <b><a href=\"https://ismir2024.ismir.net/\">25th International Society for Music Information Retrieval Conference (ISMIR 2024)</a></b>. ISMIR is the leading conference in the field of music informatics, and is currently the <a href=\"https://scholar.google.com/citations?view_op=top_venues&#x26;hl=en&#x26;vq=hum_musicmusicology\">top-cited publication for Music &#x26; Musicology</a> (source: Google Scholar). This year ISMIR will take place onsite in San Francisco (CA, USA) and online.</p>\n<p>Similar to previous years, the Centre for Digital Music will have a strong presence at ISMIR 2024.</p>\n<p><br>In the <b>Scientific Programme</b>, the following papers are authored/co-authored by C4DM members:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2409.11498\">Augment, Drop &#x26; Swap: Improving Diversity in LLM Captions for Efficient Music-Text Representation Learning</a> (Ilaria Manco, Justin Salamon, Oriol Nieto)</li>\n<li><a href=\"https://arxiv.org/abs/2407.18787\">Automatic Detection of Moral Values in Music Lyrics</a> (Vjosa Preniqi, Iacopo Ghinassi, Julia Ive, Kyriaki Kalimeri, Charalampos Saitis)</li>\n<li><a href=\"https://arxiv.org/abs/2407.21615\">Between the AI and Me: Analysing Listeners' Perspectives on AI- and Human-Composed Progressive Metal Music</a> (Pedro Sarmento, Jackson Lothn, Mathieu Barthet)</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/98625\">Can LLMs \"Reason\" in Music? An Evaluation of LLMs' Capability of Music Understanding and Generation</a> (Ziya Zhou, Yuhang Wu, Zhiyue Wu, Xinyue Zhang, Ruibin Yuan, Yinghao Ma, Lu Wang, Emmanouil Benetos, Wei Xue, Yike Guo)</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/98627\">ComposerX: Multi-Agent Music Generation with LLMs</a> (Qixin Deng, Qikai Yang, Ruibin Yuan, Yipeng Huang, Yi Wang, Xubo Liu, Zeyue Tian, Jiahao Pan, Ge Zhang, Hanfeng Lin, Yizhi Li, Yinghao Ma, Jie Fu, Chenghua Lin, Emmanouil Benetos, Wenwu Wang, Guangyu Xia, Wei Xue, Yike Guo)</li>\n<li><a href=\"https://arxiv.org/abs/2310.17162\">Content-based Controls for Music Large-scale Language Modeling</a> (Liwei Lin, Gus Xia, Junyan Jiang, Yixiao Zhang)</li>\n<li><a href=\"https://arxiv.org/abs/2406.08384\">Diff-A-Riff: Musical Accompaniment Co-creation via Latent Diffusion Models</a> (Javier Nistal, Marco Pasini, Cyran Aouameur, Maarten Grachten, Stefan Lattner)</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/98161\">Diff-MST: Differentiable Mixing Style Transfer</a> (Soumya Sai Vanka, Christian J. Steinmetz, Jean-Baptiste Rolland, Joshua D. Reiss, George Fazekas)</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/98362\">From Audio Encoders to Piano Judges: Benchmarking Performance Understanding for Solo Piano</a> (Huan Zhang, Jinhua Liang, Simon Dixon)</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/97938\">GAPS: A Large and Diverse Classical Guitar Dataset and Benchmark Transcription Model</a> (Xavier Riley, Zixun Guo, Drew Edwards, Simon Dixon)</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/99324\">I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition</a> (Yannis Vasilakis, Rachel Bittner, Johan Pauwels)</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/97939\">MIDI-to-Tab: Guitar Tablature Inference via Masked Language Modeling</a> (Drew Edwards, Xavier Riley, Pedro Sarmento, Simon Dixon)</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/98705\">MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models</a> (Benno Weck, Ilaria Manco, Emmanouil Benetos, Elio Quinton, George Fazekas, Dmitry Bogdanov)<br><b>Best Paper Nomination</b></li>\n<li><a href=\"https://www.arxiv.org/abs/2408.06500\">Music2Latent: Consistency Autoencoders for Latent Audio Compression</a> (Marco Pasini, Stefan Lattner, George Fazekas)</li>\n<li><a href=\"https://arxiv.org/abs/2407.13840\">Semi-Supervised Contrastive Learning of Musical Representations</a> (Julien Guinot, Elio Quinton, George Fazekas)</li>\n<li><a href=\"https://arxiv.org/abs/2406.17672\">SpecMaskGIT: Masked Generative Modelling of Audio Spectrogram for Efficient Audio Synthesis and Beyond</a> (Marco Comunità, Zhi Zhong, Akira Takahashi, Shiqi Yang, Mengjie Zhao, Koichi Saito, Yukara Ikemiya, Takashi Shibuya, Shusuke Takahashi, Yuki Mitsufuji)</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/98593\">ST-ITO: Controlling audio effects for style transfer with inference-time optimization</a> (Christian J. Steinmetz, Shubhr Singh, Marco Comunità, Ilias Ibnyahya, Shanxin Yuan, Emmanouil Benetos, Joshua D. Reiss)<br><b>Best Paper Nomination</b></li>\n</ul>\n<p><br>The following <b>Tutorial</b> will be presented by C4DM PhD student <a href=\"https://ilariamanco.com/\">Ilaria Manco</a>:</p>\n<ul>\n<li><a href=\"https://ismir2024.ismir.net/tutorials\">Connecting Music Audio and Natural Language</a> (Seung Heon Doh, Ilaria Manco, Zachary Novack, Jong Wook Kim and Ke Chen)</li>\n</ul>\n<p><br>The following journal paper published at <b>TISMIR</b> will be presented at the conference:</p>\n<ul>\n<li><a href=\"https://transactions.ismir.net/articles/10.5334/tismir.162\">PiJAMA: Piano Jazz with Automatic MIDI Annotations</a> (Drew Edwards, Simon Dixon, Emmanouil Benetos)</li>\n</ul>\n<p><br>As part of the <b>MIREX public evaluations</b>, C4DM PhD student <a href=\"https://ldzhangyx.github.io/\">Yixiao Zhang</a> is task captain for the <a href=\"https://www.music-ir.org/mirex/wiki/2024:Music_Description_%26_Captioning\">Music Description &#x26; Captioning task</a>.</p>\n<p>The following <b>Late-Breaking Demos</b> will be showcased at the conference:</p>\n<ul>\n<li><a href=\"https://ismir2024program.ismir.net/lbd_475.html\">Diff-MST^C: A Mixing Style Transfer Prototype for Cubase</a> (Soumya Sai Vanka, Lennart Hannink, Jean-Baptiste Rolland, George Fazekas)</li>\n<li><a href=\"https://ismir2024program.ismir.net/lbd_458.html\">Enhancement of Speech and Language Models through unsupervised Learning with Music Datasets</a> (Eviatar Bas, Iran R Roman)</li>\n<li><a href=\"https://ismir2024program.ismir.net/lbd_482.html\">Enhanced Automatic Drum Transcription via Drum Stem Source Separation</a> (Xavier Riley, Simon Dixon)</li>\n<li>[Exploring Transformer-Based Music Overpainting for Jazz Piano Variations](<a href=\"https://ismir2024program.ismir.net/lbd_479.html\">https://ismir2024program.ismir.net/lbd_479.html</a> (Eleanor Row, Ivan Shanin, George Fazekas)</li>\n<li><a href=\"https://ismir2024program.ismir.net/lbd_432.html\">Source-level pitch and timbre editing for mixtures of tones duing disentangled representations</a> (Yin-Jyun Luo, Kin Wai Cheuk, Woosung Choi, Toshimitsu Uesaka, Keisuke Toyama, Wei-Hsiang Liao, Simon Dixon, Yuki Mitsufuji)</li>\n<li><a href=\"https://ismir2024program.ismir.net/lbd_465.html\">How does the teacher rate? Observations from the NeuroPiano dataset</a> (Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Simon Dixon, Shinichi Furuya)</li>\n<li><a href=\"https://ismir2024program.ismir.net/lbd_456.html\">ReVamp: Visualisation and analysis in the digital audio workstation</a> (Chris Cannam, George Fazekas)</li>\n</ul>\n<p><br>Finally, the following C4DM members are organising <b>Satellite Events</b>:</p>\n<ul>\n<li><a href=\"https://elonashatri.github.io/\">Elona Shatri</a> as General Chair for <a href=\"https://sites.google.com/view/worms2024/\">WoRMS 2024</a></li>\n<li><a href=\"https://ilariamanco.com/\">Ilaria Manco</a> as Organising Committee member for <a href=\"https://sites.google.com/view/nlp4musa-2024/\">NLP4MUSA 2024</a></li>\n</ul>\n<p><br>See you at ISMIR!</p>","id":"e41051f1-61cb-51c3-b392-59c30c42cb96"},{"fields":{"slug":"/news/2024-10-25.DMRN2024"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#e8e8f8","images":{"fallback":{"src":"/static/6a96619ca934bdc4aee6bdedb3d8a3e9/baaed/DMRN-logo.jpg","srcSet":"/static/6a96619ca934bdc4aee6bdedb3d8a3e9/dd515/DMRN-logo.jpg 200w,\n/static/6a96619ca934bdc4aee6bdedb3d8a3e9/47930/DMRN-logo.jpg 400w,\n/static/6a96619ca934bdc4aee6bdedb3d8a3e9/baaed/DMRN-logo.jpg 800w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[{"srcSet":"/static/6a96619ca934bdc4aee6bdedb3d8a3e9/2e34e/DMRN-logo.webp 200w,\n/static/6a96619ca934bdc4aee6bdedb3d8a3e9/416c3/DMRN-logo.webp 400w,\n/static/6a96619ca934bdc4aee6bdedb3d8a3e9/c1587/DMRN-logo.webp 800w","type":"image/webp","sizes":"(min-width: 800px) 800px, 100vw"}]},"width":800,"height":800}}},"title":"DMRN+19: Digital Music Research Network One-day Workshop 2024","author":"Admin","date":"Fri 25 Oct 2024"},"html":"<p><b>DMRN+19: Digital Music Research Network 1-Day Workshop 2024</b></p>\n<p><b>Queen Mary University of London</b><br>\n<b>Tue 17 December 2024</b><br>\n<b><a href=\"https://www.qmul.ac.uk/dmrn/dmrn19/\"><a href=\"https://www.qmul.ac.uk/dmrn/dmrn19/\">https://www.qmul.ac.uk/dmrn/dmrn19/</a></a></b></p>\n<p>The Digital Music Research Network (DMRN) aims to promote research in the area of Digital Music, by bringing together researchers from UK and overseas universities and industry for its annual workshop. The workshop will include invited and contributed talks and posters. The workshop will be an ideal opportunity for networking with other people working in the area.</p>\n<p><b>Call for Contributions</b></p>\n<p>You are invited to submit a proposal for a talk and/or a poster to be presented at this event.</p>\n<p>TALKS may range from the latest research, through research overviews or surveys, to opinion pieces or position statements, particularly those likely to be of interest to an interdisciplinary audience. We plan to keep talks to about 10 minutes each, depending on the number of submissions.</p>\n<p>POSTERS can be on any research topic of interest to the members of the network. Posters will be displayed on the venue hall during coffees and lunch break.</p>\n<p>The abstracts of presentations will be collated into a digest and distributed on the day.</p>\n<p><b>Submission</b></p>\n<p>Please prepare your talk or poster proposal in the form of an abstract (1 page A4, using the template available from the DMRN+17 web page). Submit it via email to <a href=\"mailto:dmrn@lists.eecs.qmul.ac.uk\"><a href=\"mailto:dmrn@lists.eecs.qmul.ac.uk\">dmrn@lists.eecs.qmul.ac.uk</a></a> giving the following information about your presentation:</p>\n<ul>\n<li>Authors</li>\n<li>Title</li>\n<li>Abstract</li>\n<li>Preference for talk or poster (or \"no preference\").</li>\n</ul>\n<p><b>Deadlines</b></p>\n<p>21 Nov 2024: Abstract submission deadline\n24 Nov 2024: Notification of acceptance\n12 Dec 2024: Registration deadline\n17 Dec 2024: DMRN+19 Workshop in person</p>\n<p>For further information, visit:  <a href=\"https://www.qmul.ac.uk/dmrn/dmrn19/\">https://www.qmul.ac.uk/dmrn/dmrn19/</a></p>\n<p>Enquiries: Alvaro Bort (<a href=\"mailto:a.bort@qmul.ac.uk\">a.bort@qmul.ac.uk</a>)</p>","id":"fb9d04da-ec49-5104-8e8c-cf6293066c8b"}]}}}
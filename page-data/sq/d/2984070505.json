{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/YIFgS8she58","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/Sandler-Artificial-Neuroscience"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png","srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/2fd20/EPSRC.png 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/de391/EPSRC.png 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/d66e1/EPSRC.webp 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/e7160/EPSRC.webp 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/5f169/EPSRC.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Artificial Neuroscience: metrology and engineering for Deep Learning using Linear Algebra","author":"Prof Mark Sandler (PI), Boris Khoruzhenko (CI)","date":null,"link":"https://www.ukri.org/councils/epsrc/"},"html":"","id":"c1314000-fe3b-5175-88a2-f239d861ddad"},{"fields":{"slug":"/projects/Barthet-Netz"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Netz: A Novel XR Musical Instrument","author":"Dr Mathieu Barthet (PI)","date":null,"link":"https://iuk-business-connect.org.uk/programme/icure/"},"html":"","id":"0040b455-65cf-5ea1-a7ce-38ab70570dae"},{"fields":{"slug":"/projects/Benetos-Algorivm"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Project Maestro - AI Musical Analysis Platform","author":"Dr Emmanouil Benetos (PI), Prof Simon Dixon (CI)","date":null,"link":"https://www.ukri.org/councils/innovate-uk/"},"html":"","id":"78c348d8-d78f-5acf-8441-382bc6c4d791"},{"fields":{"slug":"/projects/Benetos-L-Acoustics"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Online speech enhancement in scenarios with low direct-to-reverberant-ratio","author":"Dr Emmanouil Benetos (PI), Dr Aidan Hogg (CI)","date":null,"link":"https://www.l-acoustics.com/"},"html":"","id":"ce65fc6c-7228-5805-a9b2-6e14e5fd5086"},{"fields":{"slug":"/projects/Benetos-Moises"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Enhancing lyrics transcription with open-source architectures and fine-tuning techniques","author":"Dr Emmanouil Benetos (PI)","date":null,"link":"https://moises.ai/"},"html":"","id":"d61977ff-9345-55a4-b85e-44caffa4a7f0"},{"fields":{"slug":"/projects/Dixon-assessment"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Music Performance Assessment and Feedback","author":"Prof Simon Dixon (PI), Dr Emmanouil Benetos (CI)","date":null,"link":null},"html":"","id":"68ee56ea-59b7-58de-a8c9-805c4aba4357"}]},"news":{"nodes":[{"fields":{"slug":"/news/2025-10-23.C4DM-student-awarded-Google-PhD-fellowship"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4ac60065bd12d76e2e9830ee1a3f805a/a89ca/yinghao.jpg","srcSet":"/static/4ac60065bd12d76e2e9830ee1a3f805a/96deb/yinghao.jpg 150w,\n/static/4ac60065bd12d76e2e9830ee1a3f805a/0fdf4/yinghao.jpg 300w,\n/static/4ac60065bd12d76e2e9830ee1a3f805a/a89ca/yinghao.jpg 600w","sizes":"(min-width: 600px) 600px, 100vw"},"sources":[{"srcSet":"/static/4ac60065bd12d76e2e9830ee1a3f805a/c65bc/yinghao.webp 150w,\n/static/4ac60065bd12d76e2e9830ee1a3f805a/078c3/yinghao.webp 300w,\n/static/4ac60065bd12d76e2e9830ee1a3f805a/6d09e/yinghao.webp 600w","type":"image/webp","sizes":"(min-width: 600px) 600px, 100vw"}]},"width":600,"height":600}}},"title":"C4DM PhD student awarded Google PhD Fellowship","author":"Admin","date":"Thu 23 Oct 2025"},"html":"<p>We are extremely proud to announce that <a href=\"https://nicolaus625.github.io/\">Yinghao Ma</a>, a PhD student in AI and Music at the Centre for Digital Music at QMUL, has been awarded the <a href=\"https://research.google/programs-and-events/phd-fellowship/recipients/?filtertab=2025\">2025 Google Fellowship in Machine Perception</a>.</p>\n<p>A Google spokesperson said: “The student nominations we received this year were exemplary in their quality, but Yinghao especially stood out and was endorsed by the research scientists and distinguished engineers within Google who participated in the review. Congratulations to Yinghao on this well-deserved recognition, it’s an honor to support such incredibly talented students.”</p>\n<p>Yinghao's PhD research focuses on advancing Large Language Models (LLMs) for music understanding and generation. Specifically, he studies how multimodal models can integrate audio, symbolic, and textual information to understand, reason about, and generate music.</p>\n<p>Together with colleagues, he developed <a href=\"https://arxiv.org/abs/2306.00107\">MERT</a>, a large-scale music audio representation model which has more than 10k monthly download in the past three years. His recent work includes developing music instruction-following datasets and benchmarks that help evaluate how well AI systems can comprehend and create music.</p>\n<p>He said: \"It's my great honour to receive the Google PhD Fellowship that recognises my research and strongly contribute to my future career. I’m deeply grateful to Google and QMUL for the support, providing good platforms for AI &#x26; music research.\"</p>\n<p>Congratulations Yinghao!</p>","id":"1ec9aa73-6b0c-5967-b4d1-2b1e075f8ebe"},{"fields":{"slug":"/news/2025-10-22.C4DM-Seminar_Come_Peladeau"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Côme Peladeau","author":"Admin","date":"Wed 22 Oct 2025"},"html":"<h3>C4DM Seminar: Côme Peladeau: Audio processors: estimating parameter distributions using discrete normalizing flows and DDSP</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Côme Peladeau</p>\n<p><strong>Date/time:</strong>  Wednesday, 22th October 2025, 1pm</p>\n<p><strong>Location:</strong> Online</p>\n<p><strong>Zoom Link:</strong> <a href=\"https://qmul-ac-uk.zoom.us/j/2387202947\">https://qmul-ac-uk.zoom.us/j/2387202947</a></p>\n<h2><b>Title</b>: Audio processors: estimating parameter distributions using discrete normalizing flows and DDSP</h2>\n<p><b>Abstract</b>: Audio processor, such as audio effects or synthesizers, are widely used in popular music production.</p>\n<p>Their parameters control the quality of the output sound. Multiple combinations of parameters can lead to the same sound.</p>\n<p>While recent approaches have been proposed to estimate these parameters given only the output sound, those are deterministic, i.e. they only estimate a single solution among the many possible parameter configurations.</p>\n<p>This work proposes an approach to estimate all the combinations that lead to the target sound, in the form of a probability distribution.</p>\n<p>We achieve this using differentiable signal processing and discrete normalizing flows.</p>\n<p><b>Bio</b>: Côme Peladeau graduated with a masters degree in Acoustics, Signal Processing and Computer Science applied to Music at Sorbonne Université in 2023.</p>\n<p>He is now a PhD student working with Geoffroy Peeters and Dominique Fourer at the Information Processing and Communications Laboratory (LTCI) in Telecom Paris.</p>\n<p>HI work focuses on audio effects estimation using deep learning and differentiable signal processing.</p>","id":"184cbea7-8b9a-5196-9c4d-7a5b3238c12c"},{"fields":{"slug":"/news/2025-10-22.C4DM-Seminar_Nithya_Shikarpur"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Nithya Shikarpur","author":"Yinghao Ma","date":"Wed 22 Oct 2025"},"html":"<h3>C4DM Seminar: Nithya Shikarpur: Towards Generative Modeling and Interactive Performance for Hindustani Music and beyond</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Nithya Shikarpur</p>\n<p><strong>Date/time:</strong>  Wednesday, 22th October 2025, 3.30pm</p>\n<p><strong>Location:</strong> Hybrid. Peter Landin building, room 4.24</p>\n<p><strong>Teams meeting info:</strong> Meeting ID: 324 267 638 748 , Passcode: 7sV9PL6k</p>\n<h2><b>Title</b>: Towards Generative Modeling and Interactive Performance for Hindustani Music and beyond</h2>\n<p><b>Abstract</b>:\nRecent advances in generative music modeling open up rich avenues for creative exploration and expression. As both a musician and researcher, my work focuses on two interconnected goals: (1) developing generative models that meaningfully engage with the musical context and aesthetics of specific traditions, and (2) designing interactive systems that foster creative collaboration between humans and generative models. First, I will introduce GaMaDHaNi, a hierarchical generative model for Hindustani vocal music.  Through a hierarchical system, modeling pitch contours first followed by spectrograms, GaMaDHaNi captures the microtonal nuances that characterize this tradition. I will further discuss the challenges of conditioning such models on musically relevant parameters such as raga. Second, I will share insights from “cat-in-loop”, a collaborative performance created with the Cat in Black ensemble and my collaborators Weilu Ge and Hugo Garcia. This work explores the creative (mis)use of VampNet, a masked audio transformer model, as a tool for embodied improvisation and human-AI co-performance. Together, these projects reflect a broader inquiry into how generative systems can engage with musical practices - not merely as data, but as living, evolving traditions of sound, gesture, and interaction.</p>\n<p><b>Bio</b>:\nNithya Shikarpur is a second-year PhD student at MIT advised by Prof. Cheng Zhi Anna Huang. She is interested in the modeling of human-AI interactive systems for music creation and creativity especially for low-resource genres of music. Earlier she was at Université de Montréal and Mila for her M.Sc. where she worked with Prof. Cheng-Zhi Anna Huang. She is also a practitioner and performer of Hindustani classical vocal music and draws on this knowledge to further her research projects.</p>","id":"d41d0c28-10bf-52e2-b1bb-bc28bf1cd627"},{"fields":{"slug":"/news/2025-10-06.C4DM-at_WASPAA_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#a8c8e8","images":{"fallback":{"src":"/static/6871445306f9af07141444a39ae524a8/fad00/waspaa2025.png","srcSet":"/static/6871445306f9af07141444a39ae524a8/71cba/waspaa2025.png 291w,\n/static/6871445306f9af07141444a39ae524a8/fa1e5/waspaa2025.png 582w,\n/static/6871445306f9af07141444a39ae524a8/fad00/waspaa2025.png 1163w","sizes":"(min-width: 1163px) 1163px, 100vw"},"sources":[{"srcSet":"/static/6871445306f9af07141444a39ae524a8/46435/waspaa2025.webp 291w,\n/static/6871445306f9af07141444a39ae524a8/f27b2/waspaa2025.webp 582w,\n/static/6871445306f9af07141444a39ae524a8/24d0f/waspaa2025.webp 1163w","type":"image/webp","sizes":"(min-width: 1163px) 1163px, 100vw"}]},"width":1163,"height":1163}}},"title":"C4DM at WASPAA 2025","author":"Admin","date":"Mon 06 Oct 2025"},"html":"<p></p>\n<p>On 12-15 October, several C4DM researchers will participate at the <b><a href=\"http://www.waspaa.com/\">2025 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics</a></b>, taking place at the Granlibakken Tahoe Resort near Lake Tahoe, in Tahoe City, CA, USA. WASPAA is a premier event in the field of audio signal processing, organised by the IEEE's Audio and Acoustic Signal Processing (AASP) technical committee, with a strong focus on music signal processing and computational sound scene analysis.</p>\n<p>The Centre for Digital Music (C4DM), as in previous years, will have a strong presence at WASPAA 2025.</p>\n<p>In the <b>Technical Programme</b>, the following papers are authored by C4DM members:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2507.07066\">Latent Acoustic Mapping for Direction of Arrival Estimation: A Self-Supervised Approach</a> (Adrian S. Roman, Iran R. Roman, Juan Pablo Bello)</li>\n<li><a href=\"https://arxiv.org/abs/2507.12175\">RUMAA: Repeat-Aware Unified Music Audio Analysis for Score-Performance Alignment, Transcription, and Mistake Detection</a> (Sungkyun Chang, Simon Dixon, Emmanouil Benetos)</li>\n<li><a href=\"https://arxiv.org/abs/2510.06204\">Modulation Discovery with Differentiable Digital Signal Processing</a> (Christopher Mitcheltree, Hao Hao Tan, Joshua D. Reiss)</li>\n<li>Beyond Architecture: The Critical Impact of Inference Overlap on Music Source Separation Benchmarks (Harnick Khera, Johan Pauwels, Alan W. Archer-Boyd, Mark B. Sandler)</li>\n<li><a href=\"https://arxiv.org/abs/2505.11315\">Improving Inference-Time Optimisation for Vocal Effects Style Transfer with a Gaussian Prior</a> (Chin-Yun Yu, Marco A. Martínez-Ramírez, Junghyun Koo, Wei-Hsiang Liao, Yuki Mitsufuji, George Fazekas)</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/109071\">Self-Supervised Representation Learning with a JEPA Framework for Multi-instrument Music Transcription</a> (Mary Pilataki, Matthias Mauch, Simon Dixon)</li>\n</ul>\n<p>In the <b>Demo Session</b>, the following demos will be presented by C4DM members:</p>\n<ul>\n<li>Neural Audio Synthesis for Non-Keyboard Instruments (Franco Caspe, Andrew McPherson, Mark Sandler)</li>\n<li>PCA-DiffVox: Augmenting Vocal Effects Tweakability With a Bijective Latent Space (Chin-Yun Yu, Marco A. Martínez-Ramírez, Junghyun Koo, Wei-Hsiang Liao, Yuki Mitsufuji, George Fazekas)</li>\n</ul>\n<p>See you at WASPAA!</p>","id":"5422e2a2-1826-5148-b601-a7cb31a882f5"},{"fields":{"slug":"/news/2025-09-30.C4DM-ISMIR_Awards"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8b8b8","images":{"fallback":{"src":"/static/0d9bd1939a6f78714877cb0f2df8af8a/93c00/c4dm_ismir_2025_awards.png","srcSet":"/static/0d9bd1939a6f78714877cb0f2df8af8a/383f5/c4dm_ismir_2025_awards.png 239w,\n/static/0d9bd1939a6f78714877cb0f2df8af8a/6c9f5/c4dm_ismir_2025_awards.png 477w,\n/static/0d9bd1939a6f78714877cb0f2df8af8a/93c00/c4dm_ismir_2025_awards.png 954w","sizes":"(min-width: 954px) 954px, 100vw"},"sources":[{"srcSet":"/static/0d9bd1939a6f78714877cb0f2df8af8a/2600e/c4dm_ismir_2025_awards.webp 239w,\n/static/0d9bd1939a6f78714877cb0f2df8af8a/616b7/c4dm_ismir_2025_awards.webp 477w,\n/static/0d9bd1939a6f78714877cb0f2df8af8a/1094f/c4dm_ismir_2025_awards.webp 954w","type":"image/webp","sizes":"(min-width: 954px) 954px, 100vw"}]},"width":954,"height":954}}},"title":"Best student paper and outstanding reviewer awards at ISMIR 2025","author":"Christos Plachouras","date":"Tue 30 Sep 2025"},"html":"<p>We are delighted to share that C4DM PhD student Ben Hayes, along with C4DM academics Charalampos Saitis and George Fazekas, have received the best student paper award at the ISMIR 2025 conference. The paper \"<a href=\"https://ismir2025program.ismir.net/poster_7.html\">Audio Synthesizer Inversion in Symmetric Parameter Spaces With Approximately Equivariant Flow Matching</a>\" proposes using permutation equivariant continuous normalizing flows to handle the ill-posed problem of audio synthesizer inversion, where multiple parameter configurations can produce identical sounds due to intrinsic symmetries in synthesizer design. By explicitly modeling these symmetries, particularly permutation invariance across repeated components like oscillators and filters, the method outperforms both regression-based approaches and symmetry-naive generative models on both synthetic tasks and a real-world synthesizer (Surge XT).</p>\n<p>We are also happy to share that two C4DM PhD students, Yannis Vasilakis and Ben Hayes, were recognised as outstanding reviewers.</p>","id":"0331e8af-8b6b-52e3-a193-151c89c594fc"},{"fields":{"slug":"/news/2025-09-29.C4DM-Seminar_Ahmed_Sayed"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Ahmed Sayed","author":"Admin","date":"Mon 29 Sep 2025"},"html":"<h3>C4DM Seminar: Ahmed Sayed: Advancing Decentralized AI: Scalable, Adaptive, and Client-Centric Learning Systems</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Ahmed Sayed</p>\n<p><strong>Date/time:</strong>  Monday, 29th September 2025, 1pm</p>\n<p><strong>Location:</strong> G2, Enginerring Building, Mile End Campus, Queen Mary University of London, E1 4NS</p>\n<h2><b>Title</b>: Advancing Decentralized AI: Scalable, Adaptive, and Client-Centric Learning Systems</h2>\n<p><b>Abstract</b>: Decentralised AI systems, particularly those employing federated learning (FL), offer a promising approach to training machine learning models across distributed data sources while preserving privacy. However, they face significant challenges, including system heterogeneity, dynamic client availability, and resource constraints. Addressing these issues is crucial for the effective deployment of FL in real-world scenarios.\nIn this talk, I will discuss our recent efforts to enhance the robustness and adaptability of FL systems. I will introduce REFL, a resource-efficient FL framework that decouples the collection of participant updates from model aggregation, intelligently selecting participants based on their likelihood of future availability to maximise resource utilisation. Then I will cover the development of FLOAT, an automated tuning framework that dynamically optimises resource utilisation to meet training deadlines, mitigating stragglers and dropouts through various optimisation techniques. Additionally, QKT is presented as a framework that enables tailored knowledge acquisition to fulfil specific client needs without direct data exchange, employing a data-free masking strategy to facilitate communication-efficient query-focused knowledge transfer while refining task-specific parameters to mitigate knowledge interference and forgetting. Finally, our UKRI-EPSRC-funded project, KUber, addresses these challenges by developing a distributed knowledge delivery system to enhance FL scalability and efficiency. KUber’s architecture facilitates seamless knowledge exchange among learning entities, optimising resource utilisation and model convergence.</p>\n<p><b>Bio</b>: <a href=\"https://www.qmul.ac.uk/eecs/people/profiles/sayedahmed.html\">https://www.qmul.ac.uk/eecs/people/profiles/sayedahmed.html</a></p>","id":"4813536d-c027-591d-987d-b6c4375ba0ef"}]}}}
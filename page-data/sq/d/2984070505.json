{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/YIFgS8she58","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/Sandler-Artificial-Neuroscience"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png","srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/2fd20/EPSRC.png 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/de391/EPSRC.png 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/d66e1/EPSRC.webp 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/e7160/EPSRC.webp 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/5f169/EPSRC.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Artificial Neuroscience: metrology and engineering for Deep Learning using Linear Algebra","author":"Prof Mark Sandler (PI), Boris Khoruzhenko (CI)","date":null,"link":"https://www.ukri.org/councils/epsrc/"},"html":"","id":"c1314000-fe3b-5175-88a2-f239d861ddad"},{"fields":{"slug":"/projects/Barthet-Netz"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Netz: A Novel XR Musical Instrument","author":"Dr Mathieu Barthet (PI)","date":null,"link":"https://iuk-business-connect.org.uk/programme/icure/"},"html":"","id":"0040b455-65cf-5ea1-a7ce-38ab70570dae"},{"fields":{"slug":"/projects/Benetos-L-Acoustics"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Online speech enhancement in scenarios with low direct-to-reverberant-ratio","author":"Dr Emmanouil Benetos (PI), Dr Aidan Hogg (CI)","date":null,"link":"https://www.l-acoustics.com/"},"html":"","id":"ce65fc6c-7228-5805-a9b2-6e14e5fd5086"},{"fields":{"slug":"/projects/Benetos-Algorivm"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Project Maestro - AI Musical Analysis Platform","author":"Dr Emmanouil Benetos (PI), Prof Simon Dixon (CI)","date":null,"link":"https://www.ukri.org/councils/innovate-uk/"},"html":"","id":"78c348d8-d78f-5acf-8441-382bc6c4d791"},{"fields":{"slug":"/projects/Benetos-Moises"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Enhancing lyrics transcription with open-source architectures and fine-tuning techniques","author":"Dr Emmanouil Benetos (PI)","date":null,"link":"https://moises.ai/"},"html":"","id":"d61977ff-9345-55a4-b85e-44caffa4a7f0"},{"fields":{"slug":"/projects/Dixon-assessment"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Music Performance Assessment and Feedback","author":"Prof Simon Dixon (PI), Dr Emmanouil Benetos (CI)","date":null,"link":null},"html":"","id":"68ee56ea-59b7-58de-a8c9-805c4aba4357"}]},"news":{"nodes":[{"fields":{"slug":"/news/2025-04-14.C4DM-at_ICLR_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e1b6924083744f1e06cfc014164defcc/928b9/ICLR2025.png","srcSet":"/static/e1b6924083744f1e06cfc014164defcc/b0268/ICLR2025.png 118w,\n/static/e1b6924083744f1e06cfc014164defcc/f1af1/ICLR2025.png 236w,\n/static/e1b6924083744f1e06cfc014164defcc/928b9/ICLR2025.png 472w","sizes":"(min-width: 472px) 472px, 100vw"},"sources":[{"srcSet":"/static/e1b6924083744f1e06cfc014164defcc/2c82d/ICLR2025.webp 118w,\n/static/e1b6924083744f1e06cfc014164defcc/9bbe7/ICLR2025.webp 236w,\n/static/e1b6924083744f1e06cfc014164defcc/b4137/ICLR2025.webp 472w","type":"image/webp","sizes":"(min-width: 472px) 472px, 100vw"}]},"width":472,"height":472}}},"title":"C4DM at ICLR 2025","author":"Emmanouil Benetos","date":"Mon 14 Apr 2025"},"html":"<p>On 24-28 April, C4DM researchers will participate at the <b><a href=\"https://iclr.cc/\">Thirteenth International Conference on Learning Representations (ICLR 2025)</a></b>, taking place in Singapore. ICLR is the premier gathering of professionals dedicated to the advancement of the branch of artificial intelligence called representation learning, but generally referred to as deep learning.</p>\n<p>C4DM members will be presenting the following papers at the main track of ICLR 2025:</p>\n<ul>\n<li>\n<p><a href=\"https://openreview.net/forum?id=X5hrhgndxW\">Aria-MIDI: A Dataset of MIDI Files for Symbolic Music Modeling</a>, by Louis Bradshaw, Simon Colton</p>\n</li>\n<li>\n<p><a href=\"https://openreview.net/forum?id=iAK9oHp4Zz\">MuPT: A Generative Symbolic Music Pretrained Transformer</a>, by Xingwei Qu, Yuelin Bai, Yinghao Ma, Ziya Zhou, Ka Man Lo, Jiaheng Liu, Ruibin Yuan, Lejun Min, Xueling Liu, Tianyu Zhang, Xinrun Du, Shuyue Guo, Yiming Liang, Yizhi Li, Shangda Wu, Junting Zhou, Tianyu Zheng, Ziyang Ma, Fengze Han, Wei Xue, Gus Xia, Emmanouil Benetos, Xiang Yue, Chenghua Lin, Xu Tan, Wenhao Huang, Jie Fu, Ge Zhang</p>\n</li>\n</ul>\n<p>See you all at ICLR!</p>","id":"367fb186-1809-5f9e-8c89-a644e24c1ee3"},{"fields":{"slug":"/news/2025-04-09.C4DM-Seminar_Alexander_Hawkins"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#282828","images":{"fallback":{"src":"/static/d8ef9a548b6614090478565b12a693cb/7e194/alexander-hawkins.jpg","srcSet":"/static/d8ef9a548b6614090478565b12a693cb/cd18a/alexander-hawkins.jpg 450w,\n/static/d8ef9a548b6614090478565b12a693cb/0a45a/alexander-hawkins.jpg 900w,\n/static/d8ef9a548b6614090478565b12a693cb/7e194/alexander-hawkins.jpg 1800w","sizes":"(min-width: 1800px) 1800px, 100vw"},"sources":[{"srcSet":"/static/d8ef9a548b6614090478565b12a693cb/2890f/alexander-hawkins.webp 450w,\n/static/d8ef9a548b6614090478565b12a693cb/3987a/alexander-hawkins.webp 900w,\n/static/d8ef9a548b6614090478565b12a693cb/b46b0/alexander-hawkins.webp 1800w","type":"image/webp","sizes":"(min-width: 1800px) 1800px, 100vw"}]},"width":1800,"height":1800}}},"title":"C4DM & AIL Seminar: Alexander Hawkins","author":"Admin","date":"Wed 09 Apr 2025"},"html":"<h3>C4DM &#x26; AIL Seminar: Alexander Hawkins: Some Questions from the Field: A Performer in Conversation with New Instruments</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Imperial College London, Dyson School of Design Engineering</h4>\n<h4>Centre for Digital Music Seminar Series &#x26; Augmented Instruments Lab Open Seminars</h4>\n<p><strong>Seminar by:</strong> <a href=\"https://alexanderhawkinsmusic.com/\">Alexander Hawkins</a></p>\n<p><strong>Date/time:</strong>  Wednesday, 9th April 2025, 12:30pm</p>\n<p><strong>Location:</strong> Performance Lab, Engineering Building, Mile End Campus, QMUL, E1 4NS</p>\n<h2><b>Title</b>: Some Questions from the Field: A Performer in Conversation with New Instruments</h2>\n<p><b>Abstract</b>:\nAlthough I have only ever worked as a professional musician, I did obtain a PhD in law before beginning my career. This was in a somewhat liminal area of the subject: my university called it ‘law’, but others might have called it ‘sociology’, others ‘criminology’, and so on. I also taught many ‘law’ undergraduates during my doctoral studies: but the philosophers, historians, economists, statisticians, and others would all have recognised much of the territory as their own. An ‘augmented instruments’ group may well bear out something similar, in terms of a variety of disciplinary approaches, and we are probably all familiar with the fascinating insights this variegation can yield, as well as the frustrating moments of somehow ‘talking past’ each other.</p>\n<p>In my musical career, there is possibly something similar going on. I am fortunate to work regularly with artists of wildly different aesthetics, in contexts which are sometimes fully improvised, and sometimes fully notated; which sometimes look like ‘jazz’, sometimes like ‘classical’ musics, sometimes like ‘traditional’ musics, sometimes like ‘electronic’ musics, and so on. ‘Idiom’ and ‘innovation’ are therefore ideas on which I reflect a great deal. I would like to think that I spend a lot of time chasing sounds which I’ve never heard, but at the same time, have to acknowledge my instrumentalist’s obsession with control and technique.</p>\n<p>In this seminar, I would like to take a ‘liminal’ stance, and think out loud about what augmented instruments might mean in the context of my own practice as a composer-performer: as a way of teasing out some conceptual ideas, as well, perhaps, as some of the insecurities and questions of the musician who is fascinated by, but not necessarily fluent in, the possibilities.</p>\n<p><b>Bio</b>:\nAlexander Hawkins is a composer, pianist, organist, and bandleader who is ‘unlike anything else in modern creative music’. Regarded as one of his generation’s most innovative thinkers, his own unique soundworld is shaped by a profound fascination with composition and structure, alongside a love of chance and open forms.</p>\n<p>His writing has been said to represent ‘a fundamental reassertion of composition within improvised music’, and his voice one of the ‘most vividly distinctive...in modern jazz’. As a pianist, he has been described as ‘remarkable...possessing staggering technical ability and a fecund imagination.’ Concerning his organ playing, critic Brian Morton recently commented that ‘[t]he most interesting Hammond player of the last decade and more, [Hawkins] has already extended what can be done on the instrument.’</p>\n<p>Hawkins is a frequent solo performer, and also appears in groupings ranging from duo through to large ensembles. He can be heard live and on record with a vast array of contemporary leaders of all generations, including the likes of Anthony Braxton, Joe McPhee, Nicole Mitchell, Jonny Greenwood, and many others. He has been widely commissioned, by the likes of the BBC, and festivals such as the London and Berlin Jazz Festivals. He was named ‘Instrumentalist of the Year’ in the 2016 Parliamentary Jazz Awards. In 2018, he was elected a fellow of the Civitella Ranieri.</p>","id":"e0138643-cc04-531e-88fe-2199cf0b9b35"},{"fields":{"slug":"/news/2025-04-09.brain-turns_sound_into_music"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/a71fd6e7f5a7df7ab88ff0f1853f09d8/f4e58/brain-turns-sound-into-music.png","srcSet":"/static/a71fd6e7f5a7df7ab88ff0f1853f09d8/cbc40/brain-turns-sound-into-music.png 260w,\n/static/a71fd6e7f5a7df7ab88ff0f1853f09d8/e1568/brain-turns-sound-into-music.png 521w,\n/static/a71fd6e7f5a7df7ab88ff0f1853f09d8/f4e58/brain-turns-sound-into-music.png 1041w","sizes":"(min-width: 1041px) 1041px, 100vw"},"sources":[{"srcSet":"/static/a71fd6e7f5a7df7ab88ff0f1853f09d8/0fcc6/brain-turns-sound-into-music.webp 260w,\n/static/a71fd6e7f5a7df7ab88ff0f1853f09d8/05f5c/brain-turns-sound-into-music.webp 521w,\n/static/a71fd6e7f5a7df7ab88ff0f1853f09d8/04d6c/brain-turns-sound-into-music.webp 1041w","type":"image/webp","sizes":"(min-width: 1041px) 1041px, 100vw"}]},"width":1041,"height":1041}}},"title":"Groundbreaking research reveals how the brain turns sound into music","author":"Emmanouil Benetos","date":"Wed 09 Apr 2025"},"html":"<p><a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/iroman/\">Dr. Iran R. Roman</a>, Lecturer of Artificial Intelligence at the School of Electronic Engineering and Computer Science, and a group of external collaborators have revealed a groundbreaking theory explaining how the brain transforms sound into the human experience of music.</p>\n<p>Read the full story at: <a href=\"https://www.qmul.ac.uk/eecs/news-and-events/news/items/groundbreaking-research-reveals-how-the-brain-turns-sound-into-music.html\">https://www.qmul.ac.uk/eecs/news-and-events/news/items/groundbreaking-research-reveals-how-the-brain-turns-sound-into-music.html</a></p>","id":"36b76dc6-a54b-5ecd-b724-cd2632e4ee08"},{"fields":{"slug":"/news/2025-03-24.C4DM-at_ICASSP_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/d7937701a02a2623f162148d18236437/21bc1/ICASSP2025.png","srcSet":"/static/d7937701a02a2623f162148d18236437/0ae7b/ICASSP2025.png 106w,\n/static/d7937701a02a2623f162148d18236437/facc2/ICASSP2025.png 212w,\n/static/d7937701a02a2623f162148d18236437/21bc1/ICASSP2025.png 424w","sizes":"(min-width: 424px) 424px, 100vw"},"sources":[{"srcSet":"/static/d7937701a02a2623f162148d18236437/7a2a0/ICASSP2025.webp 106w,\n/static/d7937701a02a2623f162148d18236437/fbd6b/ICASSP2025.webp 212w,\n/static/d7937701a02a2623f162148d18236437/ce7ff/ICASSP2025.webp 424w","type":"image/webp","sizes":"(min-width: 424px) 424px, 100vw"}]},"width":424,"height":424}}},"title":"C4DM at ICASSP 2025","author":"Emmanouil Benetos","date":"Mon 24 Mar 2025"},"html":"<p>On 6-11 April 2025, several C4DM researchers will participate at the <b><a href=\"https://2025.ieeeicassp.org/\">2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2025)</a></b>. ICASSP is the leading conference in the field of signal processing and the flagship event of the <a href=\"https://signalprocessingsociety.org/\">IEEE Signal Processing Society</a>.</p>\n<p>As in previous years, the Centre for Digital Music will have a strong presence at the conference, both in terms of numbers and overall impact. The below papers authored or co-authored by C4DM members will be presented at the <b>main ICASSP 2025 track</b>:</p>\n<ul>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10890076/\">Acoustic identification of individual animals with hierarchical contrastive learning</a>, by Ines Nolasco, Ilyass Moummad, Dan Stowell, Emmanouil Benetos</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10888157/\">Evaluating contrastive methodologies for music representation learning using playlist data</a>, by Gregor Meehan, Johan Pauwels</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10888557/\">GraFPrint: a GNN-based approach for audio identification</a>, by Aditya Bhattacharjee, Shubhr Singh, Emmanouil Benetos</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10887996/\">Guitar-TECHS: an electric guitar dataset covering techniques, musical excerpts, chords and scales using a diverse array of hardware</a>, by Hegel Pedroza, Wallace Abreu, Ryan M. Corey, Iran R. Roman</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10889639/\">Hybrid losses for hierarchical embedding learning</a>, by Haokun Tian, Stefan Lattner, Brian McFee, Charalampos Saitis</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10889683/\">Learning control of neural sound effects synthesis from physically inspired models</a>, by Yisu Zong, Joshua Reiss</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10887766/\">Learning music audio representations with limited data</a>, by Christos Plachouras, Emmanouil Benetos, Johan Pauwels</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10890270/\">Leave-One-EquiVariant: alleviating invariance-related information loss in contrastive music representations</a>, by Julien Guinot, Elio Quinton, György Fazekas</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10890467/\">LHGNN: local-higher order graph neural networks for audio classification and tagging</a>, by Shubhr Singh, Emmanouil Benetos, Huy Phan, Dan Stowell</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10890522/\">LLaQo: towards a query-based coach in expressive performance assessment</a>, by Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Simon Dixon, Shinichi Furuya</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10890049/\">Music2Latent2: audio compression with summary embeddings and autoregressive decoding</a>, by Marco Pasini, Stefan Lattner, György Fazekas</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10890623/\">Towards an integrated approach for expressive piano performance synthesis from music scores</a>, by Jingjing Tang, Erica Cooper, Xin Wang, Junichi Yamagishi, György Fazekas</p>\n</li>\n</ul>\n<p>The following paper accepted at the <b>IEEE Open Journal of Signal Processing ICASSP track</b> will be presented at the conference:</p>\n<ul>\n<li><a href=\"https://ieeexplore.ieee.org/document/10839319\">LC-Protonets: multi-label few-shot learning for world music audio tagging</a>, by Charilaos Papaioannou, Emmanouil Benetos, Alexandros Potamianos</li>\n</ul>\n<p>Finally, the following paper will be presented as part of the <b>special session on 50 years of Audio and Acoustic Signal Processing</b>:</p>\n<ul>\n<li><a href=\"https://ieeexplore.ieee.org/document/10888947/\">Twenty-five years of MIR research: achievements, practices, evaluations, and future challenges</a>, by Geoffroy Peeters, Zafar Rafii, Magdalena Fuentes, Zhiyao Duan, Emmanouil Benetos, Juhan Nam, Yuki Mitsufuji</li>\n</ul>\n<p>See you in Hyderabad!</p>","id":"0c5216c9-10f1-5084-957a-04fbb79aa0b9"},{"fields":{"slug":"/news/2025-03-20.AI-that_can_hear"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e12ebbeb6e3fd35a544f18327d7e81a3/5efdf/APT.png","srcSet":"/static/e12ebbeb6e3fd35a544f18327d7e81a3/9b591/APT.png 1063w,\n/static/e12ebbeb6e3fd35a544f18327d7e81a3/db283/APT.png 2126w,\n/static/e12ebbeb6e3fd35a544f18327d7e81a3/5efdf/APT.png 4252w","sizes":"(min-width: 4252px) 4252px, 100vw"},"sources":[{"srcSet":"/static/e12ebbeb6e3fd35a544f18327d7e81a3/8b369/APT.webp 1063w,\n/static/e12ebbeb6e3fd35a544f18327d7e81a3/7d238/APT.webp 2126w,\n/static/e12ebbeb6e3fd35a544f18327d7e81a3/f2efe/APT.webp 4252w","type":"image/webp","sizes":"(min-width: 4252px) 4252px, 100vw"}]},"width":4252,"height":4252}}},"title":"C4DM researchers pioneer AI that can hear: a breakthrough in multimodal generative AI","author":"Emmanouil Benetos","date":"Thu 20 Mar 2025"},"html":"<p>Researchers at the <a href=\"https://www.c4dm.eecs.qmul.ac.uk/\">Centre for Digital Music</a> have developed a novel approach that enables large language models (LLMs) to \"hear\" and \"understand\" sound.</p>\n<p>Read more at: <a href=\"https://www.qmul.ac.uk/eecs/news-and-events/news/items/eecs-phd-researcher-pioneers-ai-that-can-hear-a-breakthrough-in-multimodal-generative-ai.html\">https://www.qmul.ac.uk/eecs/news-and-events/news/items/eecs-phd-researcher-pioneers-ai-that-can-hear-a-breakthrough-in-multimodal-generative-ai.html</a></p>","id":"c8461f50-c693-55ac-8eb4-68945e06df0f"},{"fields":{"slug":"/news/2025-02-11.C4DM-Seminar_audio.md"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Connecting two continents: Immersive Audio Research in Korea Advanced Institute of Science and Technology (KAIST) and Rochester Institute of Technology (RIT)","author":"nicolaus625","date":"Tue 11 Mar 2025"},"html":"<hr>\n<h3>C4DM Seminar: Connecting two continents: Immersive Audio Research in Korea Advanced Institute of Science and Technology (KAIST) and Rochester Institute of Technology (RIT)</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Dr. Sungyoung Kim</p>\n<p><strong>Date/time:</strong> Tuesday 11th March, 13:30-14:30</p>\n<p><strong>Location:</strong> G2, Engineering Building, Mile End Campus, QMUL, E1 4NS</p>\n<h2><b>Title</b>: Connecting two continents: Immersive Audio Research in Korea Advanced Institute of Science and Technology (KAIST) and Rochester Institute of Technology (RIT)</h2>\n<p><b>Abstract</b>:\nImmersive audio is experiencing a rapid boom across various fields, driven by AI advancements that open new possibilities in areas such as auditory attention and aural heritage preservation. This presentation explores the researcher's journey across Japan, the U.S., and Korea, highlighting how auditory immersion has fostered interdisciplinary collaborations and generated new insights. Key projects include aural heritage preservation of world heritage sites in Peru and traditional temples in Korea, neural decoding of a listener’s spatial attention, and the development of a game-based auditory training program for hard-of-hearing listeners. The presentation will also introduce a new collaborative initiative aimed at integrating sound recording, music, neuroscience, and AI to further explore the transformative potential of immersive audio in future research and applications.</p>\n<p><b>Bio</b>:\nSungyoung Kim received a B.S. degree from Sogang University, Korea, and Master of Music and Ph.D. from McGill University, Canada. Currently, he works for the Korea Advanced Institute of Science and Technology (KAIST) and Rochester Institute of Technology (RIT) as an associate professor. His research interests are rendering and perceptual evaluation of spatial audio, digital preservation of aural heritage, and auditory training for hearing rehabilitation. He leads the Applied and Innovative Research for Immersive Audio (AIRIS) laboratory (<a href=\"https://airislab.kaist.ac.kr/\">https://airislab.kaist.ac.kr/</a> ).</p>","id":"6b34adf2-d954-59a0-88b1-6a1b6932314c"}]}}}
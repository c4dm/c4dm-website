{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/YIFgS8she58","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/Benetos-L-Acoustics"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Online speech enhancement in scenarios with low direct-to-reverberant-ratio","author":"Dr Emmanouil Benetos (PI), Dr Aidan Hogg (CI)","date":null,"link":"https://www.l-acoustics.com/"},"html":"","id":"ce65fc6c-7228-5805-a9b2-6e14e5fd5086"},{"fields":{"slug":"/projects/Benetos-Algorivm"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Project Maestro - AI Musical Analysis Platform","author":"Dr Emmanouil Benetos (PI), Prof Simon Dixon (CI)","date":null,"link":"https://www.ukri.org/councils/innovate-uk/"},"html":"","id":"78c348d8-d78f-5acf-8441-382bc6c4d791"},{"fields":{"slug":"/projects/Benetos-Moises"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Enhancing lyrics transcription with open-source architectures and fine-tuning techniques","author":"Dr Emmanouil Benetos (PI)","date":null,"link":"https://moises.ai/"},"html":"","id":"d61977ff-9345-55a4-b85e-44caffa4a7f0"},{"fields":{"slug":"/projects/Dixon-assessment"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Music Performance Assessment and Feedback","author":"Prof Simon Dixon (PI), Dr Emmanouil Benetos (CI)","date":null,"link":null},"html":"","id":"68ee56ea-59b7-58de-a8c9-805c4aba4357"},{"fields":{"slug":"/projects/Dixon-guitar-transcription"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"High Resolution Guitar Transcription","author":"Prof Simon Dixon (PI)","date":null,"link":null},"html":"","id":"4dee84b0-faa4-549a-a883-d4de0a21e205"},{"fields":{"slug":"/projects/Reiss-ProStyle"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Music Production Style Transfer (ProStyle)","author":"Prof Josh Reiss (PI)","date":null,"link":"https://www.musicweek.com/digital/read/audio-production-start-up-roex-awarded-250-000-grant-by-innovate-uk-s-ai-funding-competition/089706"},"html":"","id":"473b36de-2173-5bac-bfac-c8d0c8b27c41"}]},"news":{"nodes":[{"fields":{"slug":"/news/2024-09-10.C4DM-Seminar_Gloria_Dal_Santo_Sebastian_J_Schlecht:"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar:  Gloria Dal Santo & Sebastian J. Schlecht:","author":"Admin","date":"Tue 10 Sep 2024"},"html":"<h3>C4DM Seminar: Gloria Dal Santo &#x26; Sebastian J. Schlecht: Machine Learning-Based Artificial Reverberation &#x26; Non-stationary Noise Removal from Repeated sweep Measurements</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nGloria Dal Santo &#x26; Sebastian J. Schlecht</p>\n<p><strong>Date/time:  TUesday, 10th September 2024, 1.30pm</strong></p>\n<p>**Location: TBA, Graduate Centre Building, Mile End Campus, QMUL, E1 4NS **\nZoom: <a href=\"https://qmul-ac-uk.zoom.us/j/9798452959\">https://qmul-ac-uk.zoom.us/j/9798452959</a></p>\n<h2><b>Title</b>: Machine Learning-Based Artificial Reverberation &#x26; Non-stationary Noise Removal from Repeated sweep Measurements</h2>\n<p><b>Abstract</b>:\nGloria: The Feedback Delay Network (FDN) is a widely used approach in artificial reverberation, structured by generalizing the parallel comb-filter architecture through the interconnection of delays via a feedback matrix. Motivated by its cost-effectiveness, our research explores the FDN topology and integrates it into a machine learning framework with the aim of defining a novel methodology for accurate real-time room simulation. We are working towards a new paradigm for Room Impulse Response synthesis, where accuracy, cost-effectiveness, and ease of application coexist.</p>\n<p>Sebastian: Acoustic measurements using sine sweeps are prone to background noise and non-stationary disturbances.\nRepeated measurements can be averaged to improve the resulting signal-to-noise ratio. However, averaging leads to poor\nrejection of non-stationary high-energy disturbances and, in the case of a time-variant environment, causes attenuation at\nhigh frequencies. This paper proposes a robust method to combine repeated sweep measurements using across-measurement\nmedian filtering in the time-frequency domain. The method, called Mosaic, successfully rejects non-stationary noise, sup-\npresses background noise and is more robust toward time variation than averaging. The proposed method allows high-\nquality measurement of impulse responses in a noisy environment.</p>\n<p><b>Bio</b>:\nGloria Dal Santo received the B.Sc. degree in Electronic and Communications Engineering from Politecnico di Torino, Turin, Italy, and the M.Sc. degree in Electrical and Electronic Engineering from the Ecole Polytechnique Fédérale de Lausanne, Lausanne, Switzerland, in 2020 and 2022. She is currently working toward the Doctoral degree with the Acoustics Lab, Aalto University, Espoo, Finland. Her research interests include artificial reverberation and audio applications of machine learning.</p>\n<p>Sebastian J. Schlecht (Senior Member, IEEE) received the Diploma in applied mathematics from the University of Trier, Trier, Germany, in 2010, and the M.Sc. degree in digital music processing from the School of Electronic Engineering and Computer Science, Queen Mary University of London, London, U.K., in 2011, and the Doctoral degree with the International Audio Laboratories Erlangen, Erlangen, Germany, on artificial spatial reverberation and reverberation enhancement systems in 2017.\nHe is currently an associate Professor for Signal Processing at the Friedrich-Alexander-Universität Erlangen-Nürnberg, Germany. This position is part of the Chair of Multimedia Communications and Signal Processing. Until 2024, he was Professor of Practice at Acoustics Lab, Aalto University, Department of Information and Communications Engineering, and Media Lab, Department of Art and Media, Aalto University, Espoo, Finland. From 2012 to 2019, he was also an External Research and Development Consultant and Lead Developer of the 3D Reverb algorithm with Fraunhofer IIS, Erlangen.</p>","id":"890cb0ea-9268-5156-bb0f-88edad5fc2cb"},{"fields":{"slug":"/news/2024-09-09.C4DM-Seminar_Peter_Meier"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar:  Peter Meier","author":"Admin","date":"Mon 09 Sep 2024"},"html":"<h3>C4DM Seminar: Peter Meier: Real-Time Beat Tracking and Control Signal Generation for Interactive Music Applications</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nPeter Meier</p>\n<p><strong>Date/time:  Monday, 9th September 2024, 2pm</strong></p>\n<p>**Location: GC105, Graduate Centre Building, Mile End Campus, QMUL, E1 4NS **\nZoom: <a href=\"https://qmul-ac-uk.zoom.us/j/2387202947\">https://qmul-ac-uk.zoom.us/j/2387202947</a></p>\n<h2><b>Title</b>: Real-Time Beat Tracking and Control Signal Generation for Interactive Music Applications</h2>\n<p><b>Abstract</b>: In this seminar, we will explore the theory and application of a real-time beat tracking system based on the Predominant Local Pulse (PLP) method. We will start by detailing how the traditional PLP algorithm has been transformed into a real-time procedure capable of operating with zero latency. This real-time adaptation not only tracks beat positions but also provides dynamic insights such as beat context, stability, and lookahead predictions for each frame of real-time audio. Following the theoretical overview, we will demonstrate practical applications of the system's outputs. These include the generation of beat-synchronous Low Frequency Oscillators (LFOs) and confidence-based control signals for dynamic manipulation of audio effect parameters in real-time. We will showcase a prototype audio plugin integrated within a Digital Audio Workstation (DAW), highlighting its utility in mixing scenarios and live music performances. Additionally, we will present various audio examples that use our system for controlling different audio effects in sync with the beat of the music.</p>\n<p><b>Bio</b>: Peter Meier is a PhD student at the International Audio Laboratories Erlangen, a joint institution of Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU) and the Fraunhofer Institute for Integrated Circuits (IIS) in Germany. Under the supervision of Prof. Meinard Müller, his research focuses on interactive music analysis, aiming to transform offline algorithms into real-time applications for creative music making and educational music gaming. Peter holds a Master's degree in Media Technology with a specialization in Media Informatics from the Deggendorf Institute of Technology in Germany, where he also gained 15 years of experience working as an audio engineer in the university's audio labs. Outside his academic work, Peter is a passionate musician who enjoys singing, drumming, and playing guitar.</p>","id":"955d4337-37c9-5966-b315-50769f1018be"},{"fields":{"slug":"/news/2024-09-03.C4DM-Seminar_Kazunobu_KONDO"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar:  Kazunobu KONDO (Yamaha)","author":"Admin","date":"Tue 03 Sep 2024"},"html":"<h3>C4DM Seminar: Kazunobu KONDO: Introduction of Yamaha Research and Development division and global research internship program</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nKazunobu KONDO</p>\n<p><strong>Date/time:  Tuesday, 3rd September 2024, 1.30pm</strong></p>\n<p>**Location: GC601 Montagu LT, Graduate Centre Building, Mile End Campus, QMUL, E1 4NS **\nZoom: <a href=\"https://qmul-ac-uk.zoom.us/j/9798452959\">https://qmul-ac-uk.zoom.us/j/9798452959</a></p>\n<h2><b>Title</b>: Introduction of Yamaha Research and Development division and global research internship program</h2>\n<p><b>Abstract</b>: Introducing our research direction, selected research topics, facilities of Yamaha Research and Development division.\nKansei (in Japanese) for music, which is a perceptual perspective of music understanding, is one of our key research quetions.\nMusic performers listen their own sound when performing in order to describe emotional musical expression.\nTherefore, we are analyzing sounds, musical symbols and human behavior to be utilized for AI-powered products.\nFrom September, we will begin accepting for our global internship program for the summer of 2025, in this presentation, the overview of our internship program will be introduced.</p>\n<p><b>Bio</b>: Kazunobu Kondo received the B.E., M.E. and Ph.D. degrees from Nagoya University, Japan, in 1991, 1993, and 2014, respectively.\nHe joined the Electronics Development Center, Yamaha Co., Ltd. in 1993.\nHe is currently a principal engineer of Yamaha Research and Development Division.\nHis research interests include blind source separation, noise reduction, and dereverbetation.\nHe is a member of the IEICE, the Acoustical Society of Japan and the Audio Engineering Society, and an editorial board member of the Journal of the Audio Engineering Society.</p>","id":"8b30fb78-449a-5470-953b-d4ae01280258"},{"fields":{"slug":"/news/2024-08-31.C4DM-at_Interspeech_2024"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/8c174aab3a9d4d0c7f6f5b3f49e4192b/3175f/Interspeech-2024.jpg","srcSet":"/static/8c174aab3a9d4d0c7f6f5b3f49e4192b/16e4b/Interspeech-2024.jpg 102w,\n/static/8c174aab3a9d4d0c7f6f5b3f49e4192b/d4e31/Interspeech-2024.jpg 203w,\n/static/8c174aab3a9d4d0c7f6f5b3f49e4192b/3175f/Interspeech-2024.jpg 406w","sizes":"(min-width: 406px) 406px, 100vw"},"sources":[{"srcSet":"/static/8c174aab3a9d4d0c7f6f5b3f49e4192b/9b4d3/Interspeech-2024.webp 102w,\n/static/8c174aab3a9d4d0c7f6f5b3f49e4192b/6f337/Interspeech-2024.webp 203w,\n/static/8c174aab3a9d4d0c7f6f5b3f49e4192b/3a205/Interspeech-2024.webp 406w","type":"image/webp","sizes":"(min-width: 406px) 406px, 100vw"}]},"width":406,"height":406}}},"title":"C4DM at Interspeech 2024","author":"Christos Plachouras","date":"Sat 31 Aug 2024"},"html":"<p>On 31 August to 5 September, C4DM members will participate in <a href=\"https://interspeech2024.org/\">Interspeech 2024</a> and its <a href=\"https://interspeech2024.org/satellite/\">satellite events</a>. Interspeech is the premier international conference for research on the science and technology of spoken language processing.</p>\n<p>C4DM PhD student Chin-Yun Yu will present his paper with C4DM academic Dr George Fazekas  \"<a href=\"https://arxiv.org/abs/2406.05128\">Differentiable Time-Varying Linear Prediction in the Context of End-to-End Analysis-by-Synthesis</a>\". The paper introduces improvements to the GOLF voice synthesizer, by implementing a joint filtering approach for noise and harmonics using a single LP (Linear Prediction) filter, resembling a classic source-filter model, and replacing frame-wise approximation with sample-by-sample LP processing, implemented efficiently in C++ and CUDA. These modifications result in smoother spectral envelopes, reduced artefacts, and improved performance in listening tests compared to other baselines. More information can be found <a href=\"https://yoyololicon.github.io/golf2-demo/\">here</a>.</p>\n<p>C4DM PhD student Antonella Torrisi will present her paper with C4DM PhD student Inês Nolasco and C4DM academic Emmanouil Benetos entitled \"<a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/98700\">Exploratory analysis of early-life chick calls</a>\" at the <a href=\"https://vihar-2024.vihar.org/\">International Workshop on Vocal Interactivity in-and-between Humans, Animals and Robots (VIHAR)</a>. The paper describes the development of a computational framework for automatically detecting and analysing vocalizations of one-day-old chicks (Gallus gallus). Various clustering techniques are used to explore whether chick calls can be categorised into distinct types or if they exist on a continuous spectrum, aiming to provide a more systematic and unbiased approach to understanding chick vocal behaviour compared to previous studies.</p>\n<p>C4DM PhD student Farida Yusuf is part of the programme committee for the <a href=\"https://sites.google.com/view/yfrsw-2024\">Young Female Researchers in Speech Workshop (YFRSW)</a>. The workshop is designed for Bachelor’s and Master’s students currently engaged in speech science and technology research, aiming to promote interest in the field among those who haven’t yet committed to pursuing a PhD. It features panel discussions, student poster presentations, and mentoring sessions, providing participants with opportunities to showcase their research and engage with PhD students and senior researchers in the field.</p>\n<p>See you at Interspeech!</p>","id":"6bbbc6f5-d989-5582-99fd-a096953860ad"},{"fields":{"slug":"/news/2024-08-05.C4DM-at_EUSIPCO_2024"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#782868","images":{"fallback":{"src":"/static/d080ae1a003b1df4a4449f601bb0943e/b47f1/EUSIPCO2024.jpg","srcSet":"/static/d080ae1a003b1df4a4449f601bb0943e/89572/EUSIPCO2024.jpg 159w,\n/static/d080ae1a003b1df4a4449f601bb0943e/e3831/EUSIPCO2024.jpg 317w,\n/static/d080ae1a003b1df4a4449f601bb0943e/b47f1/EUSIPCO2024.jpg 634w","sizes":"(min-width: 634px) 634px, 100vw"},"sources":[{"srcSet":"/static/d080ae1a003b1df4a4449f601bb0943e/0993c/EUSIPCO2024.webp 159w,\n/static/d080ae1a003b1df4a4449f601bb0943e/1ce72/EUSIPCO2024.webp 317w,\n/static/d080ae1a003b1df4a4449f601bb0943e/6582b/EUSIPCO2024.webp 634w","type":"image/webp","sizes":"(min-width: 634px) 634px, 100vw"}]},"width":634,"height":634}}},"title":"C4DM at EUSIPCO 2024","author":"Emmanouil Benetos","date":"Mon 05 Aug 2024"},"html":"<p>On 26-30 August, C4DM researchers will participate in the <b><a href=\"https://eusipcolyon.sciencesconf.org/\">32nd European Signal Processing Conference (EUSIPCO 2024)</a></b> in Lyon, France. EUSIPCO is the flagship conference of the European Signal Processing Society (EURASIP) and offers a comprehensive technical program addressing all the latest developments in research and technology for signal processing.</p>\n<p>Centre for Digital Music members will be presenting the following works:</p>\n<ul>\n<li>\n<p><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/97337\">Towards building an end-to-end multilingual automatic lyrics transcription model</a>, by Jiawen Huang and Emmanouil Benetos</p>\n</li>\n<li>\n<p><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/97744\">Mind the domain gap: a systematic analysis on bioacoustic sound event detection</a>, by Jinhua Liang, Ines Nolasco, Burooj Ghani, Huy Phan, Emmanouil Benetos, and Dan Stowell</p>\n</li>\n</ul>\n<p>See you at EUSIPCO!</p>","id":"8f1f8257-f06a-5e41-989d-def19c908a44"},{"fields":{"slug":"/news/2024-07-30.C4DM-Seminar_Hyon_Kim"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar:  Hyon Kim","author":"Admin","date":"Tue 30 Jul 2024"},"html":"<h3>C4DM Seminar: Hyon Kim: Score Informed Note-level MIDI Velocity Estimation and Its Transcription into Symbolics</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nHyon Kim</p>\n<p><strong>Date/time:  Tuesday, 30th July 2024, 3pm</strong></p>\n<p>**Location: G2, ENG, Mile End Campus, QMUL, E1 4NS **\nZoom: <a href=\"https://qmul-ac-uk.zoom.us/j/9798452959\">https://qmul-ac-uk.zoom.us/j/9798452959</a></p>\n<h2><b>Title</b>: Score Informed Note-level MIDI Velocity Estimation and Its Transcription into Symbolics</h2>\n<p><b>Abstract</b>: It is a well known fact that the dynamics in piano performance gives significant effect in expressiveness. Taking the polyphonic nature of the instrument into account, analysing information to form dynamics for each performed note has significant meaning to understand piano performance in a quantitative way. It is also a key element in an education context for piano learners.　\nIn this study, we developed a model for estimating MIDI velocity for each note, as one of indicators to represent loudness, with a condition of score by a Deep Neural Network (DNN) and Feature-wise Linear Modulation (FiLM) conditioning. Additionally, we have conducted research to map MIDI velocities to dynamics markings by combining performance MIDI roll and MusicXML information into one sequence.</p>\n<p><b>Bio</b>: Hyon Kim is a PhD student at Music Technology Group, Universitat Pompeu Fabra under the supervision of Prof. Xavier Serra. Currently, Hyon is a Visiting Researcher at C4DM, QMUL, working under the supervision of Dr. Emmanouil Benetos. His research interests include modeling the dynamic information of piano performance and transcribing it into MIDI roll and symbolic representations using various DNN methods in a multimodal fashion, incorporating both score and audio data.</p>","id":"de847f8c-3aaf-5694-a7da-262f0eef140d"}]}}}
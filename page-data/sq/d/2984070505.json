{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/YIFgS8she58","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/Sandler-Artificial-Neuroscience"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png","srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/2fd20/EPSRC.png 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/de391/EPSRC.png 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/d66e1/EPSRC.webp 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/e7160/EPSRC.webp 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/5f169/EPSRC.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Artificial Neuroscience: metrology and engineering for Deep Learning using Linear Algebra","author":"Prof Mark Sandler (PI), Boris Khoruzhenko (CI)","date":null,"link":"https://www.ukri.org/councils/epsrc/"},"html":"","id":"c1314000-fe3b-5175-88a2-f239d861ddad"},{"fields":{"slug":"/projects/Barthet-Netz"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Netz: A Novel XR Musical Instrument","author":"Dr Mathieu Barthet (PI)","date":null,"link":"https://iuk-business-connect.org.uk/programme/icure/"},"html":"","id":"0040b455-65cf-5ea1-a7ce-38ab70570dae"},{"fields":{"slug":"/projects/Benetos-Algorivm"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Project Maestro - AI Musical Analysis Platform","author":"Dr Emmanouil Benetos (PI), Prof Simon Dixon (CI)","date":null,"link":"https://www.ukri.org/councils/innovate-uk/"},"html":"","id":"78c348d8-d78f-5acf-8441-382bc6c4d791"},{"fields":{"slug":"/projects/Benetos-L-Acoustics"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Online speech enhancement in scenarios with low direct-to-reverberant-ratio","author":"Dr Emmanouil Benetos (PI), Dr Aidan Hogg (CI)","date":null,"link":"https://www.l-acoustics.com/"},"html":"","id":"ce65fc6c-7228-5805-a9b2-6e14e5fd5086"},{"fields":{"slug":"/projects/Benetos-Moises"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Enhancing lyrics transcription with open-source architectures and fine-tuning techniques","author":"Dr Emmanouil Benetos (PI)","date":null,"link":"https://moises.ai/"},"html":"","id":"d61977ff-9345-55a4-b85e-44caffa4a7f0"},{"fields":{"slug":"/projects/Dixon-assessment"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Music Performance Assessment and Feedback","author":"Prof Simon Dixon (PI), Dr Emmanouil Benetos (CI)","date":null,"link":null},"html":"","id":"68ee56ea-59b7-58de-a8c9-805c4aba4357"}]},"news":{"nodes":[{"fields":{"slug":"/news/2025-03-24.C4DM-at_ICASSP_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/d7937701a02a2623f162148d18236437/21bc1/ICASSP2025.png","srcSet":"/static/d7937701a02a2623f162148d18236437/0ae7b/ICASSP2025.png 106w,\n/static/d7937701a02a2623f162148d18236437/facc2/ICASSP2025.png 212w,\n/static/d7937701a02a2623f162148d18236437/21bc1/ICASSP2025.png 424w","sizes":"(min-width: 424px) 424px, 100vw"},"sources":[{"srcSet":"/static/d7937701a02a2623f162148d18236437/7a2a0/ICASSP2025.webp 106w,\n/static/d7937701a02a2623f162148d18236437/fbd6b/ICASSP2025.webp 212w,\n/static/d7937701a02a2623f162148d18236437/ce7ff/ICASSP2025.webp 424w","type":"image/webp","sizes":"(min-width: 424px) 424px, 100vw"}]},"width":424,"height":424}}},"title":"C4DM at ICASSP 2025","author":"Emmanouil Benetos","date":"Mon 24 Mar 2025"},"html":"<p>On 6-11 April 2025, several C4DM researchers will participate at the <b><a href=\"https://2025.ieeeicassp.org/\">2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2025)</a></b>. ICASSP is the leading conference in the field of signal processing and the flagship event of the <a href=\"https://signalprocessingsociety.org/\">IEEE Signal Processing Society</a>.</p>\n<p>As in previous years, the Centre for Digital Music will have a strong presence at the conference, both in terms of numbers and overall impact. The below 14 papers presented at ICASSP 2025 are authored or co-authored by C4DM members:</p>\n<ul>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10890076/\">Acoustic identification of individual animals with hierarchical contrastive learning</a>, by Ines Nolasco, Ilyass Moummad, Dan Stowell, Emmanouil Benetos</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10888157/\">Evaluating contrastive methodologies for music representation learning using playlist data</a>, by Gregor Meehan, Johan Pauwels</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10888557/\">GraFPrint: a GNN-based approach for audio identification</a>, by Aditya Bhattacharjee, Shubhr Singh, Emmanouil Benetos</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10887996/\">Guitar-TECHS: an electric guitar dataset covering techniques, musical excerpts, chords and scales using a diverse array of hardware</a>, by Hegel Pedroza, Wallace Abreu, Ryan M. Corey, Iran R. Roman</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10889639/\">Hybrid losses for hierarchical embedding learning</a>, by Haokun Tian, Stefan Lattner, Brian McFee, Charalampos Saitis</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10839319\">LC-Protonets: multi-label few-shot learning for world music audio tagging</a>, by Charilaos Papaioannou, Emmanouil Benetos, Alexandros Potamianos</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10889683/\">Learning control of neural sound effects synthesis from physically inspired models</a>, by Yisu Zong, Joshua Reiss</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10887766/\">Learning music audio representations with limited data</a>, by Christos Plachouras, Emmanouil Benetos, Johan Pauwels</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10890270/\">Leave-One-EquiVariant: alleviating invariance-related information loss in contrastive music representations</a>, by Julien Guinot, Elio Quinton, György Fazekas</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10890467/\">LHGNN: local-higher order graph neural networks for audio classification and tagging</a>, by Shubhr Singh, Emmanouil Benetos, Huy Phan, Dan Stowell</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10890522/\">LLaQo: towards a query-based coach in expressive performance assessment</a>, by Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Simon Dixon, Shinichi Furuya</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10890049/\">Music2Latent2: audio compression with summary embeddings and autoregressive decoding</a>, by Marco Pasini, Stefan Lattner, György Fazekas</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10890623/\">Towards an integrated approach for expressive piano performance synthesis from music scores</a>, by Jingjing Tang, Erica Cooper, Xin Wang, Junichi Yamagishi, György Fazekas</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10888947/\">Twenty-five years of MIR research: achievements, practices, evaluations, and future challenges</a>, by Geoffroy Peeters, Zafar Rafii, Magdalena Fuentes, Zhiyao Duan, Emmanouil Benetos, Juhan Nam, Yuki Mitsufuji</p>\n</li>\n</ul>\n<p>See you all in Hyderabad!</p>","id":"0c5216c9-10f1-5084-957a-04fbb79aa0b9"},{"fields":{"slug":"/news/2025-03-20.AI-that_can_hear"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e12ebbeb6e3fd35a544f18327d7e81a3/5efdf/APT.png","srcSet":"/static/e12ebbeb6e3fd35a544f18327d7e81a3/9b591/APT.png 1063w,\n/static/e12ebbeb6e3fd35a544f18327d7e81a3/db283/APT.png 2126w,\n/static/e12ebbeb6e3fd35a544f18327d7e81a3/5efdf/APT.png 4252w","sizes":"(min-width: 4252px) 4252px, 100vw"},"sources":[{"srcSet":"/static/e12ebbeb6e3fd35a544f18327d7e81a3/8b369/APT.webp 1063w,\n/static/e12ebbeb6e3fd35a544f18327d7e81a3/7d238/APT.webp 2126w,\n/static/e12ebbeb6e3fd35a544f18327d7e81a3/f2efe/APT.webp 4252w","type":"image/webp","sizes":"(min-width: 4252px) 4252px, 100vw"}]},"width":4252,"height":4252}}},"title":"C4DM researchers pioneer AI that can hear: a breakthrough in multimodal generative AI","author":"Emmanouil Benetos","date":"Thu 20 Mar 2025"},"html":"<p>Researchers at the <a href=\"https://www.c4dm.eecs.qmul.ac.uk/\">Centre for Digital Music</a> have developed a novel approach that enables large language models (LLMs) to \"hear\" and \"understand\" sound.</p>\n<p>Read more at: <a href=\"https://www.qmul.ac.uk/eecs/news-and-events/news/items/eecs-phd-researcher-pioneers-ai-that-can-hear-a-breakthrough-in-multimodal-generative-ai.html\">https://www.qmul.ac.uk/eecs/news-and-events/news/items/eecs-phd-researcher-pioneers-ai-that-can-hear-a-breakthrough-in-multimodal-generative-ai.html</a></p>","id":"c8461f50-c693-55ac-8eb4-68945e06df0f"},{"fields":{"slug":"/news/2025-02-11.C4DM-Seminar_audio.md"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Connecting two continents: Immersive Audio Research in Korea Advanced Institute of Science and Technology (KAIST) and Rochester Institute of Technology (RIT)","author":"nicolaus625","date":"Tue 11 Mar 2025"},"html":"<hr>\n<h3>C4DM Seminar: Connecting two continents: Immersive Audio Research in Korea Advanced Institute of Science and Technology (KAIST) and Rochester Institute of Technology (RIT)</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Dr. Sungyoung Kim</p>\n<p><strong>Date/time:</strong> Tuesday 11th March, 13:30-14:30</p>\n<p><strong>Location:</strong> G2, Engineering Building, Mile End Campus, QMUL, E1 4NS</p>\n<h2><b>Title</b>: Connecting two continents: Immersive Audio Research in Korea Advanced Institute of Science and Technology (KAIST) and Rochester Institute of Technology (RIT)</h2>\n<p><b>Abstract</b>:\nImmersive audio is experiencing a rapid boom across various fields, driven by AI advancements that open new possibilities in areas such as auditory attention and aural heritage preservation. This presentation explores the researcher's journey across Japan, the U.S., and Korea, highlighting how auditory immersion has fostered interdisciplinary collaborations and generated new insights. Key projects include aural heritage preservation of world heritage sites in Peru and traditional temples in Korea, neural decoding of a listener’s spatial attention, and the development of a game-based auditory training program for hard-of-hearing listeners. The presentation will also introduce a new collaborative initiative aimed at integrating sound recording, music, neuroscience, and AI to further explore the transformative potential of immersive audio in future research and applications.</p>\n<p><b>Bio</b>:\nSungyoung Kim received a B.S. degree from Sogang University, Korea, and Master of Music and Ph.D. from McGill University, Canada. Currently, he works for the Korea Advanced Institute of Science and Technology (KAIST) and Rochester Institute of Technology (RIT) as an associate professor. His research interests are rendering and perceptual evaluation of spatial audio, digital preservation of aural heritage, and auditory training for hearing rehabilitation. He leads the Applied and Innovative Research for Immersive Audio (AIRIS) laboratory (<a href=\"https://airislab.kaist.ac.kr/\">https://airislab.kaist.ac.kr/</a> ).</p>","id":"6b34adf2-d954-59a0-88b1-6a1b6932314c"},{"fields":{"slug":"/news/2025-02-10.C4DM-Seminar_Pipa.md"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Pipa Performative Score Dataset Construction based on Intelligent Installation and Computer Vision","author":"nicolaus625","date":"Mon 10 Feb 2025"},"html":"<hr>\n<h3>C4DM Seminar: Pipa Performative Score Dataset Construction based on Intelligent Installation and Computer Vision</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nDr. Rongfeng Li</p>\n<p><strong>Date/time:  Monday, 10 February, 14:30</strong></p>\n<p>**Location: G2, Engineering Building, Mile End Campus, QMUL, E1 4NS **\nZoom: not available</p>\n<h2><b>Title</b>: Pipa Performative Score Dataset Construction based on Intelligent Installation and Computer Vision</h2>\n<p><b>Abstract</b>:\nThis study addresses the limitations of AI models in analyzing traditional music, particularly in Western classical, by focusing on the intertwined composition and performance aspects of Chinese music. Using the Pipa, a Chinese string instrument, an Arduino-based system captures real-time finger techniques and string touches. Computer vision algorithms complement this by analyzing performance videos to extract note movements and playing techniques. This approach has resulted in an hour of Pipa performance data being converted into a detailed performative score, enriching AI’s capability in music digitization. These intelligent devices and recognition algorithms are instrumental for future AI-driven analysis of traditional music performances.</p>\n<p>NB: the pipa is a Chinese instrument that is similar to a lute.</p>\n<p><b>Bio</b>:\nLi Rongfeng: Associate Professor, Master's Supervisor of the School of Digital Media and Design Art, Beijing University of Posts and Telecommunications, a member of the Art and Artificial Intelligence Committee of the Chinese Association for Artificial Intelligence, and a member of the Future Science and Technology Committee of the Chinese Musical Instrument Association. In 2012, he graduated from the Artificial Intelligence Group of the Institute of Network and Information Systems, School of Information Science and Technology, Peking University, with a doctorate degree. His research fields include music technology, machine learning, etc. He has hosted the research work of the “Machine Learning-based Automatic Translation of Chinese Gong Chi Music” project funded by the Ministry of Education’s Social Science Fund and results were published onhttp://jiugong.chimusic.net. The \"Tianxia Musical Instrument\" project he cooperated with the China Conservatory of Music and the \"AI Solfeggio\" project in cooperation with the Central University For Nationalities have been online.</p>","id":"e52f4341-a22e-5bff-a059-bab295c5259e"},{"fields":{"slug":"/news/2025-02-05.C4DM-at_AAAI_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#083878","images":{"fallback":{"src":"/static/a8d43a863464c2bb2992bb17a1b99162/3042c/AAAI2025.png","srcSet":"/static/a8d43a863464c2bb2992bb17a1b99162/2e067/AAAI2025.png 151w,\n/static/a8d43a863464c2bb2992bb17a1b99162/6d16b/AAAI2025.png 302w,\n/static/a8d43a863464c2bb2992bb17a1b99162/3042c/AAAI2025.png 603w","sizes":"(min-width: 603px) 603px, 100vw"},"sources":[{"srcSet":"/static/a8d43a863464c2bb2992bb17a1b99162/43f11/AAAI2025.webp 151w,\n/static/a8d43a863464c2bb2992bb17a1b99162/5a5e0/AAAI2025.webp 302w,\n/static/a8d43a863464c2bb2992bb17a1b99162/9e027/AAAI2025.webp 603w","type":"image/webp","sizes":"(min-width: 603px) 603px, 100vw"}]},"width":603,"height":603}}},"title":"C4DM at AAAI 2025","author":"Admin","date":"Wed 05 Feb 2025"},"html":"<p>From the 25th February to 4th March 2025, two C4DM researchers will participate at the 39th Annual AAAI Conference on Artificial Intelligence (AAAI 2025), including the one day workshop on Artificial Intelligence for Music. AAAI is one of the leading conferences on artificial intelligence. This year AAAI will take place onsite in Philadelphia (PA, USA).</p>\n<p>The following works were authored/coauthored by C4DM PhD students and academic staff:</p>\n<p>-- <a href=\"https://arxiv.org/abs/2412.16526\">\"Text2MIDI: Generating Symbolic Music from Captions\"</a> by Keshav Bhandari, Abhinaba Roy, Kyra Wang, Geeta Puri, Simon Colton, and Dorien Herremans</p>\n<p>-- <a href=\"https://www.researchgate.net/publication/388274224_Towards_Music_Industry_50_Perspectives_on_Artificial_Intelligence\">\"Towards Music Industry 5.0: Perspectives on Artificial Intelligence\"</a> by Alexander Williams and Mathieu Barthet (AAAI workshop on Artificial Intelligence for Music)</p>\n<p>See you at AAAI!</p>","id":"78f98a34-e64e-5335-8724-ab394b03f0ff"},{"fields":{"slug":"/news/2025-24-01.C4DM-Seminar_Rodrigo_Constanzo"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM & AIL Seminar: Rodrigo Constanzo","author":"Admin","date":"Fri 24 Jan 2025"},"html":"<h3>C4DM &#x26; AIL Seminar: Rodrigo Constanzo: Hitting Laptops With Drumsticks: Approaches to Performing With Drums and Electronics</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Imperial College London, Dyson School of Design Engineering</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<h4>Augmented Instruments Lab Open Seminars</h4>\n<p><strong>Seminar by:</strong><br>\nRodrigo Constanzo (Royal Nothern College of Music)</p>\n<p><strong>Date/time:  Friday, 24th January 2025, 11am</strong></p>\n<p>**Location: Boardroom (Ground floor, opposite side to the café), Dyson School of Design Engineering, Imperial College London (South Kensington Campus) **</p>\n<p>YouTube Link: <a href=\"https://www.youtube.com/live/UlILHqq3aXs?si=hdisHl9UqtgZMq9d\">https://www.youtube.com/live/UlILHqq3aXs?si=hdisHl9UqtgZMq9d</a></p>\n<h2><b>Title</b>: Hitting Laptops With Drumsticks: Approaches to Performing With Drums and Electronics</h2>\n<p><b>Abstract</b>: Rodrigo Constanzo will present some of his recent work using drums and electronics covering a range of topics including audio analysis, meta-instrument design, machine learning, and absolute position tracking on a drum head.</p>\n<p><b>Bio</b>: Rodrigo Constanzo makes art. He thinks this is an important thing to do. The art he makes is generally smeared in time, in the form of music. He improvises and acts as an antennae to the beauty, electricity, and endless surprise that is living a crazy life. He composes and tries to create new sounds, interactions and behaviors that he find interesting and challenging. He performs his own music and the music of others on a variety of instruments, with close friends. He believes in magic. He believes in sharing things. He believes in teaching. He believes in openness. He loves sweet jams, and avoids speaking with a complicated vocabulary as much as possible. He tries to live as presently and honestly as possible.</p>","id":"e41171ce-74fd-5b55-80e4-cb9c07f1ed40"}]}}}
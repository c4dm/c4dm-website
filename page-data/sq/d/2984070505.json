{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/YIFgS8she58","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/EPSRC-additional-skills"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png","srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/2fd20/EPSRC.png 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/de391/EPSRC.png 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/d66e1/EPSRC.webp 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/e7160/EPSRC.webp 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/5f169/EPSRC.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"EPSRC additional skills funding summer 2025","author":"Prof Akram Alomainy (PI), Prof Simon Dixon (CI), Dr Iran Roman (CI) plus 8 others","date":null,"link":"https://www.ukri.org/councils/epsrc/"},"html":"","id":"bc1e4be0-d104-5b53-8c02-61f296532555"},{"fields":{"slug":"/projects/Fazekas-Leverhulme"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/14ae7694223d03edb7966d74ea33ce18/82c11/RAEng.png","srcSet":"/static/14ae7694223d03edb7966d74ea33ce18/2fd20/RAEng.png 125w,\n/static/14ae7694223d03edb7966d74ea33ce18/de391/RAEng.png 250w,\n/static/14ae7694223d03edb7966d74ea33ce18/82c11/RAEng.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/14ae7694223d03edb7966d74ea33ce18/d66e1/RAEng.webp 125w,\n/static/14ae7694223d03edb7966d74ea33ce18/e7160/RAEng.webp 250w,\n/static/14ae7694223d03edb7966d74ea33ce18/5f169/RAEng.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Knowledge-driven Deep Learning for Music Informatics","author":"Dr George Fazekas (PI)","date":null,"link":"https://raeng.org.uk/programmes-and-prizes/programmes/uk-grants-and-prizes/support-for-research/research-awardees/leverhulme-awardees/2025-2026/dr-george-fazekas/"},"html":"","id":"f47b222e-21cf-5c90-bff2-6181b436d1aa"},{"fields":{"slug":"/projects/Fazekas-Steinberg-2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png","srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/4b686/aimcdt.png 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/d2213/aimcdt.png 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png 615w","sizes":"(min-width: 615px) 615px, 100vw"},"sources":[{"srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/b2942/aimcdt.webp 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/46581/aimcdt.webp 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/fa942/aimcdt.webp 615w","type":"image/webp","sizes":"(min-width: 615px) 615px, 100vw"}]},"width":615,"height":615}}},"title":"AI-Powered Audio Loop Generation for Assistive Music Production","author":"Dr George Fazekas (PI)","date":null,"link":null},"html":"","id":"90759bed-47aa-57d3-8ce9-0dbe6a6b40ac"},{"fields":{"slug":"/projects/Ma-Google"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e3011e08b70d2397f6a5f53aca3eac47/b5658/Google.png","srcSet":"/static/e3011e08b70d2397f6a5f53aca3eac47/acb7c/Google.png 256w,\n/static/e3011e08b70d2397f6a5f53aca3eac47/ccc41/Google.png 512w,\n/static/e3011e08b70d2397f6a5f53aca3eac47/b5658/Google.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e3011e08b70d2397f6a5f53aca3eac47/22bfc/Google.webp 256w,\n/static/e3011e08b70d2397f6a5f53aca3eac47/d689f/Google.webp 512w,\n/static/e3011e08b70d2397f6a5f53aca3eac47/67ded/Google.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"title":"Large Language Models for Multimodal Music Understanding and Ethical Audio Generation","author":"Dr Emmanouil Benetos (PI), Yinghao Ma (PhD fellow)","date":null,"link":"https://research.google/programs-and-events/phd-fellowship/"},"html":"","id":"e4f5df67-9d4e-57da-bb16-3efcb2359e70"},{"fields":{"slug":"/projects/Pauwels-Sofilab"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6aad6623ce4e89038628dab00ebc48e0/c8f14/Sofilab.png","srcSet":"/static/6aad6623ce4e89038628dab00ebc48e0/abab1/Sofilab.png 185w,\n/static/6aad6623ce4e89038628dab00ebc48e0/96aa8/Sofilab.png 370w,\n/static/6aad6623ce4e89038628dab00ebc48e0/c8f14/Sofilab.png 739w","sizes":"(min-width: 739px) 739px, 100vw"},"sources":[{"srcSet":"/static/6aad6623ce4e89038628dab00ebc48e0/c5b6a/Sofilab.webp 185w,\n/static/6aad6623ce4e89038628dab00ebc48e0/7fd3d/Sofilab.webp 370w,\n/static/6aad6623ce4e89038628dab00ebc48e0/5bbdd/Sofilab.webp 739w","type":"image/webp","sizes":"(min-width: 739px) 739px, 100vw"}]},"width":739,"height":739}}},"title":"Smart musical corpus technologies","author":"Dr Johan Pauwels (PI)","date":null,"link":"https://sofilab.art/"},"html":"","id":"5b93e208-074a-5baf-a108-023df35a29f0"},{"fields":{"slug":"/projects/Reiss-Yamaha"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Yamaha Visiting Researcher Collaboration","author":"Prof Josh Reiss (PI)","date":null,"link":"https://intelligentsoundengineering.wordpress.com/"},"html":"","id":"145db7f6-77af-5f89-b477-6c4d87952ef4"}]},"news":{"nodes":[{"fields":{"slug":"/news/2025-11-20.C4DM-Seminar_Eloi_Moliner"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Eloi Moliner","author":null,"date":"Thu 20 Nov 2025"},"html":"<h3>C4DM Seminar: Eloi Moliner: Diffusion Models for Audio Effects: From Blind Estimation to Automatic Mixing</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Eloi Moliner</p>\n<p><strong>Date/time:</strong>  Thursday, 20th Nov 2025, 11 am</p>\n<p><strong>Location:</strong> GC203, Graduate Centre, Mile End Campus, Queen Mary University of London</p>\n<p><a href=\"https://teams.microsoft.com/l/meetup-join/19%3ameeting_ZDcxODQ2ZTctN2NkYy00MDExLWEyOWMtNzRkMjlmMGUxMThh%40thread.v2/0?context=%7b%22Tid%22%3a%22569df091-b013-40e3-86ee-bd9cb9e25814%22%2c%22Oid%22%3a%22cad9cb9a-abc2-4aac-ab51-682e8a852753%22%7d\">Teams meeting link</a></p>\n<h2><b>Title</b>: Diffusion Models for Audio Effects: From Blind Estimation to Automatic Mixing</h2>\n<p><b>Abstract</b>:\nThis talk presents a series of recent works exploring the use of diffusion models in the analysis and generation of audio effects. In the first part, I will focus on blind and unsupervised effect estimation, where diffusion models are employed as powerful data-driven priors for recovering clean or unprocessed signals from their altered counterparts, while simultaneously estimating the parameters of a model of the underlying audio effect. In the second part, I will discuss a recent work on automatic music mixing, introducing MEGAMI (Multitrack Embedding Generative Auto MIxing)—a generative framework that models the distribution of professional mixes directly in a multitrack effect embedding space.</p>\n<p><b>Bio</b>:\nEloi Moliner received his Ph.D. degree from the Acoustics Lab of Aalto University, Espoo, Finland, in 2025. He previuosly obtained his M.Sc. degree in Telecommunications Engineering in 2021 and his B.Sc. degree in Telecommunications Technologies and Services Engineering in 2018, both from the Polytechnic University of Catalonia, Spain. He received the Best Student Paper Awards at IEEE ICASSP 2023, IWAENC 2024, and AES AIMLA 2025. His research interests include generative models for audio, audio restoration and enhancement, and audio signal processing.</p>","id":"e7b942ad-e4fe-5988-87fe-7bad707e1b48"},{"fields":{"slug":"/news/2025-11-17.C4DM-at_NeurIPS_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/a20d19fb2ed10997b9bc578028722b0a/baaed/NeurIPS-2025.jpg","srcSet":"/static/a20d19fb2ed10997b9bc578028722b0a/dd515/NeurIPS-2025.jpg 200w,\n/static/a20d19fb2ed10997b9bc578028722b0a/47930/NeurIPS-2025.jpg 400w,\n/static/a20d19fb2ed10997b9bc578028722b0a/baaed/NeurIPS-2025.jpg 800w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[{"srcSet":"/static/a20d19fb2ed10997b9bc578028722b0a/2e34e/NeurIPS-2025.webp 200w,\n/static/a20d19fb2ed10997b9bc578028722b0a/416c3/NeurIPS-2025.webp 400w,\n/static/a20d19fb2ed10997b9bc578028722b0a/c1587/NeurIPS-2025.webp 800w","type":"image/webp","sizes":"(min-width: 800px) 800px, 100vw"}]},"width":800,"height":800}}},"title":"C4DM at NeurIPS 2025","author":"Emmanouil Benetos","date":"Mon 17 Nov 2025"},"html":"<p>On 2-7 December, C4DM researchers will participate at the <a href=\"https://neurips.cc/\">39th Annual Conference on Neural Information Processing Systems (NeurIPS 2025)</a>, taking place in San Diego, USA. NeurIPS is a prestigious annual academic conference and non-profit foundation that fosters the exchange of research in artificial intelligence (AI), machine learning (ML), and computational neuroscience.</p>\n<p>The following papers from C4DM members will be presented at the <b>Datasets and Benchmarks track</b> of NeurIPS 2025:</p>\n<ul>\n<li>\n<p><a href=\"https://openreview.net/forum?id=SSF4qgsNYE\">OmniBench: Towards The Future of Universal Omni-Language Models</a> by Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, King Zhu, Hangyu Guo, Yiming Liang, Jiaheng Liu, Zekun Moore Wang, Jian Yang, Siwei Wu, Xingwei Qu, Jinjie Shi, Xinyue Zhang, Zhenzhu Yang, Yidan WEN, Yanghai Wang, Shihao Li, Zhaoxiang Zhang, Ruibo Liu, Emmanouil Benetos, Wenhao Huang, Chenghua Lin</p>\n</li>\n<li>\n<p><a href=\"https://openreview.net/forum?id=fgmrBJemlQ\">MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix</a> by Ziyang Ma, Yinghao Ma, Yanqiao Zhu, Chen Yang, Yi-Wen Chao, Ruiyang Xu, Wenxi Chen, Yuanzhe Chen, Zhuo Chen, Jian Cong, Kai Li, Keliang Li, Siyou Li, Xinfeng Li, Xiquan Li, Zheng Lian, Yuzhe Liang, Minghao Liu, Zhikang Niu, tianrui wang, Yuping Wang, Yuxuan Wang, Yihao Wu, Guanrou Yang, Jianwei Yu, Ruibin Yuan, Zhisheng Zheng, Ziya Zhou, Haina Zhu, Wei Xue, Emmanouil Benetos, Kai Yu, EngSiong Chng, Xie Chen</p>\n</li>\n</ul>\n<p>The following paper will be presented at the <b>Creative AI track</b> of NeurIPS 2025:</p>\n<ul>\n<li><a href=\"https://openreview.net/forum?id=3yeBer3J5z\">The Ghost in the Keys: A Disklavier Demo for Human-AI Musical Co-Creativity</a> by Louis Bradshaw, Alexander Spangher, Stella Biderman, Simon Colton</li>\n</ul>\n<p>The following papers will be presented at the <b><a href=\"https://aiformusicworkshop.github.io/\">NeurIPS 2025 Workshop on AI for Music</a></b>:</p>\n<ul>\n<li>\n<p><a href=\"https://openreview.net/pdf?id=yL8BrlEqHQ\">The Ghost in the Keys: A Disklavier Demo for Human-AI Musical Co-Creativity</a> by Louis Bradshaw, Alexander Spangher, Stella Biderman, Simon Colton</p>\n</li>\n<li>\n<p><a href=\"https://openreview.net/pdf?id=rXUKO0ysUy\">Perceptually Aligning Representations of Music via Noise-Augmented Autoencoders</a> by Mathias Rose Bjare, Giorgia Cantisani, Marco Pasini, Stefan Lattner, Gerhard Widmer</p>\n</li>\n<li>\n<p><a href=\"https://openreview.net/pdf?id=pbCcvZdHyG\">Evaluating Multimodal Large Language Models on Core Music Perception Tasks</a> by Brandon James Carone, Iran R Roman, Pablo Ripollés</p>\n</li>\n<li>\n<p><a href=\"https://openreview.net/pdf?id=NG187AZ71W\">Advancing Multi-Instrument Music Transcription: Results from the 2025 AMT Challenge</a> by Ojas Chaturvedi, Kayshav Bhardwaj, Tanay Gondil, Benjamin Shiue-Hal Chou, Kristen Yeon-Ji Yun, Yung-Hsiang Lu, Yujia Yan, Sungkyun Chang</p>\n</li>\n</ul>\n<p>Finally, the following paper will be presented at the <a href=\"https://differentiable-systems.github.io/workshop-eurips-2025/#schedule\">Differentiable Systems and Scientific Machine Learning workshop</a> of EurIPS:</p>\n<ul>\n<li>Accelerating Automatic Differentiation of Direct Form Digital Filters by Chin-Yun Yu, George Fazekas</li>\n</ul>\n<p>See you all at NeurIPS!</p>","id":"cadafe6e-e591-5ae2-8b62-52de8a24c21a"},{"fields":{"slug":"/news/2025-11-17.PhD-call-2026"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/f64b6/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/d59af/c4dm.png 431w,\n/static/c138fbce66e709a3f503405435de2f2c/325b2/c4dm.png 862w,\n/static/c138fbce66e709a3f503405435de2f2c/f64b6/c4dm.png 1723w","sizes":"(min-width: 1723px) 1723px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/4b79d/c4dm.webp 431w,\n/static/c138fbce66e709a3f503405435de2f2c/562bc/c4dm.webp 862w,\n/static/c138fbce66e709a3f503405435de2f2c/c4c1b/c4dm.webp 1723w","type":"image/webp","sizes":"(min-width: 1723px) 1723px, 100vw"}]},"width":1723,"height":1723}}},"title":"PhD Studentships at the Centre for Digital Music - Autumn 2026 start","author":"Admin","date":"Mon 17 Nov 2025"},"html":"<p>The Centre for Digital Music at Queen Mary University of London is inviting applications for PhD study for Autumn 2026 start across various funding schemes. Below are suggested PhD topics offered by academics; interested applicants can apply for a PhD under one of those topics, or can propose their own topic. In all cases, prospective applicants are strongly encouraged to contact academics at C4DM to informally discuss prospective research topics.</p>\n<p>Opportunities include internally and externally funded positions for PhD projects to start in Autumn 2026. It is also possible to apply as a self-funded student or with funding from another source. Studentship opportunities include:</p>\n<ul>\n<li>\n<p><a href=\"https://www.qmul.ac.uk/eecs/phd/phd-studentships/neural-dynamics-of-perceptually-aligned-artificial-intelligence/#d.en.1556147\">One UK home PhD studentship</a> (Autumn 2026 start, UK home applicants)</p>\n</li>\n<li>\n<p><a href=\"https://www.qmul.ac.uk/eecs/phd/phd-studentships/se-doctoral-research-studentships-202425-for-underrepresented-groups/#d.en.1100683\">S&#x26;E Doctoral Research Studentships for Underrepresented Groups</a> (UK home applicants, Autumn 2026 start, 3 positions funded across the Faculty of Science &#x26; Engineering)</p>\n</li>\n<li>\n<p><a href=\"https://www.qmul.ac.uk/eecs/phd/phd-studentships/csc-phd-studentships-in-electronic-engineering-and-computer-science/\">CSC PhD Studentships in Electronic Engineering and Computer Science</a> (Autumn 2026 start, Chinese applicants, up to 4 nominations allocated for the Centre for Digital Music)</p>\n</li>\n<li>\n<p><a href=\"https://www.qmul.ac.uk/postgraduate/research/funding_phd/studentships/\">International PhD Funding Schemes</a> (Autumn 2026 start, numerous international funding agencies)</p>\n</li>\n</ul>\n<p>Each funding scheme has a dedicated application process and requirements. S&#x26;E Doctoral Research Studentships and CSC applications close on 28 January 2026 at 5pm UK time. Detailed information and application links can be found on the respective funding scheme pages, following the above links.</p>\n<hr>\n<p><strong>AI Models of Music Understanding</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/sdixon/\">Simon Dixon</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, CSC PhD Studentships, International PhD Funding Scheme</p>\n<p>Music information retrieval (MIR) applies computing and engineering technologies to musical data to satisfy users' information needs. This topic involves the application of artificial intelligence technologies to the processing of music, either in audio or symbolic (score, MIDI) form. The application could be e.g. for software to enhance the listening experience, for music education, for musical practice or for the scientific study of music. Examples of topics of particular interest are automatic transcription of multi-instrumental music, providing feedback to music learners, incorporation of musical knowledge into data-driven deep learning approaches, and tracing the transmission of musical styles, ideas or influences across time or locations.</p>\n<p>It is intentional that this topic description is very general, but it is expected that applicants choose your own specific project within this broad area of research, according to your interests and experience. The research proposal should define the scope of the project, the relationship to the state of the art, the data and methods that you plan to use, and the expected outputs and means of evaluation.</p>\n<hr>\n<p><strong>Bridging Musical Intelligence and Machine Learning: Integrating Domain Knowledge into Music and Audio Representation Learning</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/gfazekas\">George Fazekas</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, CSC PhD Studentships, International PhD Funding Scheme</p>\n<p>Audio and music representation learning seeks to transform raw data into latent representations for downstream tasks such as classification, recommendation, retrieval and generation. While recent advances in deep learning, especially contrastive, self-supervised and diffusion-based approaches have achieved impressive results, most remain purely data-driven and neglect domain-specific musical structures like rhythm, melody, harmony, metrical hierarchy or genre-style traits.</p>\n<p>This PhD project will explore ways to embed theoretical and structural knowledge into modern representation learning pipelines to enhance interpretability, controllability and performance. For example, incorporating symbolic or other structured representations, inductive biases, well-known principles exploited in classic DSP algorithms, or ontological constraints, the research aims to bridge the gap between data-driven models and the structured understanding of music and audio.</p>\n<p>Potential directions include hybrid models that combine deep audio and symbolic embeddings, graph-based or relational learning of musical structure, and explainable methods for music analysis, production or generation. The project will also engage with principles of Ethical and Responsible AI: reducing data bias, improving transparency and supporting fair attribution of authorship.</p>\n<p>Examples of relevant works include but not limited to:</p>\n<p>Guinot, Quinton, Fazekas: “Semi-Supervised Contrastive Learning of Musical Representations”, ISMIR-2024</p>\n<p>Yu, Fazekas: “Singing voice synthesis using differentiable LPC and glottal-flow-inspired wavetables”, ISMIR-2023</p>\n<p>Agarwal, Wang, Richard: F-StrIPE: Fast Structure-Informed Positional Encoding for Symbolic Music Generation, ICASSP-2025</p>\n<hr>\n<p><strong>Automated machine learning for music understanding</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/ebenetos/\">Emmanouil Benetos</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, CSC PhD Studentships, International PhD Funding Scheme</p>\n<p>The field of music information retrieval (MIR) has been growing for more than 20 years, with re-cent advances in deep learning having revolutionised the way machines can make sense of music data. At the same time, research in the field is still constrained by laborious tasks involving data preparation, feature extraction, model selection, architecture optimisation, hyperparameter optimisa-tion, and transfer learning, to name but a few. Some of the model and experimental design choices made by MIR researchers also reflect their own biases.</p>\n<p>Inspired by recent developments in machine learning and automation, this PhD project will investi-gate and develop automated machine learning methods which can be applied at any stage in the MIR pipeline as to build music understanding models ready for deployment across a wide range of tasks. This project will also compare the automated decisions made on every step in the MIR pipe-line, as compared with manual model design choices made by researchers. The successful candidate will investigate, propose and develop novel deep learning methods for automating music under-standing, resulting in models that can accelerate MIR research and contribute to the democratisation of AI.</p>\n<hr>\n<p><strong>Sonification techniques for understanding hidden processes of LLMs</strong></p>\n<p>Supervisors: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/axambosedo/\">Anna Xambó</a> and <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/csaitis/\">Charalampos Saitis</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, CSC PhD Studentships, International PhD Funding Scheme</p>\n<p>Large language models (LLMs) are a type of artificial intelligence program that can recognise and generate text, which are trained on huge sets of data with a complex network of hidden processes. This PhD topic explores sonification techniques of LLMs for a better understanding of the way they process the information. Can we treat LLM engines such as ChatGPT as a musical instrument and listen to its internal processes? Can sonification techniques help us to hear and see how the information is processed? Compared to vinyl records or tape recordings, what is the acoustic signature, and what are the artefacts that are distinctive of this new medium? This work will contribute to addressing an important challenge in AI: making the inner workings and hidden knowledge of models more interpretable for people.</p>\n<p>Keywords: sonification, large language models (LLMs), explainable AI</p>\n<hr>\n<p><strong>Audio-visual sensing for machine intelligence</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/lwang/\">Lin Wang</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, International PhD Funding Scheme</p>\n<p>The project aims to develop novel audio-visual signal processing and machine learning algorithms that help improve machine intelligence and autonomy in an unknown environment, and to understand human behaviours interacting with robots. The project will investigate the application of AI algorithms for audio-visual scene analysis in real-life environments. One example is to employ multimodal sensors e.g. microphones and cameras, for analysing various sources and events present in the acoustic environment. Tasks to be considered include audio-visual source separation, localization/tracking, audio-visual event detection/recognition, audio-visual scene understanding.</p>\n<hr>\n<p><strong>Interpretable AI for Sound Event Detection and Classification</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/lwang/\">Lin Wang</a> and <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/ebenetos/\">Emmanouil Benetos</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, CSC PhD Studentships, International PhD Funding Scheme</p>\n<p>Deep-learning models have revolutionized state-of-the-art technologies for environmental sound recognition motivated by their applications in healthcare, smart homes, or urban planning. However, most of the systems used for these applications are based on black boxes and, therefore, cannot be inspected, so the rationale behind their decisions is obscure. Despite recent advances, there is still a lack of research in interpretable machine learning in the audio domain. Applicants are invited to develop ideas to reduce this gap by proposing interpretable deep-learning models for automatic sound event detection and classification in real-life environments.</p>\n<hr>\n<p><strong>Using machine learning to enhance simulation of sound phenomena</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/jreiss/\">Josh Reiss</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, International PhD Funding Scheme</p>\n<p>Physical models of sound generating phenomena are widely used in digital musical instruments, noise and vibration modelling, and sound effects. They can be incredibly high quality, but they also often have a large number of free parameters that may not be specified just from an understanding of the phenomenon.</p>\n<p>Machine learning from sample libraries could be the key to improving the physical models and speeding up the design process. Not only can optimisation approaches be used to select parameter values such that the output of the model matches samples, the accuracy of such an approach will give us insight into the limitations of a model. It also provides the opportunity to explore the overall performance of different physical modelling approaches, and to find out whether a model can be generalised to cover a large number of sounds, with a relatively small number of exposed parameters.</p>\n<p>This work will explore such approaches. It will build on recent high impact research from the team in relation to optimisation of sound effect synthesis models. Existing physical models will be used, with parameter optimisation based on gradient descent. Performance will be compared against recent neural synthesis approaches, that often provide high quality synthesis but lack a physical basis. It will also seek to measure the extent to which entire sample libraries could be replaced by a small number of physical models with parameters set to match the samples in the library.</p>\n<p>The student will have the opportunity to work closely with research engineers from the start-up company Nemisindo, though will also have the freedom to take the work in promising new directions. Publishing research in premier venues will be encouraged.</p>\n<p>The project can be tailored to the skills of the researcher, and has the potential for high impact.</p>\n<hr>\n<p><strong>Intelligent audio production for the hearing impaired</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/jreiss/\">Josh Reiss</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, International PhD Funding Scheme</p>\n<p>This project will explore new approaches to audio production to address hearing loss, a growing concern with an aging population. The overall goal is to investigate, implement and validate original strategies for mixing audio content such that it can be delivered with improved perceptual quality for hearing impaired people.</p>\n<p>Music content is typically recorded as multitracks, with different sound sources on different tracks. Similarly, soundtracks for television and radio content typically have dialogue, sound effects and music mixed together with normal-hearing listeners in mind. But a hearing impairment may result in this final mix sounding muddy and cluttered. The research team here have made strong advances on simulating hearing loss, understanding how to mix for hearing loss, and attempting to automatically deliver enhanced mixes for hearing loss. But these initial steps identified many unresolved issues and challenges. Why do hearing loss simulators differ from real world hearing loss, and how can this be corrected? How should hearing loss simulators be evaluated and how should they be used in the music production process? What is the best approach to mix audio content to address hearing loss? These questions will be investigated in this project.</p>\n<p>The project can be tailored to the skills of the researcher, and has the potential for high impact.</p>\n<hr>\n<p><strong>Neural Dynamics of Perceptually Aligned Artificial Intelligence</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/iroman/\">Iran Roman</a></p>\n<p>Eligible funding schemes: Fully-funded UK Home student fees and London stipend</p>\n<p>The brain easily makes sense of complex perceptual tasks, while sophisticated AI systems still struggle. This PhD project aims to bridge computational neuroscience, machine learning, and multimodal perception to build AI that perceives the world more like living organisms. Current AI often relies on statistical shortcuts, not genuine understanding. This project will draw on Neural Resonance Theory to replicate perceptual alignment, where biological networks resonate and synchronize to embody perceptual structure. The project will investigate how principles of oscillation, resonance, and attunement can be embedded in neural networks. The successful candidate will develop new theories and algorithms. Potential applications include multimodal self-supervised learning, neurodynamical models, and embodied, interactive AI systems that can understand and anticipate actions in real-time.</p>\n<hr>\n<p><strong>Deep neural modelling of music and speech perception</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/chcc/people/mpearce\">Marcus Pearce</a> and <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/iroman/\">Iran Roman</a></p>\n<p>Eligible funding schemes: CSC PhD Studentships</p>\n<p>Evidence suggests speech and music perception depend on cognitive models acquired through implicit statistical learning. While deep neural networks (DNNs) use analogous mechanisms for generating music and language, it is unknown if they truly simulate human perception. This project will develop novel neural network architectures to simulate speech and music perception. The project will use existing probabilistic methods both as a benchmark and as a tool for interpreting the abstract representations learned by the DNNs. Models will be tested through iterative comparison with behavioural and neural data from human psychological experiments. The successful candidate will also investigate cross-cultural comparisons and the psychological relationships between speech and music. The project's outcome will be a computational understanding of the psychology of human cultural learning in auditory perception.</p>\n<hr>\n<p><strong>Neural Resonance and the Perception of Timbre: A Neurodynamic Modeling Approach</strong></p>\n<p>Supervisor: <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/csaitis/\">Charalampos Saitis</a> and <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/iroman/\">Iran Roman</a></p>\n<p>Eligible funding schemes: S&#x26;E Studentships for Underrepresented Groups, International PhD Funding Schemes</p>\n<p>The perception of timbre is central to auditory recognition, yet its neurodynamic basis remains underexplored compared to pitch or rhythm. Neural Resonance Theory (NRT) posits that musical experience arises from brain-body dynamics entraining to structured sound, resulting in stable, pattern-forming oscillatory activity. Recent models of cochlear and brainstem activity using nonlinear resonator networks provide biologically grounded support for this view. This project investigates how networks of nonlinear oscillators and recurrent neural networks (RNNs) respond to stimuli varying only in timbre (e.g., sinusoids, violin, voice), aiming to identify differential resonance patterns attributable to spectral characteristics alone. In parallel, RNNs trained on pitch tasks will be analyzed to determine whether their emergent internal dynamics replicate resonance phenomena. Finally, EEG recordings from human listeners will be used to detect entrainment signatures matching model predictions. This interdisciplinary approach offers a novel application of NRT to timbre perception, bridging biologically inspired modeling, machine learning, and empirical neuroscience.</p>","id":"f318f0d9-d5ff-5115-93cf-a18a2e6dd83f"},{"fields":{"slug":"/news/2025-11-05.BMVA-Workshop"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#a8a898","images":{"fallback":{"src":"/static/f50b3f2ab8deecae63e8cdd69d401e0b/2b34e/BMVA2025.png","srcSet":"/static/f50b3f2ab8deecae63e8cdd69d401e0b/bcbe6/BMVA2025.png 245w,\n/static/f50b3f2ab8deecae63e8cdd69d401e0b/78a05/BMVA2025.png 490w,\n/static/f50b3f2ab8deecae63e8cdd69d401e0b/2b34e/BMVA2025.png 979w","sizes":"(min-width: 979px) 979px, 100vw"},"sources":[{"srcSet":"/static/f50b3f2ab8deecae63e8cdd69d401e0b/c4230/BMVA2025.webp 245w,\n/static/f50b3f2ab8deecae63e8cdd69d401e0b/a5fb7/BMVA2025.webp 490w,\n/static/f50b3f2ab8deecae63e8cdd69d401e0b/758a3/BMVA2025.webp 979w","type":"image/webp","sizes":"(min-width: 979px) 979px, 100vw"}]},"width":979,"height":979}}},"title":"The British Machine Vision Association Workshop on Multimodal LLMs","author":"admin","date":"Wed 05 Nov 2025"},"html":"<p>The <a href=\"https://www.bmva.org/meetings/25-11-05-MultimodalLargeModels.html\">British Machine Vision Association and Society for Pattern Recognition (BMVA) Workshop on Multimodal Large Models Bridging Vision, Language, and Beyond</a> was held at British Computer Society (BCS), 25 Copthall Avenue, London EC2R 7BP on November 5th, 2025.</p>\n<p>Among the selected oral presentations, C4DM PhD student <strong>Yinghao Ma</strong> (supervised by Prof. Emmanouil Benetos) presented his latest work <em>MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix</em>. MMAR is a newly released benchmark that spans 1,000 real-world audio reasoning tasks, covering speech, sound events, music, and mixed-modality scenarios. It is one of the first benchmarks to explicitly evaluate multi-step reasoning abilities in audio-language and omni-modal large models, with tasks ranging from low-level signal perception to high-level cultural understanding.</p>\n<p>The talk was part of the “Domain Applications and Human-Centric Modalities” session, alongside research on sign language translation, visual illusions, and 3D-aware facial editing. MMAR attracted interest from both academia and industry attendees, especially as multimodal reasoning becomes a key focus in the next wave of AI foundation models.</p>\n<p>The BMVA workshop featured keynote speakers from Google DeepMind, UCL, and the University of Surrey, and brought together researchers advancing the frontier of multimodal intelligence across vision, language, audio, and embodied learning.</p>\n<p>MMAR is open-source and available on arXiv: <strong>arXiv:2505.13032</strong>. The video recording of presentation is available at <a href=\"https://www.bmva.org/meetings/25-11-05-MultimodalLargeModels.html\">here</a></p>","id":"8f6388ab-107d-5585-8923-6db01cfbded3"},{"fields":{"slug":"/news/2025-11-05.C4DM-Seminar_Masataka_Goto"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Masataka Goto","author":null,"date":"Wed 05 Nov 2025"},"html":"<h3>C4DM Seminar: Masataka Goto: Advancing Music Experience Through Music Information Research</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Masataka Goto</p>\n<p><strong>Date/time:</strong>  Wednesday, 5th Nov 2025, 3.00pm</p>\n<p><strong>Location:</strong> In-person only. Mile Eng campus, Eng building, room G2</p>\n<h2><b>Title</b>: Advancing Music Experience Through Music Information Research</h2>\n<p><b>Abstract</b>:\nMusic technologies will open up new ways of enjoying music, both in terms of creating and appreciating music. In this talk, I will discuss how music information research can enrich music experiences by introducing examples of our research outcomes. For example, \"Lyric Apps\" offer a new form of lyric-driven visual art, dynamically rendering different visual content based on user interaction. After releasing \"TextAlive App API\", a web-based framework for creating lyric apps, which has received several awards including at ACM CHI 2023, we have held annual programming contests since 2020. Another example is \"Kiite Cafe\", a web service that allows users to get together virtually to listen to music. It lets users enjoy the same song simultaneously while reacting in real time, creating a shared listening experience, and has also received multiple awards. In the future, further advances in music information research will make interactions between people and music more active and enriching.</p>\n<p><b>Bio</b>:\nMasataka Goto received the Doctor of Engineering degree from Waseda University in 1998. He is currently the Senior Principal Researcher at the National Institute of Advanced Industrial Science and Technology (AIST), Japan. Over the past 33 years, he has published more than 350 papers in refereed journals and international conference proceedings and has received 76 awards, including several best paper awards, best presentation awards, the Tenth Japan Academy Medal, and the Tenth JSPS PRIZE. He has served as a committee member of over 140 scientific societies and conferences, including as the General Chair of ISMIR 2009 and 2014 and the Program Chair of ISMIR 2022.</p>","id":"ac072711-e924-5b7a-a17a-02e79c202de8"},{"fields":{"slug":"/news/2025-10-23.C4DM-student-awarded-Google-PhD-fellowship"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4ac60065bd12d76e2e9830ee1a3f805a/a89ca/yinghao.jpg","srcSet":"/static/4ac60065bd12d76e2e9830ee1a3f805a/96deb/yinghao.jpg 150w,\n/static/4ac60065bd12d76e2e9830ee1a3f805a/0fdf4/yinghao.jpg 300w,\n/static/4ac60065bd12d76e2e9830ee1a3f805a/a89ca/yinghao.jpg 600w","sizes":"(min-width: 600px) 600px, 100vw"},"sources":[{"srcSet":"/static/4ac60065bd12d76e2e9830ee1a3f805a/c65bc/yinghao.webp 150w,\n/static/4ac60065bd12d76e2e9830ee1a3f805a/078c3/yinghao.webp 300w,\n/static/4ac60065bd12d76e2e9830ee1a3f805a/6d09e/yinghao.webp 600w","type":"image/webp","sizes":"(min-width: 600px) 600px, 100vw"}]},"width":600,"height":600}}},"title":"C4DM PhD student awarded Google PhD Fellowship","author":"Admin","date":"Thu 23 Oct 2025"},"html":"<p>We are extremely proud to announce that <a href=\"https://nicolaus625.github.io/\">Yinghao Ma</a>, a PhD student in AI and Music at the Centre for Digital Music at QMUL, has been awarded the <a href=\"https://research.google/programs-and-events/phd-fellowship/recipients/?filtertab=2025\">2025 Google Fellowship in Machine Perception</a>.</p>\n<p>A Google spokesperson said: “The student nominations we received this year were exemplary in their quality, but Yinghao especially stood out and was endorsed by the research scientists and distinguished engineers within Google who participated in the review. Congratulations to Yinghao on this well-deserved recognition, it’s an honor to support such incredibly talented students.”</p>\n<p>Yinghao's PhD research focuses on advancing Large Language Models (LLMs) for music understanding and generation. Specifically, he studies how multimodal models can integrate audio, symbolic, and textual information to understand, reason about, and generate music.</p>\n<p>Together with colleagues, he developed <a href=\"https://arxiv.org/abs/2306.00107\">MERT</a>, a large-scale music audio representation model which has more than 10k monthly download in the past three years. His recent work includes developing music instruction-following datasets and benchmarks that help evaluate how well AI systems can comprehend and create music.</p>\n<p>He said: \"It's my great honour to receive the Google PhD Fellowship that recognises my research and strongly contribute to my future career. I’m deeply grateful to Google and QMUL for the support, providing good platforms for AI &#x26; music research.\"</p>\n<p>Congratulations Yinghao!</p>","id":"1ec9aa73-6b0c-5967-b4d1-2b1e075f8ebe"}]}}}
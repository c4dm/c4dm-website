{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/YIFgS8she58","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/Sandler-Artificial-Neuroscience"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png","srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/2fd20/EPSRC.png 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/de391/EPSRC.png 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/d66e1/EPSRC.webp 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/e7160/EPSRC.webp 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/5f169/EPSRC.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Artificial Neuroscience: metrology and engineering for Deep Learning using Linear Algebra","author":"Prof Mark Sandler (PI), Boris Khoruzhenko (CI)","date":null,"link":"https://www.ukri.org/councils/epsrc/"},"html":"","id":"c1314000-fe3b-5175-88a2-f239d861ddad"},{"fields":{"slug":"/projects/Barthet-Netz"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Netz: A Novel XR Musical Instrument","author":"Dr Mathieu Barthet (PI)","date":null,"link":"https://iuk-business-connect.org.uk/programme/icure/"},"html":"","id":"0040b455-65cf-5ea1-a7ce-38ab70570dae"},{"fields":{"slug":"/projects/Benetos-Algorivm"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Project Maestro - AI Musical Analysis Platform","author":"Dr Emmanouil Benetos (PI), Prof Simon Dixon (CI)","date":null,"link":"https://www.ukri.org/councils/innovate-uk/"},"html":"","id":"78c348d8-d78f-5acf-8441-382bc6c4d791"},{"fields":{"slug":"/projects/Benetos-L-Acoustics"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Online speech enhancement in scenarios with low direct-to-reverberant-ratio","author":"Dr Emmanouil Benetos (PI), Dr Aidan Hogg (CI)","date":null,"link":"https://www.l-acoustics.com/"},"html":"","id":"ce65fc6c-7228-5805-a9b2-6e14e5fd5086"},{"fields":{"slug":"/projects/Benetos-Moises"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Enhancing lyrics transcription with open-source architectures and fine-tuning techniques","author":"Dr Emmanouil Benetos (PI)","date":null,"link":"https://moises.ai/"},"html":"","id":"d61977ff-9345-55a4-b85e-44caffa4a7f0"},{"fields":{"slug":"/projects/Dixon-assessment"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Music Performance Assessment and Feedback","author":"Prof Simon Dixon (PI), Dr Emmanouil Benetos (CI)","date":null,"link":null},"html":"","id":"68ee56ea-59b7-58de-a8c9-805c4aba4357"}]},"news":{"nodes":[{"fields":{"slug":"/news/2025-08-09.C4DM-at_ISMIR_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/1f63c29bf8c9e3effafc2164312893eb/a5b34/ismir2025logo.png","srcSet":"/static/1f63c29bf8c9e3effafc2164312893eb/13677/ismir2025logo.png 1000w,\n/static/1f63c29bf8c9e3effafc2164312893eb/f21c0/ismir2025logo.png 2000w,\n/static/1f63c29bf8c9e3effafc2164312893eb/a5b34/ismir2025logo.png 4000w","sizes":"(min-width: 4000px) 4000px, 100vw"},"sources":[{"srcSet":"/static/1f63c29bf8c9e3effafc2164312893eb/3cd29/ismir2025logo.webp 1000w,\n/static/1f63c29bf8c9e3effafc2164312893eb/62c39/ismir2025logo.webp 2000w,\n/static/1f63c29bf8c9e3effafc2164312893eb/a6ca8/ismir2025logo.webp 4000w","type":"image/webp","sizes":"(min-width: 4000px) 4000px, 100vw"}]},"width":4000,"height":4000}}},"title":"C4DM at ISMIR 2025","author":"Admin","date":"Mon 08 Sep 2025"},"html":"<p></p>\n<p>On 21-25 September 2025, several C4DM researchers will participate at the <b><a href=\"https://ismir2025.ismir.net/\">26th International Society for Music Information Retrieval Conference (ISMIR 2025)</a></b>. ISMIR is the leading conference in the field of music informatics, and is currently the <a href=\"https://scholar.google.com/citations?view_op=top_venues&#x26;hl=en&#x26;vq=hum_musicmusicology\">top-cited publication for Music &#x26; Musicology</a> (source: Google Scholar). This year ISMIR will take place onsite in Daejeon, Korea.</p>\n<p>Similar to previous years, the Centre for Digital Music will have a strong presence at ISMIR 2025.</p>\n<p><br>In the <b>Scientific Programme</b>, the following papers are authored/co-authored by C4DM members:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2506.07199\">Audio Synthesizer Inversion in Symmetric Parameter Spaces with Approximately Equivariant Flow Matching</a> (Ben Hayes, Charalampos Saitis, György Fazekas)</li>\n<li><a href=\"https://arxiv.org/abs/2506.17815\">SLAP: Siamese Language Audio Pretraining without Negative Samples for Music Understanding</a> (Julien Guinot, Alain Riou, Elio Quinton, György Fazekas)</li>\n<li><a href=\"https://arxiv.org/abs/2506.17886\">GD-Retriever: Controllable Generative Text Music Retrieval with Diffusion Models</a> (Julien Guinot, Elio Quinton, György Fazekas)</li>\n<li><a href=\"https://arxiv.org/abs/2405.18386\">Instruct-MusicGen: Unlocking Text to Music Editing for Music Language Models via Instruction Tuning</a> (Yixiao Zhang, Yukara Ikemiya, Woosung Choi, Naoki Murata, Marco A. Martínez Ramírez, Liwei Lin, Gus Xia, Wei Hsiang Liao, Yuki Mitsufuji, Simon Dixon)</li>\n<li><a href=\"https://arxiv.org/abs/2506.23869\">Scaling Self Supervised Representation Learning for Symbolic Piano Performance</a> (Louis Bradshaw, Honglu Fan, Alexander Spangher, Stella Biderman, Simon Colton)</li>\n<li>Codicodec: Unifying Continuous and Discrete Compressed Representations of Audio (Marco Pasini, Stefan Lattner, György Fazekas)</li>\n<li><a href=\"https://arxiv.org/abs/2507.08530\">MIDI-VALLE: Improving Expressive Piano Performance Synthesis through Neural Codec Language Modelling</a> (Jingjing Tang, Xin Wang, Zhe Zhang, Junichi Yamagishi, Geraint Wiggins, György Fazekas)</li>\n<li><a href=\"https://arxiv.org/abs/2506.17055\">Universal Music Representations? Evaluating Foundation Models on World Music Corpora</a> (Charilaos Papaioannou, Emmanouil Benetos, Alexandros Potamianos)</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/107959\">Perceptual Errors in Music Source Separation: Looking Beyond SDR Averages</a> (Saurjya Sarkar, Victoria Moomjian, Basil Woods, Emmanouil Benetos, Mark Sandler)</li>\n<li><a href=\"https://github.com/JackJamesLoth/GOAT-Dataset/blob/main/GOAT_paper.pdf\">GOAT: a Large Dataset of Paired Guitar Audio Recordings and Tablatures</a> (Jackson Loth, Pedro Sarmento, Saurjya Sarkar, Zixun Guo, Mathieu Barthet, Mark Sandler)</li>\n<li><a href=\"https://arxiv.org/abs/2506.12285\">CMI-Bench: a Comprehensive Benchmark for Evaluating Music Instruction Following</a> (Yinghao Ma, Siyou Li, Juntao Yu, Emmanouil Benetos, Akira Maezawa)</li>\n<li><a href=\"https://arxiv.org/abs/2507.07764\">Assessing the Alignment of Audio Representations with Timbre Similarity Ratings</a> (Haokun Tian, Stefan Lattner, Charalampos Saitis)</li>\n<li><a href=\"https://arxiv.org/abs/2507.11233\">Improving Neural Pitch Estimation with SWIPE Kernels</a> (David Marttila, Joshua D. Reiss)</li>\n<li><a href=\"https://www.arxiv.org/abs/2506.14684\">Refining Music Sample Identification with a Self Supervised Graph Neural Network</a> (Aditya Bhattacharjee, Ivan Meresman Higgs, Mark Sandler, Emmanouil Benetos)</li>\n</ul>\n<p><br>The following <b><a href=\"https://ismir2025.ismir.net/program-tutorials\">Tutorials</a></b> will be co-presented by C4DM PhD students Rodrigo Diaz and Julien Guinot:</p>\n<ul>\n<li>Differentiable Physical Modeling Sound Synthesis: Theory, Musical Application, and Programming (Jin Woo Lee, Stefan Bilbao, Rodrigo Diaz)</li>\n<li>Self-supervised Learning for Music - An Overview and New Horizons (Julien Guinot, Alain Riou, Yuexuan Kong, Marco Pasini, Gabriel Meseguer-Brocal, Stefan Lattner)</li>\n</ul>\n<p><br>The following journal papers published at <b>TISMIR</b> which are co-authored by C4DM members will be presented at the conference:</p>\n<ul>\n<li><a href=\"https://transactions.ismir.net/articles/10.5334/tismir.214\">Predicting Eurovision Song Contest Results: A Hit Song Science Approach</a> (Katarzyna Adamska, Joshua Reiss)</li>\n<li><a href=\"https://transactions.ismir.net/articles/10.5334/tismir.203\">The GigaMIDI Dataset with Features for Expressive Music Performance Detection</a> (Keon Ju Lee, Jeff Ens, Sara Adkins, Pedro Sarmento, Mathieu Barthet, Philippe Pasquier)</li>\n</ul>\n<p><br>As part of the <b>MIREX public evaluations</b>:</p>\n<ul>\n<li>C4DM PhD student Yinghao Ma is task captain for the <a href=\"https://www.music-ir.org/mirex/wiki/2025:Music_Reasoning_QA\">Music Reasoning QA</a>, <a href=\"https://www.music-ir.org/mirex/wiki/2025:Audio_Beat_Tracking\">Audio Beat Tracking</a>, and <a href=\"https://www.music-ir.org/mirex/wiki/2025:Audio_Key_Detection\">Audio Key Detection</a> tasks</li>\n<li>C4DM PhD student Huan Zhang is task captain for <a href=\"https://www.music-ir.org/mirex/wiki/2025:RenCon\">RenCon 2025: Expressive Performance Rendering Competitionk</a></li>\n</ul>\n<p>Finally, on the organisational side:</p>\n<ul>\n<li>C4DM PhD student Chin-Yun Yu is Virtual Co-Chair for the ISMIR 2025 conference.</li>\n<li>C4DM PhD student Yinghao Ma is co-organising the satellite workshop <a href=\"https://m-a-p.ai/LLM4Music/\">LLM4MA: Large Language Models for Music &#x26; Audio</a></li>\n</ul>\n<p><br>See you at Daejeon!</p>","id":"0b1307c1-c5c7-5b93-86f5-afca1e60b144"},{"fields":{"slug":"/news/2025-09-08.C4DM-team_wins_QbVI_challenge"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/ac1cc7c4c269a84076b622a95d18bb09/6e2bb/QVIM.jpg","srcSet":"/static/ac1cc7c4c269a84076b622a95d18bb09/2ca18/QVIM.jpg 342w,\n/static/ac1cc7c4c269a84076b622a95d18bb09/5309a/QVIM.jpg 683w,\n/static/ac1cc7c4c269a84076b622a95d18bb09/6e2bb/QVIM.jpg 1366w","sizes":"(min-width: 1366px) 1366px, 100vw"},"sources":[{"srcSet":"/static/ac1cc7c4c269a84076b622a95d18bb09/7ab00/QVIM.webp 342w,\n/static/ac1cc7c4c269a84076b622a95d18bb09/3e123/QVIM.webp 683w,\n/static/ac1cc7c4c269a84076b622a95d18bb09/f0a0a/QVIM.webp 1366w","type":"image/webp","sizes":"(min-width: 1366px) 1366px, 100vw"}]},"width":1366,"height":1366}}},"title":"C4DM team wins Query-by-Vocal Imitation Challenge","author":"Admin","date":"Mon 08 Sep 2025"},"html":"<p>Congratulations to C4DM members <a href=\"https://www.qmul.ac.uk/eecs/people/profiles/adityabhattacharjee-.html\">Aditya Bhattacharjee</a>, <a href=\"https://chrispla.me/\">Christos Plachouras</a> and <a href=\"https://www.qmul.ac.uk/eecs/people/profiles/sungkyun-chang.html\">Sungkyun Chang</a> who secured first place at the <a href=\"https://qvim-aes.github.io/\">Query-by-Vocal Imitation (QbVI) Challenge</a>, held as part of the <a href=\"https://aes2.org/events-calendar/2025-aes-international-conference-on-artificial-intelligence-and-machine-learning-for-audio/\">AES International Conference on Artificial Intelligence and Machine Learning for Audio conference (AES AIMLA 2025)</a> taking place from September 8-10, 2025.</p>\n<p>The winning entry addressed the task, which entails retrieving relevant audio clips from a database using only a vocal imitation as a query. This is a particularly complex problem due to the variability in how people vocalise sounds and the acoustic diversity across sound categories. Successful approaches must bridge the gap between vocal and non-vocal audio, while handling the unpredictability of human-generated imitations.</p>\n<p>The team’s submission, titled “Effective Finetuning Methods for Query-by Vocal Imitation”, advances the state-of-the-art in QbVI by integrating a triplet-based regularisation objective with supervised contrastive learning. This method addresses the issue of limited data by sampling from an unused subset of the VocalSketch dataset, which comprises practice recordings and human-rejected vocal imitations. While this data may not be suitable for positive matches, the vocal imitation data is useful for creating confounding examples during training. More specifically, this increases the size of the pool of negative examples, which is utilised by the added regularisation method.</p>\n<p>The proposed method surpassed state-of-the-art methods for both subjective and objective evaluation metrics, opening up scope for product-based innovations and software tools that can be used by artists to effectively search large repositories of sound effects.</p>","id":"076d131a-78aa-5cf2-b090-ecfc67318e1e"},{"fields":{"slug":"/news/2025-09-05.C4DM-AAAI_26_EAIM_Workshop"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8b8b8","images":{"fallback":{"src":"/static/1c5a33d5a82a2c279722b06017b80f38/c8f14/EAIM.png","srcSet":"/static/1c5a33d5a82a2c279722b06017b80f38/abab1/EAIM.png 185w,\n/static/1c5a33d5a82a2c279722b06017b80f38/96aa8/EAIM.png 370w,\n/static/1c5a33d5a82a2c279722b06017b80f38/c8f14/EAIM.png 739w","sizes":"(min-width: 739px) 739px, 100vw"},"sources":[{"srcSet":"/static/1c5a33d5a82a2c279722b06017b80f38/c5b6a/EAIM.webp 185w,\n/static/1c5a33d5a82a2c279722b06017b80f38/7fd3d/EAIM.webp 370w,\n/static/1c5a33d5a82a2c279722b06017b80f38/5bbdd/EAIM.webp 739w","type":"image/webp","sizes":"(min-width: 739px) 739px, 100vw"}]},"width":739,"height":739}}},"title":"C4DM will co-host AI for Music workshop at AAAI-26 in Singapore","author":"Keshav Bhandari","date":"Fri 05 Sep 2025"},"html":"<p>C4DM is co-hosting a full-day AI for Music workshop titled \"1st International Workshop on Emerging AI Technologies for Music (EAIM)\" at the AAAI-26 conference in Singapore, from January 20-27, 2026. The workshop will explore how AI can be designed for controllability and human-centered collaboration, moving beyond automation toward systems that empower people in creative processes.</p>\n<p>The accepted papers will be published as a volume in the Proceedings of Machine Learning Research (PMLR). The paper submission deadline is October 24, 2025.</p>\n<p>Further information on topics, submission requirements, keynote speakers, and more is avaliable <a href=\"https://amaai-lab.github.io/EAIM2026/\">here</a></p>","id":"c7c159df-6dac-533f-b6cd-e8eaca8638f7"},{"fields":{"slug":"/news/2025-09-03.C4DM-at_UKAIRS"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/0a520cc6b532ebe8e12d32e1128fe298/1ff6a/ukairs_2025.png","srcSet":"/static/0a520cc6b532ebe8e12d32e1128fe298/a5ae6/ukairs_2025.png 334w,\n/static/0a520cc6b532ebe8e12d32e1128fe298/40a65/ukairs_2025.png 668w,\n/static/0a520cc6b532ebe8e12d32e1128fe298/1ff6a/ukairs_2025.png 1336w","sizes":"(min-width: 1336px) 1336px, 100vw"},"sources":[{"srcSet":"/static/0a520cc6b532ebe8e12d32e1128fe298/c6d30/ukairs_2025.webp 334w,\n/static/0a520cc6b532ebe8e12d32e1128fe298/27806/ukairs_2025.webp 668w,\n/static/0a520cc6b532ebe8e12d32e1128fe298/93676/ukairs_2025.webp 1336w","type":"image/webp","sizes":"(min-width: 1336px) 1336px, 100vw"}]},"width":1336,"height":1336}}},"title":"C4DM at UKAIRS 2025","author":"Christos Plachouras","date":"Wed 03 Sep 2025"},"html":"<p>On 8-10 September, C4DM members will participate in the <a href=\"https://www.ukairs.ac.uk/\">UK AI Research Symposium (UKAIRS)</a> in Northumbria University, Newcastle. UKAIRS will bring together researchers from different disciplines, aiming to inform UK AI research priorities, foster interdisciplinary collaboration, and shape discourse on delivering AI for societal and economic benefit.</p>\n<p>The following works C4DM members have been involved in will be presented:</p>\n<ul>\n<li><a href=\"https://hal.science/hal-04943901v1/document\">Towards Music Industry 5.0: Perspectives on Artificial Intelligence</a>, Alexander Williams, Mathieu Barthet</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/103081/Xambo%20Sedo%20Human-machine%20agencies%20in%20live%20coding%20for%20music%20performance%202024%20Accepted.pdf?sequence=2&#x26;isAllowed=y\">Human–machine agencies in live coding for music performance</a>, Anna Xambó, Gerard Roma</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/106803\">Towards a Unified Representation Evaluation Framework Beyond Downstream Tasks</a>, Christos Plachouras, Julien Guinot, George Fazekas, Elio Quinton, Emmanouil Benetos, Johan Pauwels</li>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/110219/Xambo%20Soundscape-based%20music%202025%20Accepted.pdf?sequence=2\">Soundscape-based music and creative AI: Insights and promises</a>, Anna Xambó, Peter Batchelor, Luigi Marino, Gerard Roma, Mike Bell, George Xenakis</li>\n<li>Split Fine-Tuning of BERT-based Music Models in the Edge-Cloud Continuum: An Empirical Analysis, Bradley Aldous, Ahmed M. A. Sayed</li>\n<li>Composing Kernel Models with Self-Attention, Carey Bunks, Simon Dixon, Bruno Di Giorgi</li>\n</ul>\n<p>Additionally, on 10 September, during the UKAIRS Early Career Researcher Day, C4DM PhD student Elona Shatri is organising the workshop <a href=\"https://www.ukairs.ac.uk/ecr-day/\">\"Should Ethics Committees Expand Their Mandates to Review AI Research?\"</a>.</p>\n<p>See you at UKAIRS!</p>","id":"0b82bc45-d8be-59c4-8e5c-cfef869e665a"},{"fields":{"slug":"/news/2025-09-03.C4DM-organises_AIMLA_2025_conference"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#083858","images":{"fallback":{"src":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/18c0c/AIMLA-logo.jpg","srcSet":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/0fdf4/AIMLA-logo.jpg 300w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/7706b/AIMLA-logo.jpg 601w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/18c0c/AIMLA-logo.jpg 1201w","sizes":"(min-width: 1201px) 1201px, 100vw"},"sources":[{"srcSet":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/078c3/AIMLA-logo.webp 300w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/2b014/AIMLA-logo.webp 601w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/1fcf1/AIMLA-logo.webp 1201w","type":"image/webp","sizes":"(min-width: 1201px) 1201px, 100vw"}]},"width":1201,"height":1201}}},"title":"C4DM organises AES AIMLA 2025 conference","author":"Admin","date":"Wed 03 Sep 2025"},"html":"<p>The <a href=\"https://aes2.org/contributions/2025-1st-aes-international-conference-on-artificial-intelligence-and-machine-learning-for-audio-call-for-contributions/\">AES International Conference on Artificial Intelligence and Machine Learning for Audio (AIMLA 2025)</a> will be hosted by the <a href=\"https://www.c4dm.eecs.qmul.ac.uk/\">Centre for Digital Music</a> of Queen Mary University of London and is taking place on Sept. 8-10, 2025.</p>\n<p>Several C4DM members are involved in the organisation of the conference, including but not limited to:</p>\n<ul>\n<li>Josh Reiss (General Chair)</li>\n<li>George Fazekas (Papers Co-chair)</li>\n<li>Soumya Vanka (Special Sessions Co-Chair)</li>\n<li>Franco Caspe (Special Sessions Co-Chair)</li>\n<li>Farida Yusuf (Sponsorship Chair)</li>\n<li>Emmanouil Benetos (Publicity Chair)</li>\n<li>Nelly Garcia (Social Events Coordinator)</li>\n<li>Ilias Ibnyahya (Treasurer)</li>\n<li>Chin-Yun Yu (Late Breaking Papers Chair)</li>\n<li>Marikaiti Primenta (Invited Speakers Chair)</li>\n</ul>\n<p>Several papers and presentations will be made from C4DM members at AIMLA as well. The following peer-reviewed papers will be presented at the conference:</p>\n<ul>\n<li>\n<p><a href=\"https://aesaimla2025.sched.com/event/28Oay/nablafx-a-framework-for-differentiable-black-box-and-gray-box-modeling-of-audio-effects?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">NablAFx: A Framework for Differentiable Black-box and Gray-box Modeling of Audio Effects</a>, by Marco Comunità, Christian Steinmetz, Joshua Reiss</p>\n</li>\n<li>\n<p><a href=\"https://aesaimla2025.sched.com/event/28Ob1/transfer-learning-for-neural-modelling-of-nonlinear-distortion-effects?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">Transfer Learning for Neural Modelling of Nonlinear Distortion Effects</a>, by Tara Vanhatalo, Pierrick Legrand, Myriam Desainte-Catherine, Pierre Hanna, Guillaume Pille, Antoine Brusco, Joshua Reiss</p>\n</li>\n<li>\n<p><a href=\"https://aesaimla2025.sched.com/event/28Obz/sound-matching-an-analogue-levelling-amplifier-using-the-newton-raphson-method?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">Sound Matching an Analogue Levelling Amplifier Using the Newton-Raphson Method</a>, by Chin-Yun Yu, George Fazekas</p>\n</li>\n<li>\n<p><a href=\"https://aesaimla2025.sched.com/event/28OcW/procedural-music-generation-systems-in-games?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">Procedural Music Generation Systems in Games</a>, by Shangxuan Luo, Joshua Reiss</p>\n</li>\n<li>\n<p><a href=\"https://aesaimla2025.sched.com/event/28Obt/neutone-sdk-an-open-source-framework-for-neural-audio-processing?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">Neutone SDK: An Open Source Framework for Neural Audio Processing</a>, by Christopher Mitcheltree, Bogdan Teleaga, Andrew Fyfe, Naotake Masuda, Matthias Schäfer, Alfie Bradic, Nao Tokui</p>\n</li>\n</ul>\n<p>The following late-breaking posters from C4DM members will be presented at AIMLA:</p>\n<ul>\n<li>\n<p><a href=\"https://aesaimla2025.sched.com/event/28TJN/transformer-based-sustain-pedal-reconstruction-for-expressive-piano-performance-midi?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">Transformer-Based Sustain Pedal Reconstruction for Expressive Piano Performance MIDI</a>, by Wenhao Liu, George Fazekas, Jingjing Tang</p>\n</li>\n<li>\n<p><a href=\"https://aesaimla2025.sched.com/event/28TJ2/decoding-melodic-acoustic-features-from-neural-data?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">Decoding Melodic Acoustic Features from Neural Data</a>, by Zorka Bozilovic, Iran Roman</p>\n</li>\n<li>\n<p><a href=\"https://aesaimla2025.sched.com/event/28TJH/towards-intelligent-music-education-score-informed-transcription-and-performance-assessment?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">Towards Intelligent Music Education: Score-Informed Transcription and Performance Assessment</a>, by Jack Loth, Marikaiti Primenta, Jingjing Tang, Xavier Riley, Simon Dixon, Emmanouil Benetos</p>\n</li>\n<li>\n<p><a href=\"https://sched.co/28TJE\">MULTIVOX: Spatial &#x26; Multimodal Data of Singing Groups</a>, by Gerardo Meza Gómez, Iran Roman, Adrian Roman, Rodrigo Sigal Sefchovich, Mariana Sepúlveda</p>\n</li>\n</ul>\n<p>Last but not least, the following tutorial will be co-presented by C4DM PhD student Franco Caspe:</p>\n<ul>\n<li><a href=\"https://aesaimla2025.sched.com/event/28OaU/tutorial-real-time-neural-audio-inference?iframe=no&#x26;w=100%25&#x26;sidebar=yes&#x26;bg=no\">Real-Time Neural Audio Inference </a>, by Franco Caspe and Jatin Chowdhury</li>\n</ul>\n<p>See you in London!</p>","id":"8ca8145f-7bcc-55ae-b8a6-1a7b454f655e"},{"fields":{"slug":"/news/2025-07-29.C4DM-at_Interspeech_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#084898","images":{"fallback":{"src":"/static/fbc593813a2397e9f177cd960126bb83/869a6/Interspeech-2025.jpg","srcSet":"/static/fbc593813a2397e9f177cd960126bb83/97a19/Interspeech-2025.jpg 480w,\n/static/fbc593813a2397e9f177cd960126bb83/d4aca/Interspeech-2025.jpg 960w,\n/static/fbc593813a2397e9f177cd960126bb83/869a6/Interspeech-2025.jpg 1920w","sizes":"(min-width: 1920px) 1920px, 100vw"},"sources":[{"srcSet":"/static/fbc593813a2397e9f177cd960126bb83/21b1a/Interspeech-2025.webp 480w,\n/static/fbc593813a2397e9f177cd960126bb83/d6f60/Interspeech-2025.webp 960w,\n/static/fbc593813a2397e9f177cd960126bb83/26222/Interspeech-2025.webp 1920w","type":"image/webp","sizes":"(min-width: 1920px) 1920px, 100vw"}]},"width":1920,"height":1920}}},"title":"C4DM at Interspeech 2025","author":"admin","date":"Tue 29 Jul 2025"},"html":"<p>On 17-21 August, C4DM members will participate in <a href=\"https://www.interspeech2025.org/home\">Interspeech 2025</a>. Interspeech is the premier international conference for research on the science and technology of spoken language processing.</p>\n<p>The following papers authored/co-authored by C4DM members will be presented at Interspeech 2025:</p>\n<ul>\n<li>\n<p><a href=\"https://arxiv.org/abs/2505.23509\">Spectrotemporal Modulation: Efficient and Interpretable Feature Representation for Classifying Speech, Music, and Environmental Sounds</a> by Andrew Chang, Yike Li, Iran R. Roman, David Poeppel</p>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/abs/2506.02339\">Enhancing Lyrics Transcription on Music Mixtures with Consistency Loss</a> by Jiawen Huang, Felipe Sousa, Emir Demirel, Emmanouil Benetos, Igor Gadelha</p>\n</li>\n</ul>\n<p>See you at Interspeech!</p>","id":"6bc651c0-77cf-5031-a443-082420242949"}]}}}
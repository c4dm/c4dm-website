{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/YIFgS8she58","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/Benetos-L-Acoustics"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Online speech enhancement in scenarios with low direct-to-reverberant-ratio","author":"Dr Emmanouil Benetos (PI), Dr Aidan Hogg (CI)","date":null,"link":"https://www.l-acoustics.com/"},"html":"","id":"ce65fc6c-7228-5805-a9b2-6e14e5fd5086"},{"fields":{"slug":"/projects/Benetos-Algorivm"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Project Maestro - AI Musical Analysis Platform","author":"Dr Emmanouil Benetos (PI), Prof Simon Dixon (CI)","date":null,"link":"https://www.ukri.org/councils/innovate-uk/"},"html":"","id":"78c348d8-d78f-5acf-8441-382bc6c4d791"},{"fields":{"slug":"/projects/Benetos-Moises"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Enhancing lyrics transcription with open-source architectures and fine-tuning techniques","author":"Dr Emmanouil Benetos (PI)","date":null,"link":"https://moises.ai/"},"html":"","id":"d61977ff-9345-55a4-b85e-44caffa4a7f0"},{"fields":{"slug":"/projects/Dixon-assessment"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Music Performance Assessment and Feedback","author":"Prof Simon Dixon (PI), Dr Emmanouil Benetos (CI)","date":null,"link":null},"html":"","id":"68ee56ea-59b7-58de-a8c9-805c4aba4357"},{"fields":{"slug":"/projects/Dixon-guitar-transcription"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"High Resolution Guitar Transcription","author":"Prof Simon Dixon (PI)","date":null,"link":null},"html":"","id":"4dee84b0-faa4-549a-a883-d4de0a21e205"},{"fields":{"slug":"/projects/Reiss-ProStyle"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Music Production Style Transfer (ProStyle)","author":"Prof Josh Reiss (PI)","date":null,"link":"https://www.musicweek.com/digital/read/audio-production-start-up-roex-awarded-250-000-grant-by-innovate-uk-s-ai-funding-competition/089706"},"html":"","id":"473b36de-2173-5bac-bfac-c8d0c8b27c41"}]},"news":{"nodes":[{"fields":{"slug":"/news/2024-07-15.New-PhD_Yamaha"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/040073a0863ddefed87d40085e8120d8/86b19/aim-small-logo.png","srcSet":"/static/040073a0863ddefed87d40085e8120d8/559e8/aim-small-logo.png 85w,\n/static/040073a0863ddefed87d40085e8120d8/19b99/aim-small-logo.png 170w,\n/static/040073a0863ddefed87d40085e8120d8/86b19/aim-small-logo.png 340w","sizes":"(min-width: 340px) 340px, 100vw"},"sources":[{"srcSet":"/static/040073a0863ddefed87d40085e8120d8/8f0cc/aim-small-logo.webp 85w,\n/static/040073a0863ddefed87d40085e8120d8/5376c/aim-small-logo.webp 170w,\n/static/040073a0863ddefed87d40085e8120d8/89ae4/aim-small-logo.webp 340w","type":"image/webp","sizes":"(min-width: 340px) 340px, 100vw"}]},"width":340,"height":340}}},"title":"New industry-funded PhD position","author":"Admin","date":"Mon 15 Jul 2024"},"html":"<p>We have one industry-funded PhD position to join <a href=\"https://www.c4dm.eecs.qmul.ac.uk/\">C4DM</a> and the <a href=\"https://www.aim.qmul.ac.uk/\">UKRI CDT in AI and Music</a> in September 2024, on the topic of <strong><a href=\"https://www.aim.qmul.ac.uk/wp-content/uploads/2024/07/Topic-Smart-EQ-description-Sept-24-2.pdf\">Smart EQ: Personalizing Audio with Context-aware AI using Listener Preferences and Psychological Factors</a></strong> supervised by <a href=\"https://comma.eecs.qmul.ac.uk/\">Dr Charalampos Saitis</a> and <a href=\"https://eecs.qmul.ac.uk/~gyorgyf/index.html\">Dr George Fazekas</a> in collaboration with Yamaha.</p>\n<p><a href=\"https://www.aim.qmul.ac.uk/apply/\">Apply here</a> before 26th August.</p>\n<p>Interested candidates should reach out to the supervisors by email: <a href=\"mailto:c.saitis@qmul.ac.uk\">c.saitis@qmul.ac.uk</a> and <a href=\"mailto:george.fazekas@qmul.ac.uk\">george.fazekas@qmul.ac.uk</a>.</p>","id":"e925c13b-0e20-5f93-874d-bedfd5a69a94"},{"fields":{"slug":"/news/2024-07-05.C4DM-study_pop_song_melodies"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#081818","images":{"fallback":{"src":"/static/6229b0afbe19c353213a241e7279156c/2497f/iStock-1913125761.jpg","srcSet":"/static/6229b0afbe19c353213a241e7279156c/248cc/iStock-1913125761.jpg 313w,\n/static/6229b0afbe19c353213a241e7279156c/8414e/iStock-1913125761.jpg 625w,\n/static/6229b0afbe19c353213a241e7279156c/2497f/iStock-1913125761.jpg 1250w","sizes":"(min-width: 1250px) 1250px, 100vw"},"sources":[{"srcSet":"/static/6229b0afbe19c353213a241e7279156c/f7ab9/iStock-1913125761.webp 313w,\n/static/6229b0afbe19c353213a241e7279156c/c3c5b/iStock-1913125761.webp 625w,\n/static/6229b0afbe19c353213a241e7279156c/03ce1/iStock-1913125761.webp 1250w","type":"image/webp","sizes":"(min-width: 1250px) 1250px, 100vw"}]},"width":1250,"height":1250}}},"title":"C4DM Study finds popular song melodies have become simpler over time","author":"Emmanouil Benetos","date":"Fri 05 Jul 2024"},"html":"<p>Melodies of popular songs have become simpler since the 1950s, according to a study carried out by C4DM PhD student <a href=\"https://www.qmul.ac.uk/eecs/people/profiles/hamiltonmadelineann.html\">Madeline Hamilton</a> and C4DM academic <a href=\"https://www.marcus-pearce.com/\">Dr Marcus Pearce</a>, published in the journal Scientific Reports. The full paper can be found at: : <a href=\"https://www.nature.com/articles/s41598-024-64571-x\">https://www.nature.com/articles/s41598-024-64571-x</a></p>\n<p>An analysis of hundreds of chart hits from the past 70 years has shown “a significant decline” in the complexity of rhythm and pitch in song melodies. They said the biggest transitions – or “bursts of change” – occurred in the years 1975 and 2000 – when music genres such as new wave, disco and stadium rock started gaining popularity in the mid-1970s, and hip-hop became more prominent in the early Noughties. The researchers said the findings suggest complexity and creative expression in popular music is shifting away from melody and towards other elements such as quality of the sound.</p>\n<p>See the full newsitem published in the Independent: <a href=\"https://www.independent.co.uk/arts-entertainment/music/news/top-song-melodies-simpler-study-b2574633.html\">https://www.independent.co.uk/arts-entertainment/music/news/top-song-melodies-simpler-study-b2574633.html</a></p>","id":"c404174d-8683-5050-841a-b426037fcc54"},{"fields":{"slug":"/news/2024-06-24.C4DM-academic_at_AI_chamber_music_symposium"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/0f7df906e23f5363c766cfa5e0df523a/f4df4/ccdd-event-poster.png","srcSet":"/static/0f7df906e23f5363c766cfa5e0df523a/b4532/ccdd-event-poster.png 281w,\n/static/0f7df906e23f5363c766cfa5e0df523a/31998/ccdd-event-poster.png 563w,\n/static/0f7df906e23f5363c766cfa5e0df523a/f4df4/ccdd-event-poster.png 1125w","sizes":"(min-width: 1125px) 1125px, 100vw"},"sources":[{"srcSet":"/static/0f7df906e23f5363c766cfa5e0df523a/a37a7/ccdd-event-poster.webp 281w,\n/static/0f7df906e23f5363c766cfa5e0df523a/0fbb3/ccdd-event-poster.webp 563w,\n/static/0f7df906e23f5363c766cfa5e0df523a/94e11/ccdd-event-poster.webp 1125w","type":"image/webp","sizes":"(min-width: 1125px) 1125px, 100vw"}]},"width":1125,"height":1125}}},"title":"C4DM academic at AI & Chamber Music Symposium","author":"Admin","date":"Mon 24 Jun 2024"},"html":"<p>On 24 June, C4DM academic Johan Pauwels will give a talk on \"Opportunities Unleashed by AI for Music\" at <a href=\"https://ilcs.sas.ac.uk/events/critical-creative-digital-dynamics-a-symposium-ai-digital-innovations-inter-art-chamber-0\">Critical &#x26; Creative Digital Dynamics: A Symposium on AI &#x26; Digital Innovations for Inter-art Chamber Music\nPractices</a>. The\nevent is hosted by the <a href=\"https://ilcs.sas.ac.uk/\">Institute of Languages, Cultures and Societies</a> at the <a href=\"https://ilcs.sas.ac.uk/\">School of Advanced Study, University of London</a>. It will take place between 10am and 7pm at <a href=\"https://www.openstreetmap.org/#map=16/51.5210/-0.1314\">Senate\nHouse Library, Malet Street, WC1E 7HU London</a>.</p>","id":"8c16cf0d-e7fa-5607-99b6-c04571ecbefb"},{"fields":{"slug":"/news/2024-06-21.C4DM- Ilyass_Moummad"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar:  Ilyass Moummad","author":"Admin","date":"Fri 21 Jun 2024"},"html":"<h3>C4DM Seminar: Ilyass Moummad: Self-Supervised Invariant Learning of Bird Sound Representations</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nIlyass Moummad</p>\n<p><strong>Date/time:  Friday, 21st June 2024, 2pm</strong></p>\n<p>**Location: G2, ENG, Mile End Campus, QMUL, E1 4NS **\nZoom: <a href=\"https://qmul-ac-uk.zoom.us/j/9798452959\">https://qmul-ac-uk.zoom.us/j/9798452959</a></p>\n<h2><b>Title</b>: Self-Supervised Invariant Learning of Bird Sound Representations</h2>\n<p><b>Abstract</b>: Abstract: Self-supervised learning (SSL) involves the learning of data representations without any manual annotation. It consists of solving a pretext task relevant for learning informative data representations, which can be used for transfer learning to solve downstream tasks. Among the different learning paradigms, the most successful in learning discriminative features for classification tasks are “Invariant Learning” methods. They train the model to be insensitive to pre-defined transformations (e.g. if pitch shift is used as a transformation, the model is shown two versions of the same signal with different pitch shifts and is trained to output the same representation for both versions). The choice of data transformations for learning invariance is crucial and depends on the data domain and its relevance to downstream tasks.\nIn bioacoustics, it is not yet clear which data transformations the model should be robust to. In this work, we show that simple and domain-agnostic data augmentations (which do not use any prior knowledge of the nature of bioacoustic sounds) can learn robust and informative features. We evaluate the learned representations through transfer learning to downstream tasks with different challenges such as novel classes (downstream datasets can have classes never seen during pretraining), few-shot (very few annotations given for a downstream), label shift (evaluation recordings come from different geographical regions), and covariate shift (difference in recording settings and environmental conditions of the same classes between pretraining and evaluation data).</p>\n<p><b>Bio</b>: Ilyass Moummad is a PhD student (December 2021 - November 2024) at IMT Atlantique, Brest, France. He works under the supervision of Nicolas Farrugia and is co-supervised by Romain Serizel. Currently, Ilyass is a Visiting Researcher at C4DM, QMUL, working under the supervision of Emmanouil Benetos (April - June 2024). Ilyass’s PhD topic is Deep Learning for Bioacoustics, with an interest in representation learning (both self-supervised and supervised) of animal sounds, as well as few-shot learning (species sound classification and detection from very few annotated examples).</p>","id":"c92e4d72-68bf-52d3-a4f0-b47e54d89aa1"},{"fields":{"slug":"/news/2024-06-21.C4DM-at_UKIS_2024"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/a076365473a8534739f1e58530645d6a/30cdc/UKIS-2024.png","srcSet":"/static/a076365473a8534739f1e58530645d6a/7458e/UKIS-2024.png 75w,\n/static/a076365473a8534739f1e58530645d6a/de3a1/UKIS-2024.png 150w,\n/static/a076365473a8534739f1e58530645d6a/30cdc/UKIS-2024.png 300w","sizes":"(min-width: 300px) 300px, 100vw"},"sources":[{"srcSet":"/static/a076365473a8534739f1e58530645d6a/18188/UKIS-2024.webp 75w,\n/static/a076365473a8534739f1e58530645d6a/c65bc/UKIS-2024.webp 150w,\n/static/a076365473a8534739f1e58530645d6a/078c3/UKIS-2024.webp 300w","type":"image/webp","sizes":"(min-width: 300px) 300px, 100vw"}]},"width":300,"height":300}}},"title":"C4DM at UK and Ireland Speech workshop","author":"Emmanouil Benetos","date":"Fri 21 Jun 2024"},"html":"<p>On 1-2 July 2024, C4DM researchers will participate at the <b><a href=\"https://ukis2024.eng.cam.ac.uk/\">UK and Ireland Speech Workshop (UKIS 2024)</a></b>. The UK and Ireland Speech Workshop aims to bring together researchers within the UK and Ireland’s Speech Science and Speech Technology community, both in academia and industry. UKIS 2024 is organised by the Speech Research Group, Department of Engineering, University of Cambridge in collaboration with the UK Speech Community.</p>\n<p>The below works from C4DM researchers will be presented at UKIS 2024:</p>\n<ul>\n<li>\n<p><a href=\"http://ukis2024.eng.cam.ac.uk/wp-content/uploads/2024/06/ukis2024-AbstractBook.pdf#section*.20\">Multilingual Integration in Lyrics Transcription: Data, Language Conditioning, and Transliteration Augmentation</a>, by Jiawen Huang and Emmanouil Benetos</p>\n</li>\n<li>\n<p><a href=\"http://ukis2024.eng.cam.ac.uk/wp-content/uploads/2024/06/ukis2024-AbstractBook.pdf#section*.103\">Spontaneous and Scripted Speech Classification for Multilingual Audio</a>, by Shahar Elisha, Mariano Beguerisse-Díaz, and Emmanouil Benetos</p>\n</li>\n</ul>\n<p>See you all at UKIS!</p>","id":"0984059b-8777-5d36-bd73-938e38222049"},{"fields":{"slug":"/news/2024-06-20.C4DM-at_ACM_CC_2024"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/378b54314e1a61271fd1713f1ec7a3fc/cb5b3/cc24_logo_new.png","srcSet":"/static/378b54314e1a61271fd1713f1ec7a3fc/2655f/cc24_logo_new.png 318w,\n/static/378b54314e1a61271fd1713f1ec7a3fc/be4dc/cc24_logo_new.png 636w,\n/static/378b54314e1a61271fd1713f1ec7a3fc/cb5b3/cc24_logo_new.png 1271w","sizes":"(min-width: 1271px) 1271px, 100vw"},"sources":[{"srcSet":"/static/378b54314e1a61271fd1713f1ec7a3fc/4ce64/cc24_logo_new.webp 318w,\n/static/378b54314e1a61271fd1713f1ec7a3fc/1852a/cc24_logo_new.webp 636w,\n/static/378b54314e1a61271fd1713f1ec7a3fc/7e90c/cc24_logo_new.webp 1271w","type":"image/webp","sizes":"(min-width: 1271px) 1271px, 100vw"}]},"width":1271,"height":1271}}},"title":"C4DM at ACM Creativity & Cognition 2024","author":"Corey Ford","date":"Thu 20 Jun 2024"},"html":"<p>On 23rd-26th June, several C4DM researchers will participate in the <a href=\"https://cc.acm.org/2024/\">16th ACM Conference on Creativity and Cognition 2024 (C&#x26;C 2024)</a>. C&#x26;C is a leading international conference that brings together researchers and practitioners from various disciplines to explore technologies wide impacts on creativity, from designing and working with AI tools, to the social and cultural aspects of creativity.</p>\n<p>The following paper received an <b>honorable mention award</b> at the conference:</p>\n<ul>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/97327/Ford%20Reflection%20Across%20AI-based%202024%20Accepted.pdf?sequence=2&#x26;isAllowed=y\">Reflection Across AI-based Music Composition</a>, by Corey Ford, Ashley Noel-Hirst, Sara Cardinale, Jackson Loth, Pedro Sarmento, Elizabeth Wilson, Lewis Wolstanholme, Kyle Worrall and Nick Bryan-Kinns.</li>\n</ul>\n<p>The paper collects first-person accounts, interview and questionnaire measures, on how several C4DM researchers used AI tools of their choice in their music making, contributing descriptions of how they reflected when using AI generated content.</p>\n<p>C4DM members <a href=\"https://ashleynoelhirst.co.uk/\">Ashley Noel-Hirst</a> and <a href=\"http://codetta.codes/\">Corey Ford</a> are also co-authors of the paper:</p>\n<ul>\n<li><a href=\"https://ualresearchonline.arts.ac.uk/id/eprint/22055/1/incongrous_unmarked.pdf\">Using Incongruous Genres to Explore Music Making with AI Generated Content</a>, by Nick Bryan-Kinns, Ashley Noel-Hirst, and Corey Ford.</li>\n</ul>\n<p>The paper explores how an AI tool trained on a Folk dataset is used and appropriated by musicians in the genres of both, as a playful way to explore Human-AI Interaction.</p>\n<p>The <a href=\"https://xaixarts.github.io/2024\">2nd international workshop on eXplainable AI for the Arts</a> is also run at the conference, co-organized by C4DM members Corey Ford and Shuoyang Zheng. This workshop examines the challenges and opportunities at the intersection of explainable AI and the Arts, offering a critical view on the explainable aspects of Responsible AI and Human-Centred AI. With an accepted paper from C4DM of:</p>\n<ul>\n<li>A Mapping Strategy for Interacting with Latent Audio Synthesis Using Artistic Materials by Shuoyang Zheng, Anna Xambó Sedó and Nick Bryan-Kinns.</li>\n</ul>\n<p>The paper describes how mapping sketches to the latent space of the audio synthesis RAVE model can be used to support temporal and cross-modal aspects of explainable AI. See this paper alongside other proceedings at <a href=\"https://xaixarts.github.io/2024\">https://xaixarts.github.io/2024</a></p>\n<p><a href=\"http://codetta.codes/\">Corey Ford</a> is also on the organising committee as a chair for Student Volunteers.</p>\n<p>We hope to see you all at ACM C&#x26;C!</p>","id":"eeac8eee-d488-5a11-b34d-f7a8a8dd2ad0"}]}}}
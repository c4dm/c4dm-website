{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/YIFgS8she58","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/Sandler-Artificial-Neuroscience"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png","srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/2fd20/EPSRC.png 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/de391/EPSRC.png 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/d66e1/EPSRC.webp 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/e7160/EPSRC.webp 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/5f169/EPSRC.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Artificial Neuroscience: metrology and engineering for Deep Learning using Linear Algebra","author":"Prof Mark Sandler (PI), Boris Khoruzhenko (CI)","date":null,"link":"https://www.ukri.org/councils/epsrc/"},"html":"","id":"c1314000-fe3b-5175-88a2-f239d861ddad"},{"fields":{"slug":"/projects/Barthet-Netz"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Netz: A Novel XR Musical Instrument","author":"Dr Mathieu Barthet (PI)","date":null,"link":"https://iuk-business-connect.org.uk/programme/icure/"},"html":"","id":"0040b455-65cf-5ea1-a7ce-38ab70570dae"},{"fields":{"slug":"/projects/Benetos-Algorivm"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Project Maestro - AI Musical Analysis Platform","author":"Dr Emmanouil Benetos (PI), Prof Simon Dixon (CI)","date":null,"link":"https://www.ukri.org/councils/innovate-uk/"},"html":"","id":"78c348d8-d78f-5acf-8441-382bc6c4d791"},{"fields":{"slug":"/projects/Benetos-L-Acoustics"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Online speech enhancement in scenarios with low direct-to-reverberant-ratio","author":"Dr Emmanouil Benetos (PI), Dr Aidan Hogg (CI)","date":null,"link":"https://www.l-acoustics.com/"},"html":"","id":"ce65fc6c-7228-5805-a9b2-6e14e5fd5086"},{"fields":{"slug":"/projects/Benetos-Moises"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Enhancing lyrics transcription with open-source architectures and fine-tuning techniques","author":"Dr Emmanouil Benetos (PI)","date":null,"link":"https://moises.ai/"},"html":"","id":"d61977ff-9345-55a4-b85e-44caffa4a7f0"},{"fields":{"slug":"/projects/Dixon-assessment"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Music Performance Assessment and Feedback","author":"Prof Simon Dixon (PI), Dr Emmanouil Benetos (CI)","date":null,"link":null},"html":"","id":"68ee56ea-59b7-58de-a8c9-805c4aba4357"}]},"news":{"nodes":[{"fields":{"slug":"/news/2025-07-29.C4DM-at_Interspeech_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#084898","images":{"fallback":{"src":"/static/fbc593813a2397e9f177cd960126bb83/869a6/Interspeech-2025.jpg","srcSet":"/static/fbc593813a2397e9f177cd960126bb83/97a19/Interspeech-2025.jpg 480w,\n/static/fbc593813a2397e9f177cd960126bb83/d4aca/Interspeech-2025.jpg 960w,\n/static/fbc593813a2397e9f177cd960126bb83/869a6/Interspeech-2025.jpg 1920w","sizes":"(min-width: 1920px) 1920px, 100vw"},"sources":[{"srcSet":"/static/fbc593813a2397e9f177cd960126bb83/21b1a/Interspeech-2025.webp 480w,\n/static/fbc593813a2397e9f177cd960126bb83/d6f60/Interspeech-2025.webp 960w,\n/static/fbc593813a2397e9f177cd960126bb83/26222/Interspeech-2025.webp 1920w","type":"image/webp","sizes":"(min-width: 1920px) 1920px, 100vw"}]},"width":1920,"height":1920}}},"title":"C4DM at Interspeech 2025","author":"admin","date":"Tue 29 Jul 2025"},"html":"<p>On 17-21 August, C4DM members will participate in <a href=\"https://www.interspeech2025.org/home\">Interspeech 2025</a>. Interspeech is the premier international conference for research on the science and technology of spoken language processing.</p>\n<p>The following papers authored/co-authored by C4DM members will be presented at IJCNN 2025:</p>\n<ul>\n<li>\n<p><a href=\"https://arxiv.org/abs/2505.23509\">Spectrotemporal Modulation: Efficient and Interpretable Feature Representation for Classifying Speech, Music, and Environmental Sounds</a> by Andrew Chang, Yike Li, Iran R. Roman, David Poeppel</p>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/abs/2506.02339\">Enhancing Lyrics Transcription on Music Mixtures with Consistency Loss</a> by Jiawen Huang, Felipe Sousa, Emir Demirel, Emmanouil Benetos, Igor Gadelha</p>\n</li>\n</ul>\n<p>See you at Interspeech!</p>","id":"6bc651c0-77cf-5031-a443-082420242949"},{"fields":{"slug":"/news/2025-07-28.AIM student to join the Alan Turing Institute in 2025-2026"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/84457edde2978607b183e69bea1840e1/cabd1/ATI_logo_black_W500px.jpg","srcSet":"/static/84457edde2978607b183e69bea1840e1/19e71/ATI_logo_black_W500px.jpg 128w,\n/static/84457edde2978607b183e69bea1840e1/68974/ATI_logo_black_W500px.jpg 256w,\n/static/84457edde2978607b183e69bea1840e1/cabd1/ATI_logo_black_W500px.jpg 511w","sizes":"(min-width: 511px) 511px, 100vw"},"sources":[{"srcSet":"/static/84457edde2978607b183e69bea1840e1/6766a/ATI_logo_black_W500px.webp 128w,\n/static/84457edde2978607b183e69bea1840e1/22bfc/ATI_logo_black_W500px.webp 256w,\n/static/84457edde2978607b183e69bea1840e1/9f973/ATI_logo_black_W500px.webp 511w","type":"image/webp","sizes":"(min-width: 511px) 511px, 100vw"}]},"width":511,"height":511}}},"title":"C4DM student to join the Alan Turing Institute in 2025-2026","author":"Admin","date":"Mon 28 Jul 2025"},"html":"<p>C4DM PhD student <a href=\"https://www.qmul.ac.uk/eecs/people/profiles/adityabhattacharjee-.html\">Aditya Bhattacharjee</a> has been awarded an <a href=\"https://www.turing.ac.uk/work-turing/studentships/enrichment\">enrichment placement</a> by the <a href=\"https://www.turing.ac.uk/\">Alan Turing Institute</a>, the UK’s national institute in artificial intelligence and data science, enabling him to join and interact with institute researchers and its community in the 2025/26 academic year. Aditya’s placement will be hosted by the Turing’s <a href=\"https://www.turing.ac.uk/research/research-programmes/fundamental-research\">Fundamental research in data science and AI</a> research programme.</p>\n<p>Congratulations to Aditya!</p>","id":"049b3ca8-72c8-5274-af77-62fb37ecb440"},{"fields":{"slug":"/news/2025-06-30.CfLBDP.AIMLA.2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#083858","images":{"fallback":{"src":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/18c0c/AIMLA-logo.jpg","srcSet":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/0fdf4/AIMLA-logo.jpg 300w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/7706b/AIMLA-logo.jpg 601w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/18c0c/AIMLA-logo.jpg 1201w","sizes":"(min-width: 1201px) 1201px, 100vw"},"sources":[{"srcSet":"/static/4c3e3835a3bc6ad9f7d8a77915c8904c/078c3/AIMLA-logo.webp 300w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/2b014/AIMLA-logo.webp 601w,\n/static/4c3e3835a3bc6ad9f7d8a77915c8904c/1fcf1/AIMLA-logo.webp 1201w","type":"image/webp","sizes":"(min-width: 1201px) 1201px, 100vw"}]},"width":1201,"height":1201}}},"title":"Call for Late Breaking Demo Papers: First AES International Conference on AI and Machine Learning for Audio (AIMLA 2025)","author":"Christos Plachouras","date":"Mon 30 Jun 2025"},"html":"<p>The AES International Conference on Artificial Intelligence and Machine Learning for Audio (AIMLA 2025), hosted at the Centre for Digital Music of Queen Mary University of London and taking place on Sept. 8-10, 2025 is calling for Late Breaking Demo Paper submissions.</p>\n<p>We are seeking 2-page extended abstracts showcasing prototype systems and early research results that are highly relevant to the conference theme. At least one author must register for the conference and present their work as a poster in person during the main track poster session.</p>\n<p>Submissions open on July 1, 2025 with a deadline of August 1, 2025. Submissions will be reviewed on a rolling basis.</p>\n<p>For more information on submission guidelines, templates, and technical requirements, please visit: <a href=\"https://aes2.org/contributions/2025-1st-aes-international-conference-on-artificial-intelligence-and-machine-learning-for-audio-call-for-contributions/\">https://aes2.org/contributions/2025-1st-aes-international-conference-on-artificial-intelligence-and-machine-learning-for-audio-call-for-contributions/</a></p>","id":"8db047da-7a1b-554b-b451-7b22c99c199a"},{"fields":{"slug":"/news/2025-06-28.C4dM-at_Sonar"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/cc8173d3891b7c4607cde479aee1ccce/3dc4a/sonar_2025.png","srcSet":"/static/cc8173d3891b7c4607cde479aee1ccce/7c333/sonar_2025.png 344w,\n/static/cc8173d3891b7c4607cde479aee1ccce/8e0d2/sonar_2025.png 688w,\n/static/cc8173d3891b7c4607cde479aee1ccce/3dc4a/sonar_2025.png 1376w","sizes":"(min-width: 1376px) 1376px, 100vw"},"sources":[{"srcSet":"/static/cc8173d3891b7c4607cde479aee1ccce/0f0fa/sonar_2025.webp 344w,\n/static/cc8173d3891b7c4607cde479aee1ccce/a7ca0/sonar_2025.webp 688w,\n/static/cc8173d3891b7c4607cde479aee1ccce/a8d24/sonar_2025.webp 1376w","type":"image/webp","sizes":"(min-width: 1376px) 1376px, 100vw"}]},"width":1376,"height":1376}}},"title":"C4DM at Sónar+D 2025","author":"Shuoyang Zheng","date":"Sat 28 Jun 2025"},"html":"<p>Sónar is a pioneering festival that's reflected the evolution and expansion of electronic music and digital culture since its first edition in 1994. The interactive exhibition space, <a href=\"https://sonar.es/en/activity/project-area\">Project Area at Sónar+D</a>, showcases state-of-the-art technology, innovative design, radical thinking, and cutting-edge research side-by-side in the heart of the music festival Sónar by Day.</p>\n<p>At Sónar+D Project Area, C4DM members Shuoyang Zheng and Franco Caspe joined the AI &#x26; Music exhibition area powered by S+T+ARTS to present their innovative tools for AI-driven sound creation. In addition, C4DM members Christopher Mitcheltree represented Neutone to present the cutting-edge audio plugins.</p>\n<p>Franco Caspe presented BRAVE, a timbre transfer tool that allows performers to play an AI model as an instrument, transforming timbre in real-time. Shuoyang Zheng presented Latent Terrain Synthesis, an innovative method to explore sonic landscapes dissected from the internal space of a generative AI model.</p>\n<p><a href=\"https://sonar.es/en/activity/ai-performance-playground-live\">The AI Performance Playground</a> took place between 11th and 14th June as part of Sónar+D 2025, co-organised by C4DM Senior Lecturer Anna Xambó, powered by S+T+ARTS, with support from La Salle-URL. This collaborative hacklab brought together artists, coders, musicians, DIY creators, and creative technologists to explore and deepen their use of machine learning tools, AI, and other related technologies for musical performance. C4DM member Teresa Pelinski participated in the hacklab and joined a collaborative performance at SonarÀgora - open to the general public at Sónar by Day.</p>\n<p>C4DM members Christopher Mitcheltree and Shuoyang Zheng, together with Rebecca Fiebrink (University of the Arts London) and Nao Tokui (Neutone) joined the enlightening talk panel during the hacklab with Ben Cantil (DataMind) to discuss the challenges and opportunities of being an artist using AI tools.</p>","id":"c17c66e8-67ac-5caf-a6e8-d1f066836164"},{"fields":{"slug":"/news/2025-06-26.C4DM-at_NIME_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/8dbe26f23211ec1e5b3bee060a26b5c2/8b205/NIME2025.png","srcSet":"/static/8dbe26f23211ec1e5b3bee060a26b5c2/6db79/NIME2025.png 67w,\n/static/8dbe26f23211ec1e5b3bee060a26b5c2/21a05/NIME2025.png 134w,\n/static/8dbe26f23211ec1e5b3bee060a26b5c2/8b205/NIME2025.png 268w","sizes":"(min-width: 268px) 268px, 100vw"},"sources":[{"srcSet":"/static/8dbe26f23211ec1e5b3bee060a26b5c2/8e9df/NIME2025.webp 67w,\n/static/8dbe26f23211ec1e5b3bee060a26b5c2/71b64/NIME2025.webp 134w,\n/static/8dbe26f23211ec1e5b3bee060a26b5c2/0784a/NIME2025.webp 268w","type":"image/webp","sizes":"(min-width: 268px) 268px, 100vw"}]},"width":268,"height":268}}},"title":"C4DM at NIME 2025","author":"Charalampos Saitis","date":"Thu 26 Jun 2025"},"html":"<p>On 24-27 June 2025, several C4DM researchers will participate at the <b><a href=\"https://nime2025.org/\">2025 International Conference on New Interfaces for Musical Expression (NIME 2025)</a></b>. Once again, this year's edition will have <a href=\"https://bela.io/\">Bela</a> as an official sponsor.</p>\n<p>As in previous years, C4DM will have a strong presence at the conference, both in terms of numbers and overall impact. The below papers authored or co-authored by C4DM members will be presented at the <b>paper track</b>:</p>\n<ul>\n<li><a href=\"http://nime.org/proceedings/2025/nime2025_29.pdf\">(De)Constructing Timbre at NIME: Reflecting on Technology and Aesthetic Entanglements in Instrument Design</a>, by Charalampos Saitis, Courtney N. Reed, Ashley Laurent Noel-Hirst, Giacomo Lepri, and Andrew McPherson</li>\n<li><a href=\"http://nime.org/proceedings/2025/nime2025_66.pdf\">Designing Percussive Timbre Remappings: Negotiating Audio Representations and Evolving Parameter Spaces</a>, by Jordie Shier, Rodrigo Constanzo, Charalampos Saitis, Andrew Robertson, and Andrew McPherson</li>\n<li><a href=\"http://nime.org/proceedings/2025/nime2025_67.pdf\">Sonicolour: Exploring Colour Control of Sound Synthesis with Interactive Machine Learning</a>, by Tug F. O'Flaherty, Luigi Marino, Charalampos Saitis, and Anna Xambó Sedó</li>\n<li><a href=\"http://nime.org/proceedings/2025/nime2025_9.pdf\">pybela: a Python library to interface scientific and physical computing</a>, by Teresa Pelinski, Giulio Moro, and Andrew McPherson</li>\n<li><a href=\"http://nime.org/proceedings/2025/nime2025_10.pdf\">Waveform Autoencoding at the Edge of Perceivable Latency</a>, by Franco Caspe, Andrew McPherson, and Mark Sandler</li>\n<li><a href=\"http://nime.org/proceedings/2025/nime2025_15.pdf\">Drum Modal Feedback: Concept Design of an Augmented Percussion Instrument</a>, by Lewis Wolstanholme, Jordie Shier, Rodrigo Constanzo, and Andrew McPherson</li>\n<li><a href=\"http://nime.org/proceedings/2025/nime2025_37.pdf\">Making the Immaterial Material: A Diffractive Approach Toward a Politics of Material Culture Within NIME</a>, by Brittney Allen, Andrew McPherson, Alexandria Smith, and Jason Freeman</li>\n<li><a href=\"http://nime.org/proceedings/2025/nime2025_38.pdf\">The Sparksichord: Practical Implementation of a Lorentz Force Electromagnetic Actuation and Feedback System</a>, by Adam Schmidt, Jeffrey Snyder, Gian Torrano Jacobs, Joseph Gascho, Joyce Chen, and Andrew McPherson</li>\n<li><a href=\"http://nime.org/proceedings/2025/nime2025_61.pdf\">Augmentation of a Historical Harpsichord Keyboard Replica for Haptic-Enabled Interaction in Museum Exhibitions</a>, by Matthew Hamilton, Michele Ducceschi, Roberto Livi, Catalina Vicens, and Andrew McPherson</li>\n<li><a href=\"http://nime.org/proceedings/2025/nime2025_94.pdf\">Negotiating Entanglements in the Composition and Curation of an Ultrasonic Art Installation</a>, by Nicole Robson, Andrew McPherson, and Nick Bryan-Kinns</li>\n</ul>\n<p><b>Music track</b></p>\n<ul>\n<li><a href=\"https://nime2025.org/proceedings/201.html\">Diffy</a>, by Jordie Shier; Xiaowan Yi</li>\n</ul>\n<p><b>Workshop</b></p>\n<ul>\n<li><a href=\"https://nime2025.org/proceedings/339.html\">Entangled Listening: Exploring Relational and Diverse Listening Practices for DMI Design</a>, by June Kuhn, Brittney Allen, Nicole Robson, and Andrew McPherson</li>\n</ul>","id":"0c3b786e-6948-5535-855b-3c8c81fe0c80"},{"fields":{"slug":"/news/2025-06-25.C4DM-at_Pretrain_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM at Pretrain 2025","author":"Christos Plachouras","date":"Wed 25 Jun 2025"},"html":"<p>Work from C4DM researchers was presented on June 25th at the <a href=\"https://pretrain2025.github.io/index.html\">PreTrain 2025</a> pre-conference presentation event. The event, hosted by the King's College London NLP Group in the Department of Informatics at King's College London, aimed to showcase accepted work in the ACL 2025, ICML 2025, and ICLR 2025 conferences and foster discussions.</p>\n<p>The following paper was presented:</p>\n<p><a href=\"https://arxiv.org/abs/2404.06393\">MuPT: A Generative Symbolic Music Pretrained Transformer</a>, Xingwei Qu, Yuelin Bai, <strong>Yinghao Ma</strong>, Ziya Zhou, Ka Man Lo, Jiaheng Liu, Ruibin Yuan, Lejun Min, Xueling Liu, Tianyu Zhang, Xinrun Du, Shuyue Guo, Yiming Liang, Yizhi Li, Shangda Wu, Junting Zhou, Tianyu Zheng, Ziyang Ma, Fengze Han, Wei Xue, Gus Xia, <strong>Emmanouil Benetos</strong>, Xiang Yue, Chenghua Lin, Xu Tan, Stephen W. Huang, Jie Fu, Ge Zhang</p>","id":"5c81c4fc-91f6-564c-9e91-619b0974d0ba"}]}}}
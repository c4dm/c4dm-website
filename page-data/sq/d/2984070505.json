{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/YIFgS8she58","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/EPSRC-additional-skills"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png","srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/2fd20/EPSRC.png 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/de391/EPSRC.png 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/d66e1/EPSRC.webp 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/e7160/EPSRC.webp 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/5f169/EPSRC.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"EPSRC additional skills funding summer 2025","author":"Prof Akram Alomainy (PI), Prof Simon Dixon (CI), Dr Iran Roman (CI) plus 8 others","date":null,"link":"https://www.ukri.org/councils/epsrc/"},"html":"","id":"bc1e4be0-d104-5b53-8c02-61f296532555"},{"fields":{"slug":"/projects/Fazekas-Leverhulme"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/14ae7694223d03edb7966d74ea33ce18/82c11/RAEng.png","srcSet":"/static/14ae7694223d03edb7966d74ea33ce18/2fd20/RAEng.png 125w,\n/static/14ae7694223d03edb7966d74ea33ce18/de391/RAEng.png 250w,\n/static/14ae7694223d03edb7966d74ea33ce18/82c11/RAEng.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/14ae7694223d03edb7966d74ea33ce18/d66e1/RAEng.webp 125w,\n/static/14ae7694223d03edb7966d74ea33ce18/e7160/RAEng.webp 250w,\n/static/14ae7694223d03edb7966d74ea33ce18/5f169/RAEng.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Knowledge-driven Deep Learning for Music Informatics","author":"Dr George Fazekas (PI)","date":null,"link":"https://raeng.org.uk/programmes-and-prizes/programmes/uk-grants-and-prizes/support-for-research/research-awardees/leverhulme-awardees/2025-2026/dr-george-fazekas/"},"html":"","id":"f47b222e-21cf-5c90-bff2-6181b436d1aa"},{"fields":{"slug":"/projects/Fazekas-Steinberg-2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png","srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/4b686/aimcdt.png 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/d2213/aimcdt.png 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png 615w","sizes":"(min-width: 615px) 615px, 100vw"},"sources":[{"srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/b2942/aimcdt.webp 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/46581/aimcdt.webp 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/fa942/aimcdt.webp 615w","type":"image/webp","sizes":"(min-width: 615px) 615px, 100vw"}]},"width":615,"height":615}}},"title":"AI-Powered Audio Loop Generation for Assistive Music Production","author":"Dr George Fazekas (PI)","date":null,"link":null},"html":"","id":"90759bed-47aa-57d3-8ce9-0dbe6a6b40ac"},{"fields":{"slug":"/projects/Ma-Google"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e3011e08b70d2397f6a5f53aca3eac47/b5658/Google.png","srcSet":"/static/e3011e08b70d2397f6a5f53aca3eac47/acb7c/Google.png 256w,\n/static/e3011e08b70d2397f6a5f53aca3eac47/ccc41/Google.png 512w,\n/static/e3011e08b70d2397f6a5f53aca3eac47/b5658/Google.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e3011e08b70d2397f6a5f53aca3eac47/22bfc/Google.webp 256w,\n/static/e3011e08b70d2397f6a5f53aca3eac47/d689f/Google.webp 512w,\n/static/e3011e08b70d2397f6a5f53aca3eac47/67ded/Google.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"title":"Large Language Models for Multimodal Music Understanding and Ethical Audio Generation","author":"Dr Emmanouil Benetos (PI), Yinghao Ma (PhD fellow)","date":null,"link":"https://research.google/programs-and-events/phd-fellowship/"},"html":"","id":"e4f5df67-9d4e-57da-bb16-3efcb2359e70"},{"fields":{"slug":"/projects/Pauwels-Sofilab"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6aad6623ce4e89038628dab00ebc48e0/c8f14/Sofilab.png","srcSet":"/static/6aad6623ce4e89038628dab00ebc48e0/abab1/Sofilab.png 185w,\n/static/6aad6623ce4e89038628dab00ebc48e0/96aa8/Sofilab.png 370w,\n/static/6aad6623ce4e89038628dab00ebc48e0/c8f14/Sofilab.png 739w","sizes":"(min-width: 739px) 739px, 100vw"},"sources":[{"srcSet":"/static/6aad6623ce4e89038628dab00ebc48e0/c5b6a/Sofilab.webp 185w,\n/static/6aad6623ce4e89038628dab00ebc48e0/7fd3d/Sofilab.webp 370w,\n/static/6aad6623ce4e89038628dab00ebc48e0/5bbdd/Sofilab.webp 739w","type":"image/webp","sizes":"(min-width: 739px) 739px, 100vw"}]},"width":739,"height":739}}},"title":"Smart musical corpus technologies","author":"Dr Johan Pauwels (PI)","date":null,"link":"https://sofilab.art/"},"html":"","id":"5b93e208-074a-5baf-a108-023df35a29f0"},{"fields":{"slug":"/projects/Reiss-Yamaha"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Yamaha Visiting Researcher Collaboration","author":"Prof Josh Reiss (PI)","date":null,"link":"https://intelligentsoundengineering.wordpress.com/"},"html":"","id":"145db7f6-77af-5f89-b477-6c4d87952ef4"}]},"news":{"nodes":[{"fields":{"slug":"/news/2026-01-23.C4DM-Seminar_Zhaokai_Wang"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Zhaokai Wang","author":null,"date":"Fri 23 Jan 2026"},"html":"<h3>C4DM Seminar: Zhaokai Wang: From Frames to Beats: Progress and Challenges in Video-to-Music Generation</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Zhaokai Wang</p>\n<p><strong>Date/time:</strong>  Friday, 23th Jan 2026, 2 pm</p>\n<p><strong>Location:</strong> GC222, Graduate Centre, Mile End Campus, Queen Mary University of London</p>\n<h2><b>Title</b>: From Frames to Beats: Progress and Challenges in Video-to-Music Generation</h2>\n<p><b>Abstract</b>:\nVideo-to-music generation aims to create original music that is semantically, rhythmically, and emotionally aligned with the content of a given video, addressing a critical need in media creation, entertainment, and content production. This talk provides an overview of the video-to-music generation field, including a comprehensive list of models, datasets and evaluation metrics. We will share our team's research journey in this domain, spanning early symbolic music generation with handcrafted rule-based systems to the recent MLLM-driven audio synthesis approaches. We will also discuss the current challenges and impacts on the music industry, with potential future directions for advancing video-to-music generation toward more practical, creative, and human-centric applications.</p>\n<p><b>Bio</b>:\nZhaokai Wang is currently a Ph.D. student at Shanghai Jiao Tong University and Shanghai AI Laboratory, supervised by Prof. Jifeng Dai. He is currently a visiting student at UCL, supervised by Prof. Jun Wang. He obtained his bachelor’s degree from Beihang University. His research interest includes multimodal large language models and music generation. He has published 10+ papers in TPAMI, NeurIPS, CVPR, ISMIR, etc, and has won the Best Paper Award in ACM MM 2021 and Best Paper Runner Up in NeurIPS 2025.\n<a href=\"https://scholar.google.com/citations?hl=zh-CN&#x26;user=W0zVf-oAAAAJ&#x26;view_op=list_works&#x26;sortby=pubdate\">https://scholar.google.com/citations?hl=zh-CN&#x26;user=W0zVf-oAAAAJ&#x26;view_op=list_works&#x26;sortby=pubdate</a></p>","id":"64487876-511c-5026-9f08-839c65669f07"},{"fields":{"slug":"/news/2026-01-10.C4DM-at_DAFx25"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#c8c8c8","images":{"fallback":{"src":"/static/cf113bdd255752d19416cea062d0244e/d0e31/dafx25.png","srcSet":"/static/cf113bdd255752d19416cea062d0244e/c1559/dafx25.png 130w,\n/static/cf113bdd255752d19416cea062d0244e/baeb4/dafx25.png 259w,\n/static/cf113bdd255752d19416cea062d0244e/d0e31/dafx25.png 518w","sizes":"(min-width: 518px) 518px, 100vw"},"sources":[{"srcSet":"/static/cf113bdd255752d19416cea062d0244e/a819e/dafx25.webp 130w,\n/static/cf113bdd255752d19416cea062d0244e/495a8/dafx25.webp 259w,\n/static/cf113bdd255752d19416cea062d0244e/da710/dafx25.webp 518w","type":"image/webp","sizes":"(min-width: 518px) 518px, 100vw"}]},"width":518,"height":518}}},"title":"C4DM at DAFx 2025","author":"Admin","date":"Sat 10 Jan 2026"},"html":"<p>The last Digital Audio Effects (DAFx) conference was held from 2nd to 5th September 2025 at the historic Mole Vanvitelliana in Ancona, Italy.\nDAFx is a leading international conference dedicated to the exploration of digital audio effects, signal processing, and related technologies.</p>\n<p>Day one consisted of a series of tutorials, covering topics such as non-iterative numerical simulation, filter design with logarithmic frequency resolution, to differentiable pipelines for feedback delay networks and acoustic environment editation.\nThe remaining days featured a rich technical program, including oral and poster presentations, interesting demos, and keynote talks from renowned researchers in the field.\nThe banquet on the second day was located at the other side of city, in the beautiful setting of the MaWay restaurant, overlooking the Adriatic Sea, featuring amazing seafood and risotto dishes.\nThe next day concluded with an amazing electronic sound and video performance at the conference venue, featuring a selection of Italian vintage synthesisers.</p>\n<p><img src=\"./dafx25_concert.png\" alt=\"\"></p>\n<p>This time C4DM had three technical papers accepted at DAFx 2025, focusing on the theme of <strong>differentiable systems</strong> in different paradigms and applications.</p>\n<p>The first paper, titled <a href=\"https://www.dafx.de/paper-archive/2025/DAFx25_paper_35.pdf\">Fast Differentiable Modal Simulation of Non-Linear Strings, Membranes, and Plates</a>, was presented by PhD student Rodrigo Diaz.\nThis work develop GPU-accelerated and differentiable implementations for physical modelling of musical instruments, focusing on modal synthesis of strings, membranes, and plates with non-linear effects.\nThe resulting implementations in JAX can be accessed <a href=\"https://github.com/rodrigodzf/jaxdiffmodal\">here</a>.\nThis work makes differentiable simulations easy to use, effectively bridging classical modal techniques with modern automatic differentiation methods.</p>\n<p>The second paper, titled <a href=\"https://www.dafx.de/paper-archive/2025/DAFx25_paper_54.pdf\">Differentiable Attenuation Filters for Feedback Delay Networks</a>, was presented by PhD student Ilias Ibnyahya.\nThis work focus on designing the attenutation filters used in feedback loop of feedback delay networks (FDNs) for artificial reverberation.\nCascaded second-order sections are used and shared across all delay lines, reducing the number of parameters needed.\nThe proposed design is differentiable, allowing for end-to-end optimisation of the FDN parameters using gradient-based methods.\nThe source code is available <a href=\"https://github.com/ilias-audio/iir_match\">here</a>.</p>\n<p>The last paper, titled <a href=\"https://www.dafx.de/paper-archive/2025/DAFx25_paper_9.pdf\">DiffVox: A Differentiable Model for Capturing and Analysing Vocal Effects Distributions</a>, was presented by PhD student Chin-Yun Yu.\nThis work introduces DiffVox, a chain of differentiable audio effects implemented in PyTorch.\nDiffVox consists of a parametric equaliser, a dynamic range compressor, a ping-pong delay, and a FDN-based reverb.\nThe effects were implemented in a way that is fast to evaluate, differentiable, but still represents the original audio effect well.\nIn collaboration with Sony AI, DiffVox was optimised on hundreds of proprietary solo vocal tracks processed with various vocal effects.\nThe resultings presets were analysed and visualised to understand the distributions of real-world effects settings.\nThe source code is available <a href=\"https://github.com/SonyResearch/diffvox\">here</a>.\nAn <a href=\"https://huggingface.co/spaces/yoyolicoris/diffvox\">interactive demo</a> on huggingface was also demonstrated on the last day of the conference.</p>","id":"9e940adc-ebd0-5dc8-9add-a9d85b6d82c1"},{"fields":{"slug":"/news/2026-01-06.Reimagining-music-videos-with-AI"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#081828","images":{"fallback":{"src":"/static/1d9355dcb7ccbd2e8acf2f603acb6fc0/74886/AutoMV.png","srcSet":"/static/1d9355dcb7ccbd2e8acf2f603acb6fc0/67a35/AutoMV.png 224w,\n/static/1d9355dcb7ccbd2e8acf2f603acb6fc0/c0546/AutoMV.png 448w,\n/static/1d9355dcb7ccbd2e8acf2f603acb6fc0/74886/AutoMV.png 895w","sizes":"(min-width: 895px) 895px, 100vw"},"sources":[{"srcSet":"/static/1d9355dcb7ccbd2e8acf2f603acb6fc0/f42a0/AutoMV.webp 224w,\n/static/1d9355dcb7ccbd2e8acf2f603acb6fc0/c48dd/AutoMV.webp 448w,\n/static/1d9355dcb7ccbd2e8acf2f603acb6fc0/31b61/AutoMV.webp 895w","type":"image/webp","sizes":"(min-width: 895px) 895px, 100vw"}]},"width":895,"height":895}}},"title":"Reimagining music videos with AI: C4DM research breaks new ground","author":"admin","date":"Tue 06 Jan 2026"},"html":"<p><a href=\"https://nicolaus625.github.io/\">Yinghao Ma</a>, a PhD candidate in the Centre for Digital Music at Queen Mary University of London, has helped develop AutoMV, the first open-source AI system capable of generating complete music videos directly from full-length songs.</p>\n<p>Music-to-video generation remains a major challenge for generative AI. While recent video models can produce visually impressive short clips, they often struggle with long-form storytelling, musical alignment, and character consistency. AutoMV addresses these limitations by introducing a multi-agent AI system designed specifically for full-length music video production.</p>\n<p>Developed through a collaboration between Queen Mary researchers and partners at Beijing University of Posts and Telecommunications, Nanjing University, Hong Kong University of Science and Technology, and the University of Manchester, AutoMV brings together expertise in music information retrieval, multimodal AI, and creative computing. The work was led by <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/ebenetos/\">Dr Emmanouil Benetos</a>, with contributions from Yinghao Ma as well as <a href=\"https://www.seresearch.qmul.ac.uk/cmai/people/coh/\">Dr. Changjae Oh</a> and <a href=\"https://noone65536.github.io/\">Chaoran Zhu</a> from the Centre for Intelligent Sensing.</p>\n<p>AutoMV works like a virtual film production team. First, it analyses a song’s musical structure, beats, and time-aligned lyrics. Then, a set of specialised AI agents—taking on roles such as screenwriter, director, and editor—collaborate to plan scenes, maintain character identity, and generate images and video clips. A final quality-control “verifier” agent checks for coherence and consistency, regenerating content where needed.</p>\n<p>This approach allows AutoMV to produce music videos that follow a song from beginning to end, maintaining narrative flow and visual identity throughout. Human expert evaluations show that AutoMV significantly outperforms existing commercial tools, narrowing the gap between AI-generated videos and professionally produced music videos.</p>\n<p>By lowering the cost of music video production from tens of thousands of pounds to roughly the cost of an API call, AutoMV has the potential to empower independent musicians, educators, and creators who previously lacked access to professional video production. As an open-source project, it also supports transparent, reproducible research and encourages community collaboration.</p>\n<p>The team is actively inviting researchers and students to contribute to the codebase, extend the benchmark, and explore future directions for long-form, multimodal AI systems.</p>\n<ul>\n<li>Code: <a href=\"https://github.com/multimodal-art-projection/AutoMV\">https://github.com/multimodal-art-projection/AutoMV</a></li>\n<li>Paper: <a href=\"https://arxiv.org/abs/2512.12196\">https://arxiv.org/abs/2512.12196</a></li>\n<li>Project website: <a href=\"https://m-a-p.ai/AutoMV/\">https://m-a-p.ai/AutoMV/</a></li>\n</ul>","id":"47d03e8e-fbc9-51ae-a11d-4169551a78c4"},{"fields":{"slug":"/news/2025-12-02.DMRN2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#e8e8f8","images":{"fallback":{"src":"/static/6a96619ca934bdc4aee6bdedb3d8a3e9/baaed/DMRN-logo.jpg","srcSet":"/static/6a96619ca934bdc4aee6bdedb3d8a3e9/dd515/DMRN-logo.jpg 200w,\n/static/6a96619ca934bdc4aee6bdedb3d8a3e9/47930/DMRN-logo.jpg 400w,\n/static/6a96619ca934bdc4aee6bdedb3d8a3e9/baaed/DMRN-logo.jpg 800w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[{"srcSet":"/static/6a96619ca934bdc4aee6bdedb3d8a3e9/2e34e/DMRN-logo.webp 200w,\n/static/6a96619ca934bdc4aee6bdedb3d8a3e9/416c3/DMRN-logo.webp 400w,\n/static/6a96619ca934bdc4aee6bdedb3d8a3e9/c1587/DMRN-logo.webp 800w","type":"image/webp","sizes":"(min-width: 800px) 800px, 100vw"}]},"width":800,"height":800}}},"title":"DMRN+20: Digital Music Research Network One-day Workshop 2025","author":"Admin","date":"Tue 02 Dec 2025"},"html":"<p><b>DMRN+20: Digital Music Research Network 1-Day Workshop 2025</b></p>\n<p><b>King’s College London (in person only)</b><br>\n<b>Tue 16 December 2025</b><br>\n<b><a href=\"https://www.qmul.ac.uk/dmrn/dmrn20/\"><a href=\"https://www.qmul.ac.uk/dmrn/dmrn20/\">https://www.qmul.ac.uk/dmrn/dmrn20/</a></a></b></p>\n<p>The 2020 Digital Music Research Network workshop (DMRN+20) marks the 20th edition of DMRN. Moreover, this annual gathering has a pre-history under other names, making it an institution that’s more like 25 years old! This year, we celebrate that history and the ever-growing presence of music computing in and around London. As well as the usual offering (top-notch research and friendly social atmosphere), we’ll also consider the past and future of DMRN, and the possible roles for a regionally organised network of this kind in the few (perhaps even 25) years. This event is hosted at KCL in collaboration with – but outside of – C4DM for the first time in many years, and takes the theme “Collaboration, Coordination, and Community”.</p>\n<p><b>Theme: “Collaboration, Coordination, and Community”.</b></p>\n<p>This years’ theme is “Collaboration, Coordination, and Community”. You may like to include a nod to this theme in your submission (this is optional!) and/or in your chats with others at the event. For example, you might like to discuss research communities at both local (e.g., DMRN) and global (e.g., ISMIR) scales. Likewise, you might give thought to the wider music scholarship communities with which we sometime have less interaction than we ought (e.g., ICMPC). And what about the much wider communities of musicians (professional and amateur) in London and beyond? What does successful collaborative, coordinated, and community-oriented work looks like, and what might DMRN’s role be?</p>\n<p>For further information, visit: <a href=\"https://www.qmul.ac.uk/dmrn/dmrn20/\">https://www.qmul.ac.uk/dmrn/dmrn20/</a></p>\n<p>Enquiries: Alvaro Bort (<a href=\"mailto:a.bort@qmul.ac.uk\">a.bort@qmul.ac.uk</a>)</p>","id":"e799d1ed-b2b6-5065-868e-edc40ed3782a"},{"fields":{"slug":"/news/2025-11-20.C4DM-Seminar_Eloi_Moliner"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Eloi Moliner","author":null,"date":"Thu 20 Nov 2025"},"html":"<h3>C4DM Seminar: Eloi Moliner: Diffusion Models for Audio Effects: From Blind Estimation to Automatic Mixing</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Eloi Moliner</p>\n<p><strong>Date/time:</strong>  Thursday, 20th Nov 2025, 11 am</p>\n<p><strong>Location:</strong> GC203, Graduate Centre, Mile End Campus, Queen Mary University of London</p>\n<p><a href=\"https://teams.microsoft.com/l/meetup-join/19%3ameeting_ZDcxODQ2ZTctN2NkYy00MDExLWEyOWMtNzRkMjlmMGUxMThh%40thread.v2/0?context=%7b%22Tid%22%3a%22569df091-b013-40e3-86ee-bd9cb9e25814%22%2c%22Oid%22%3a%22cad9cb9a-abc2-4aac-ab51-682e8a852753%22%7d\">Teams meeting link</a></p>\n<h2><b>Title</b>: Diffusion Models for Audio Effects: From Blind Estimation to Automatic Mixing</h2>\n<p><b>Abstract</b>:\nThis talk presents a series of recent works exploring the use of diffusion models in the analysis and generation of audio effects. In the first part, I will focus on blind and unsupervised effect estimation, where diffusion models are employed as powerful data-driven priors for recovering clean or unprocessed signals from their altered counterparts, while simultaneously estimating the parameters of a model of the underlying audio effect. In the second part, I will discuss a recent work on automatic music mixing, introducing MEGAMI (Multitrack Embedding Generative Auto MIxing)—a generative framework that models the distribution of professional mixes directly in a multitrack effect embedding space.</p>\n<p><b>Bio</b>:\nEloi Moliner received his Ph.D. degree from the Acoustics Lab of Aalto University, Espoo, Finland, in 2025. He previuosly obtained his M.Sc. degree in Telecommunications Engineering in 2021 and his B.Sc. degree in Telecommunications Technologies and Services Engineering in 2018, both from the Polytechnic University of Catalonia, Spain. He received the Best Student Paper Awards at IEEE ICASSP 2023, IWAENC 2024, and AES AIMLA 2025. His research interests include generative models for audio, audio restoration and enhancement, and audio signal processing.</p>","id":"e7b942ad-e4fe-5988-87fe-7bad707e1b48"},{"fields":{"slug":"/news/2025-11-17.C4DM-at_NeurIPS_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/a20d19fb2ed10997b9bc578028722b0a/baaed/NeurIPS-2025.jpg","srcSet":"/static/a20d19fb2ed10997b9bc578028722b0a/dd515/NeurIPS-2025.jpg 200w,\n/static/a20d19fb2ed10997b9bc578028722b0a/47930/NeurIPS-2025.jpg 400w,\n/static/a20d19fb2ed10997b9bc578028722b0a/baaed/NeurIPS-2025.jpg 800w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[{"srcSet":"/static/a20d19fb2ed10997b9bc578028722b0a/2e34e/NeurIPS-2025.webp 200w,\n/static/a20d19fb2ed10997b9bc578028722b0a/416c3/NeurIPS-2025.webp 400w,\n/static/a20d19fb2ed10997b9bc578028722b0a/c1587/NeurIPS-2025.webp 800w","type":"image/webp","sizes":"(min-width: 800px) 800px, 100vw"}]},"width":800,"height":800}}},"title":"C4DM at NeurIPS 2025","author":"Emmanouil Benetos","date":"Mon 17 Nov 2025"},"html":"<p>On 2-7 December, C4DM researchers will participate at the <a href=\"https://neurips.cc/\">39th Annual Conference on Neural Information Processing Systems (NeurIPS 2025)</a>, taking place in San Diego, USA. NeurIPS is a prestigious annual academic conference and non-profit foundation that fosters the exchange of research in artificial intelligence (AI), machine learning (ML), and computational neuroscience.</p>\n<p>The following papers from C4DM members will be presented at the <b>Datasets and Benchmarks track</b> of NeurIPS 2025:</p>\n<ul>\n<li>\n<p><a href=\"https://openreview.net/forum?id=SSF4qgsNYE\">OmniBench: Towards The Future of Universal Omni-Language Models</a> by Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, King Zhu, Hangyu Guo, Yiming Liang, Jiaheng Liu, Zekun Moore Wang, Jian Yang, Siwei Wu, Xingwei Qu, Jinjie Shi, Xinyue Zhang, Zhenzhu Yang, Yidan WEN, Yanghai Wang, Shihao Li, Zhaoxiang Zhang, Ruibo Liu, Emmanouil Benetos, Wenhao Huang, Chenghua Lin</p>\n</li>\n<li>\n<p><a href=\"https://openreview.net/forum?id=fgmrBJemlQ\">MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix</a> by Ziyang Ma, Yinghao Ma, Yanqiao Zhu, Chen Yang, Yi-Wen Chao, Ruiyang Xu, Wenxi Chen, Yuanzhe Chen, Zhuo Chen, Jian Cong, Kai Li, Keliang Li, Siyou Li, Xinfeng Li, Xiquan Li, Zheng Lian, Yuzhe Liang, Minghao Liu, Zhikang Niu, tianrui wang, Yuping Wang, Yuxuan Wang, Yihao Wu, Guanrou Yang, Jianwei Yu, Ruibin Yuan, Zhisheng Zheng, Ziya Zhou, Haina Zhu, Wei Xue, Emmanouil Benetos, Kai Yu, EngSiong Chng, Xie Chen</p>\n</li>\n</ul>\n<p>The following paper will be presented at the <b>Creative AI track</b> of NeurIPS 2025:</p>\n<ul>\n<li><a href=\"https://openreview.net/forum?id=3yeBer3J5z\">The Ghost in the Keys: A Disklavier Demo for Human-AI Musical Co-Creativity</a> by Louis Bradshaw, Alexander Spangher, Stella Biderman, Simon Colton</li>\n</ul>\n<p>The following papers will be presented at the <b><a href=\"https://aiformusicworkshop.github.io/\">NeurIPS 2025 Workshop on AI for Music</a></b>:</p>\n<ul>\n<li>\n<p><a href=\"https://openreview.net/pdf?id=yL8BrlEqHQ\">The Ghost in the Keys: A Disklavier Demo for Human-AI Musical Co-Creativity</a> by Louis Bradshaw, Alexander Spangher, Stella Biderman, Simon Colton</p>\n</li>\n<li>\n<p><a href=\"https://openreview.net/pdf?id=rXUKO0ysUy\">Perceptually Aligning Representations of Music via Noise-Augmented Autoencoders</a> by Mathias Rose Bjare, Giorgia Cantisani, Marco Pasini, Stefan Lattner, Gerhard Widmer</p>\n</li>\n<li>\n<p><a href=\"https://openreview.net/pdf?id=pbCcvZdHyG\">Evaluating Multimodal Large Language Models on Core Music Perception Tasks</a> by Brandon James Carone, Iran R Roman, Pablo Ripollés</p>\n</li>\n<li>\n<p><a href=\"https://openreview.net/pdf?id=NG187AZ71W\">Advancing Multi-Instrument Music Transcription: Results from the 2025 AMT Challenge</a> by Ojas Chaturvedi, Kayshav Bhardwaj, Tanay Gondil, Benjamin Shiue-Hal Chou, Kristen Yeon-Ji Yun, Yung-Hsiang Lu, Yujia Yan, Sungkyun Chang</p>\n</li>\n</ul>\n<p>Finally, the following paper will be presented at the <a href=\"https://differentiable-systems.github.io/workshop-eurips-2025/#schedule\">Differentiable Systems and Scientific Machine Learning workshop</a> of EurIPS:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2511.14390\">Accelerating Automatic Differentiation of Direct Form Digital Filters</a> by Chin-Yun Yu, George Fazekas</li>\n</ul>\n<p>See you all at NeurIPS!</p>","id":"cadafe6e-e591-5ae2-8b62-52de8a24c21a"}]}}}
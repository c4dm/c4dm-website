{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/YIFgS8she58","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/Sandler-Artificial-Neuroscience"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png","srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/2fd20/EPSRC.png 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/de391/EPSRC.png 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/82c11/EPSRC.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/81ae28e0342a97c266a706c00e4e8ea1/d66e1/EPSRC.webp 125w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/e7160/EPSRC.webp 250w,\n/static/81ae28e0342a97c266a706c00e4e8ea1/5f169/EPSRC.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Artificial Neuroscience: metrology and engineering for Deep Learning using Linear Algebra","author":"Prof Mark Sandler (PI), Boris Khoruzhenko (CI)","date":null,"link":"https://www.ukri.org/councils/epsrc/"},"html":"","id":"c1314000-fe3b-5175-88a2-f239d861ddad"},{"fields":{"slug":"/projects/Barthet-Netz"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Netz: A Novel XR Musical Instrument","author":"Dr Mathieu Barthet (PI)","date":null,"link":"https://iuk-business-connect.org.uk/programme/icure/"},"html":"","id":"0040b455-65cf-5ea1-a7ce-38ab70570dae"},{"fields":{"slug":"/projects/Benetos-Algorivm"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Project Maestro - AI Musical Analysis Platform","author":"Dr Emmanouil Benetos (PI), Prof Simon Dixon (CI)","date":null,"link":"https://www.ukri.org/councils/innovate-uk/"},"html":"","id":"78c348d8-d78f-5acf-8441-382bc6c4d791"},{"fields":{"slug":"/projects/Benetos-L-Acoustics"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Online speech enhancement in scenarios with low direct-to-reverberant-ratio","author":"Dr Emmanouil Benetos (PI), Dr Aidan Hogg (CI)","date":null,"link":"https://www.l-acoustics.com/"},"html":"","id":"ce65fc6c-7228-5805-a9b2-6e14e5fd5086"},{"fields":{"slug":"/projects/Benetos-Moises"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Enhancing lyrics transcription with open-source architectures and fine-tuning techniques","author":"Dr Emmanouil Benetos (PI)","date":null,"link":"https://moises.ai/"},"html":"","id":"d61977ff-9345-55a4-b85e-44caffa4a7f0"},{"fields":{"slug":"/projects/Dixon-assessment"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Music Performance Assessment and Feedback","author":"Prof Simon Dixon (PI), Dr Emmanouil Benetos (CI)","date":null,"link":null},"html":"","id":"68ee56ea-59b7-58de-a8c9-805c4aba4357"}]},"news":{"nodes":[{"fields":{"slug":"/news/2025-06-28.C4dM-at_Sonar"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/cc8173d3891b7c4607cde479aee1ccce/3dc4a/sonar_2025.png","srcSet":"/static/cc8173d3891b7c4607cde479aee1ccce/7c333/sonar_2025.png 344w,\n/static/cc8173d3891b7c4607cde479aee1ccce/8e0d2/sonar_2025.png 688w,\n/static/cc8173d3891b7c4607cde479aee1ccce/3dc4a/sonar_2025.png 1376w","sizes":"(min-width: 1376px) 1376px, 100vw"},"sources":[{"srcSet":"/static/cc8173d3891b7c4607cde479aee1ccce/0f0fa/sonar_2025.webp 344w,\n/static/cc8173d3891b7c4607cde479aee1ccce/a7ca0/sonar_2025.webp 688w,\n/static/cc8173d3891b7c4607cde479aee1ccce/a8d24/sonar_2025.webp 1376w","type":"image/webp","sizes":"(min-width: 1376px) 1376px, 100vw"}]},"width":1376,"height":1376}}},"title":"C4DM at Sónar+D 2025","author":"Shuoyang Zheng","date":"Sat 28 Jun 2025"},"html":"<p>Sónar is a pioneering festival that's reflected the evolution and expansion of electronic music and digital culture since its first edition in 1994. The interactive exhibition space, <a href=\"https://sonar.es/en/activity/project-area\">Project Area at Sónar+D</a>, showcases state-of-the-art technology, innovative design, radical thinking, and cutting-edge research side-by-side in the heart of the music festival Sónar by Day.</p>\n<p>At Sónar+D Project Area, C4DM members Shuoyang Zheng and Franco Caspe joined the AI &#x26; Music exhibition area powered by S+T+ARTS to present their innovative tools for AI-driven sound creation. In addition, C4DM members Christopher Mitcheltree represented Neutone to present the cutting-edge audio plugins.</p>\n<p>Franco Caspe presented BRAVE, a timbre transfer tool that allows performers to play an AI model as an instrument, transforming timbre in real-time. Shuoyang Zheng presented Latent Terrain Synthesis, an innovative method to explore sonic landscapes dissected from the internal space of a generative AI model.</p>\n<p><a href=\"https://sonar.es/en/activity/ai-performance-playground-live\">The AI Performance Playground</a> took place between 11th and 14th June as part of Sónar+D 2025, co-organised by C4DM Senior Lecturer Anna Xambó, powered by S+T+ARTS, with support from La Salle-URL. This collaborative hacklab brought together artists, coders, musicians, DIY creators, and creative technologists to explore and deepen their use of machine learning tools, AI, and other related technologies for musical performance. C4DM member Teresa Pelinski participated in the hacklab and joined a collaborative performance at SonarÀgora - open to the general public at Sónar by Day.</p>\n<p>C4DM members Christopher Mitcheltree and Shuoyang Zheng, together with Rebecca Fiebrink (University of the Arts London) and Nao Tokui (Neutone) joined the enlightening talk panel during the hacklab with Ben Cantil (DataMind) to discuss the challenges and opportunities of being an artist using AI tools.</p>","id":"c17c66e8-67ac-5caf-a6e8-d1f066836164"},{"fields":{"slug":"/news/2025-06-26.C4DM-at_NIME_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/8dbe26f23211ec1e5b3bee060a26b5c2/8b205/NIME2025.png","srcSet":"/static/8dbe26f23211ec1e5b3bee060a26b5c2/6db79/NIME2025.png 67w,\n/static/8dbe26f23211ec1e5b3bee060a26b5c2/21a05/NIME2025.png 134w,\n/static/8dbe26f23211ec1e5b3bee060a26b5c2/8b205/NIME2025.png 268w","sizes":"(min-width: 268px) 268px, 100vw"},"sources":[{"srcSet":"/static/8dbe26f23211ec1e5b3bee060a26b5c2/8e9df/NIME2025.webp 67w,\n/static/8dbe26f23211ec1e5b3bee060a26b5c2/71b64/NIME2025.webp 134w,\n/static/8dbe26f23211ec1e5b3bee060a26b5c2/0784a/NIME2025.webp 268w","type":"image/webp","sizes":"(min-width: 268px) 268px, 100vw"}]},"width":268,"height":268}}},"title":"C4DM at NIME 2025","author":"Charalampos Saitis","date":"Thu 26 Jun 2025"},"html":"<p>On 24-27 June 2025, several C4DM researchers will participate at the <b><a href=\"https://nime2025.org/\">2025 International Conference on New Interfaces for Musical Expression (NIME 2025)</a></b>. Once again, this year's edition will have <a href=\"https://bela.io/\">Bela</a> as an official sponsor.</p>\n<p>As in previous years, C4DM will have a strong presence at the conference, both in terms of numbers and overall impact. The below papers authored or co-authored by C4DM members will be presented at the <b>paper track</b>:</p>\n<ul>\n<li><a href=\"http://nime.org/proceedings/2025/nime2025_29.pdf\">(De)Constructing Timbre at NIME: Reflecting on Technology and Aesthetic Entanglements in Instrument Design</a>, by Charalampos Saitis, Courtney N. Reed, Ashley Laurent Noel-Hirst, Giacomo Lepri, and Andrew McPherson</li>\n<li><a href=\"http://nime.org/proceedings/2025/nime2025_66.pdf\">Designing Percussive Timbre Remappings: Negotiating Audio Representations and Evolving Parameter Spaces</a>, by Jordie Shier, Rodrigo Constanzo, Charalampos Saitis, Andrew Robertson, and Andrew McPherson</li>\n<li><a href=\"http://nime.org/proceedings/2025/nime2025_67.pdf\">Sonicolour: Exploring Colour Control of Sound Synthesis with Interactive Machine Learning</a>, by Tug F. O'Flaherty, Luigi Marino, Charalampos Saitis, and Anna Xambó Sedó</li>\n<li><a href=\"http://nime.org/proceedings/2025/nime2025_9.pdf\">pybela: a Python library to interface scientific and physical computing</a>, by Teresa Pelinski, Giulio Moro, and Andrew McPherson</li>\n<li><a href=\"http://nime.org/proceedings/2025/nime2025_10.pdf\">Waveform Autoencoding at the Edge of Perceivable Latency</a>, by Franco Caspe, Andrew McPherson, and Mark Sandler</li>\n<li><a href=\"http://nime.org/proceedings/2025/nime2025_15.pdf\">Drum Modal Feedback: Concept Design of an Augmented Percussion Instrument</a>, by Lewis Wolstanholme, Jordie Shier, Rodrigo Constanzo, and Andrew McPherson</li>\n<li><a href=\"http://nime.org/proceedings/2025/nime2025_37.pdf\">Making the Immaterial Material: A Diffractive Approach Toward a Politics of Material Culture Within NIME</a>, by Brittney Allen, Andrew McPherson, Alexandria Smith, and Jason Freeman</li>\n<li><a href=\"http://nime.org/proceedings/2025/nime2025_38.pdf\">The Sparksichord: Practical Implementation of a Lorentz Force Electromagnetic Actuation and Feedback System</a>, by Adam Schmidt, Jeffrey Snyder, Gian Torrano Jacobs, Joseph Gascho, Joyce Chen, and Andrew McPherson</li>\n<li><a href=\"http://nime.org/proceedings/2025/nime2025_61.pdf\">Augmentation of a Historical Harpsichord Keyboard Replica for Haptic-Enabled Interaction in Museum Exhibitions</a>, by Matthew Hamilton, Michele Ducceschi, Roberto Livi, Catalina Vicens, and Andrew McPherson</li>\n<li><a href=\"http://nime.org/proceedings/2025/nime2025_94.pdf\">Negotiating Entanglements in the Composition and Curation of an Ultrasonic Art Installation</a>, by Nicole Robson, Andrew McPherson, and Nick Bryan-Kinns</li>\n</ul>\n<p><b>Music track</b></p>\n<ul>\n<li><a href=\"https://nime2025.org/proceedings/201.html\">Diffy</a>, by Jordie Shier; Xiaowan Yi</li>\n</ul>\n<p><b>Workshop</b></p>\n<ul>\n<li><a href=\"https://nime2025.org/proceedings/339.html\">Entangled Listening: Exploring Relational and Diverse Listening Practices for DMI Design</a>, by June Kuhn, Brittney Allen, Nicole Robson, and Andrew McPherson</li>\n</ul>","id":"0c3b786e-6948-5535-855b-3c8c81fe0c80"},{"fields":{"slug":"/news/2025-06-25.C4DM-at_Pretrain_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM at Pretrain 2025","author":"Christos Plachouras","date":"Wed 25 Jun 2025"},"html":"<p>Work from C4DM researchers was presented on June 25th at the <a href=\"https://pretrain2025.github.io/index.html\">PreTrain 2025</a> pre-conference presentation event. The event, hosted by the King's College London NLP Group in the Department of Informatics at King's College London, aimed to showcase accepted work in the ACL 2025, ICML 2025, and ICLR 2025 conferences and foster discussions.</p>\n<p>C4DM Members presented the following papers they co-authored:</p>\n<p><a href=\"https://arxiv.org/abs/2404.06393\">MuPT: A Generative Symbolic Music Pretrained Transformer</a>, Xingwei Qu, Yuelin Bai, <strong>Yinghao Ma</strong>, Ziya Zhou, Ka Man Lo, Jiaheng Liu, Ruibin Yuan, Lejun Min, Xueling Liu, Tianyu Zhang, Xinrun Du, Shuyue Guo, Yiming Liang, Yizhi Li, Shangda Wu, Junting Zhou, Tianyu Zheng, Ziyang Ma, Fengze Han, Wei Xue, Gus Xia, <strong>Emmanouil Benetos</strong>, Xiang Yue, Chenghua Lin, Xu Tan, Stephen W. Huang, Jie Fu, Ge Zhang</p>\n<p>Reproducibility: The New Frontier in AI Governance,\tIsrael Mason-Williams, <strong>Gabryel Mason-Williams</strong></p>","id":"5c81c4fc-91f6-564c-9e91-619b0974d0ba"},{"fields":{"slug":"/news/2025-06-24.C4DM-Seminar_Ye_Wang"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/7679529f166e79781779fa56b1619637/a687e/yewang.jpg","srcSet":"/static/7679529f166e79781779fa56b1619637/60f77/yewang.jpg 37w,\n/static/7679529f166e79781779fa56b1619637/deb6c/yewang.jpg 74w,\n/static/7679529f166e79781779fa56b1619637/a687e/yewang.jpg 148w","sizes":"(min-width: 148px) 148px, 100vw"},"sources":[{"srcSet":"/static/7679529f166e79781779fa56b1619637/7e23d/yewang.webp 37w,\n/static/7679529f166e79781779fa56b1619637/e1943/yewang.webp 74w,\n/static/7679529f166e79781779fa56b1619637/4c54c/yewang.webp 148w","type":"image/webp","sizes":"(min-width: 148px) 148px, 100vw"}]},"width":148,"height":148}}},"title":"C4DM Seminar: Ye Wang","author":"nicolaus625","date":"Tue 24 Jun 2025"},"html":"<h3>C4DM Seminar: Ye Wang: Speech and Music AI: Connecting the Dots of AI and Human Potential</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong> Prof. Ye Wang (National University of Singapore)</p>\n<p><strong>Date/time:</strong>  Tuesday, 24th June 2025, 11am</p>\n<p><strong>Location:</strong> NOW CHANGE TO GC202, Graduate Centre, Mile End Campus, QMUL</p>\n<h2><b>Title</b>: Speech and Music AI: Connecting the Dots of AI and Human Potential</h2>\n<p><b>Abstract</b>: Speech and music are traditionally studied by different research communities. In this talk, I will share some insights from my journey of connecting the dots of speech, music, AI and neuroscience for real world applications in boosting human health and potential. This decade-long exploration has shaped my signature research – Sound and Music Computing for Human Health and Potential (SMC4HHP).</p>\n<p><b>Bio</b>: Prof. Ye Wang is affiliated with the Computer Science Department at the National University of Singapore (NUS), the NUS Graduate School’s Integrative Sciences and Engineering Programme (ISEP), the Institute of Data Science (IDS), as well as the Institute for Applied Learning Sciences &#x26; Educational Technology (ALSET). He established and continues to direct the NUS Sound and Music Computing (SMC) Lab (<a href=\"https://smcnus.comp.nus.edu.sg/\">https://smcnus.comp.nus.edu.sg/</a>). Before joining NUS, he was a member of the technical staff at Nokia Research Center in Tampere, Finland for 9 years. His research sits at the intersection of Sound and Music Computing (SMC) and Human Health and Potential (HHP).</p>","id":"fc49e0d5-2fe3-5419-b19e-61f9d82a3e24"},{"fields":{"slug":"/news/2025-06-16.C4DM-at_IJCNN_2025"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/a58a8504bc44da5ee614b02910398e33/13677/IJCNN-2025.png","srcSet":"/static/a58a8504bc44da5ee614b02910398e33/de391/IJCNN-2025.png 250w,\n/static/a58a8504bc44da5ee614b02910398e33/82c11/IJCNN-2025.png 500w,\n/static/a58a8504bc44da5ee614b02910398e33/13677/IJCNN-2025.png 1000w","sizes":"(min-width: 1000px) 1000px, 100vw"},"sources":[{"srcSet":"/static/a58a8504bc44da5ee614b02910398e33/e7160/IJCNN-2025.webp 250w,\n/static/a58a8504bc44da5ee614b02910398e33/5f169/IJCNN-2025.webp 500w,\n/static/a58a8504bc44da5ee614b02910398e33/3cd29/IJCNN-2025.webp 1000w","type":"image/webp","sizes":"(min-width: 1000px) 1000px, 100vw"}]},"width":1000,"height":1000}}},"title":"C4DM at IJCNN 2025","author":"Admin","date":"Mon 16 Jun 2025"},"html":"<p>On 30 June - 5 July 2025, C4DM researchers will participate at the <b><a href=\"https://2025.ijcnn.org/\">IEEE International Joint Conference on Neural Networks (IJCNN 2025)</a></b>, the flagship conference of the IEEE Computational Intelligence Society and the International Neural Network Society.</p>\n<p>As in previous years, the Centre for Digital Music will have a strong presence at the conference. The following papers authored/co-authored by C4DM members will be presented at IJCNN 2025:</p>\n<ul>\n<li>\n<p><a href=\"https://arxiv.org/abs/2502.04522\">ImprovNet - Generating Controllable Musical Improvisations with Iterative Corruption Refinement</a>, by Keshav Bhandari, Sungkyun Chang, Tongyu Lu, Fareza Rahman Enus, Louis Bradshaw, Dorien Herremans, Simon Colton</p>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/abs/2505.03314\">Mamba-Diffusion Model with Learnable Wavelet for Controllable Symbolic Music Generation</a>, by Jincheng Zhang, George Fazekas, Charalampos Saitis</p>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/abs/2505.06224\">Towards a Unified Representation Evaluation Framework Beyond Downstream Tasks</a>, by Christos Plachouras, Julien Guinot, George Fazekas, Elio Quinton, Emmanouil Benetos, Johan Pauwels</p>\n</li>\n</ul>\n<p>The following presentation from C4DM members will also be made at IJCNN 2025:</p>\n<ul>\n<li>Split Fine-Tuning of BERT-based Music Models in the Edge-Cloud Continuum: An Empirical Analysis, by Bradley Aldous, Wai Fong Tam, Ahmed M. A. Sayed</li>\n</ul>\n<p>See you in Rome!</p>","id":"4511d736-f385-5145-b164-f56c1e9a168f"},{"fields":{"slug":"/news/2025-06-12.Mark-Sandler_presents_at_Erlangen_AI_Hub_Conference"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#585858","images":{"fallback":{"src":"/static/9eef75c01631f1a2caf17509f71a027f/d931f/Sandler-Erlangen.jpg","srcSet":"/static/9eef75c01631f1a2caf17509f71a027f/c2c05/Sandler-Erlangen.jpg 384w,\n/static/9eef75c01631f1a2caf17509f71a027f/b9792/Sandler-Erlangen.jpg 768w,\n/static/9eef75c01631f1a2caf17509f71a027f/d931f/Sandler-Erlangen.jpg 1536w","sizes":"(min-width: 1536px) 1536px, 100vw"},"sources":[{"srcSet":"/static/9eef75c01631f1a2caf17509f71a027f/6d535/Sandler-Erlangen.webp 384w,\n/static/9eef75c01631f1a2caf17509f71a027f/482be/Sandler-Erlangen.webp 768w,\n/static/9eef75c01631f1a2caf17509f71a027f/52a60/Sandler-Erlangen.webp 1536w","type":"image/webp","sizes":"(min-width: 1536px) 1536px, 100vw"}]},"width":1536,"height":1536}}},"title":"Mark Sandler presents at Erlangen AI Hub Conference","author":"Admin","date":"Thu 12 Jun 2025"},"html":"<p></p>\n<p>The <a href=\"https://erlangenhub.ox.ac.uk/hubs-major-conference-brings-together-leading-minds-at-the-intersection-of-mathematics-and-ai/\">Erlangen AI Hub Conference</a> took place on 9-11 June 2025 at Queen Mary University of London. It brought together over 100 leading minds from across the UK’s mathematical, algorithmic and computational communities to advance the application of pure mathematics in AI. It formed a key element of an exciting programme that aims to unite and revolutionise the mathematical field to unlock new and improved AI systems.</p>\n<p>At the conference, C4DM Director <a href=\"https://www.qmul.ac.uk/eecs/people/profiles/sandlermark.html\">Prof. Mark Sandler</a> presented \"The Case for Artificial Neuroscience: Holistic Rigour for Understanding and Engineering Better Deep Learning\". The talk is linked with Prof. Sandler's ongoing project funded by the EPSRC entitled <a href=\"https://gtr.ukri.org/projects?ref=EP%2FZ535448%2F1\">Artificial Neuroscience: metrology and engineering for Deep Learning using Linear Algebra</a>.</p>\n<p>The EPSRC-funded Erlangen AI Hub exists to revolutionise the application of pure mathematics to understand AI, unifying and expanding the field to unlock new, more intelligent systems - more information on the hub can be found at <a href=\"https://erlangenhub.ox.ac.uk/\">https://erlangenhub.ox.ac.uk/</a>.</p>","id":"c8d84686-13c0-5a52-838b-2badd8267e63"}]}}}
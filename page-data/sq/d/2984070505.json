{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/YIFgS8she58","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/Benetos-Algorivm"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Project Maestro - AI Musical Analysis Platform","author":"Dr Emmanouil Benetos (PI), Prof Simon Dixon (CI)","date":null,"link":"https://www.ukri.org/councils/innovate-uk/"},"html":"","id":"78c348d8-d78f-5acf-8441-382bc6c4d791"},{"fields":{"slug":"/projects/Benetos-L-Acoustics"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Online speech enhancement in scenarios with low direct-to-reverberant-ratio","author":"Dr Emmanouil Benetos (PI), Dr Aidan Hogg (CI)","date":null,"link":"https://www.l-acoustics.com/"},"html":"","id":"ce65fc6c-7228-5805-a9b2-6e14e5fd5086"},{"fields":{"slug":"/projects/Benetos-Moises"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Enhancing lyrics transcription with open-source architectures and fine-tuning techniques","author":"Dr Emmanouil Benetos (PI)","date":null,"link":"https://moises.ai/"},"html":"","id":"d61977ff-9345-55a4-b85e-44caffa4a7f0"},{"fields":{"slug":"/projects/Dixon-assessment"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Music Performance Assessment and Feedback","author":"Prof Simon Dixon (PI), Dr Emmanouil Benetos (CI)","date":null,"link":null},"html":"","id":"68ee56ea-59b7-58de-a8c9-805c4aba4357"},{"fields":{"slug":"/projects/Dixon-guitar-transcription"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"High Resolution Guitar Transcription","author":"Prof Simon Dixon (PI)","date":null,"link":null},"html":"","id":"4dee84b0-faa4-549a-a883-d4de0a21e205"},{"fields":{"slug":"/projects/Reiss-ProStyle"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png","srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/2fd20/IUK.png 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/de391/IUK.png 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/82c11/IUK.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/4a9a1456d73a0d828225e48944ebd69a/d66e1/IUK.webp 125w,\n/static/4a9a1456d73a0d828225e48944ebd69a/e7160/IUK.webp 250w,\n/static/4a9a1456d73a0d828225e48944ebd69a/5f169/IUK.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":500}}},"title":"Music Production Style Transfer (ProStyle)","author":"Prof Josh Reiss (PI)","date":null,"link":"https://www.musicweek.com/digital/read/audio-production-start-up-roex-awarded-250-000-grant-by-innovate-uk-s-ai-funding-competition/089706"},"html":"","id":"473b36de-2173-5bac-bfac-c8d0c8b27c41"}]},"news":{"nodes":[{"fields":{"slug":"/news/2024-25-09.C4DM-Seminar_Kavya_Ranjan_Saxena"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar:  Kavya Ranjan Saxena","author":"Admin","date":"Wed 25 Sep 2024"},"html":"<h3>C4DM Seminar: Kavya Ranjan Saxena: Meta-learning-based domain adaptation for melody extraction</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nKavya Ranjan Saxena</p>\n<p><strong>Date/time:  Wednesday, 25th September 2024, 11am</strong></p>\n<p>**Location: GC205, Graduate Centre Building, Mile End Campus, QMUL, E1 4NS **\nZoom: <a href=\"https://qmul-ac-uk.zoom.us/j/2387202947\">https://qmul-ac-uk.zoom.us/j/2387202947</a></p>\n<h2><b>Title</b>: Meta-learning-based domain adaptation for melody extraction</h2>\n<p><b>Abstract</b>: The task of extracting the dominant pitch from polyphonic audio is crucial in the music information retrieval field. A substantial amount of labelled audio data is required to effectively train the machine learning models to perform the task. Generally, the traditional models trained on audios of one domain, i.e., source, may not accurately extract pitch from audios of different domains, i.e., target. To boost the performance, the models are adapted on minimal labelled data from the target domain, a method known as the supervised domain adaptation. We use the meta-learning algorithm as the supervised domain adaptation method for the task of melody extraction, by proposing a novel weighting technique to handle the class imbalance when adapting to a few audios in the target domain. Further, this method can be extended as an efficient interactive melody extraction method based on active adaptation. This method selects the regions in the target audio that require human annotation using a confidence criterion based on normalized true class probability. The annotations are used by the model to adapt itself to the target domain using meta-learning. The meta-learning-based domain adaptation method is model-agnostic and can be applied to other non-adaptive melody extraction models to boost their performance.</p>\n<p><b>Bio</b>: Kavya Ranjan Saxena is a Ph.D. student at the Indian Institute of Technology Kanpur, India. Her research interests are in machine learning for signal processing with a focus on domain adaptation for melody extraction in the field of music information retrieval. Currently, she is working as an Intern – Speech Research Scientist at Krutrim (an Ola company), where her work focuses on Audio LLMs.</p>","id":"93032c40-6437-5799-a4cc-3a6ef2c5de8c"},{"fields":{"slug":"/news/2024-09-19.MoralBERTApp"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"Moral Values Detection App","author":"Admin","date":"Thu 19 Sep 2024"},"html":"<p>C4DM PhD researchers Vjosa Preniqi and Iacopo Ghinassi, funded via the <a href=\"https://dame.qmul.ac.uk/\">DAME CDT</a>, have developed <a href=\"https://huggingface.co/spaces/vjosap/MoralBERTApp\">a new web app that automatically predicts moral values in text and music lyrics</a>. Users can upload CSV or Excel files containing text or lyrics, and the app will assign probabilities for 10 moral categories. It is particularly effective for social media posts and music lyrics. This new tool provides a simple, user-friendly interface for analysing how language reflects moral beliefs, offering useful insights for research in music and media analysis. Try it out!</p>\n<p>The app is based on models from two recent papers:</p>\n<ul>\n<li><a href=\"https://dl.acm.org/doi/10.1145/3677525.3678694\">MoralBERT: A Fine-Tuned Language Model for Capturing Moral Values in Social Discussions</a> (ACM GoodIT 2024)</li>\n<li><a href=\"https://arxiv.org/abs/2407.18787\">Automatic Detection of Moral Values in Music Lyrics</a> (ISMIR 2024)</li>\n</ul>\n<p>This work is a collaboration between <a href=\"https://www.c4dm.eecs.qmul.ac.uk/\">C4DM</a> and <a href=\"https://www.isi.it/\">ISI Foundation</a>.</p>","id":"6771d8b5-21db-5edc-b861-cd71e808ca26"},{"fields":{"slug":"/news/2024-09-10.C4DM-Seminar_Gloria_Dal_Santo_Sebastian_J_Schlecht:"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar:  Gloria Dal Santo & Sebastian J. Schlecht:","author":"Admin","date":"Tue 10 Sep 2024"},"html":"<h3>C4DM Seminar: Gloria Dal Santo &#x26; Sebastian J. Schlecht: Machine Learning-Based Artificial Reverberation &#x26; Non-stationary Noise Removal from Repeated sweep Measurements</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nGloria Dal Santo &#x26; Sebastian J. Schlecht</p>\n<p><strong>Date/time:  TUesday, 10th September 2024, 1.30pm</strong></p>\n<p>**Location: GC601, Graduate Centre Building, Mile End Campus, QMUL, E1 4NS **\nZoom: <a href=\"https://qmul-ac-uk.zoom.us/j/9798452959\">https://qmul-ac-uk.zoom.us/j/9798452959</a></p>\n<h2><b>Title</b>: Machine Learning-Based Artificial Reverberation &#x26; Non-stationary Noise Removal from Repeated sweep Measurements</h2>\n<p><b>Abstract</b>:\nGloria: The Feedback Delay Network (FDN) is a widely used approach in artificial reverberation, structured by generalizing the parallel comb-filter architecture through the interconnection of delays via a feedback matrix. Motivated by its cost-effectiveness, our research explores the FDN topology and integrates it into a machine learning framework with the aim of defining a novel methodology for accurate real-time room simulation. We are working towards a new paradigm for Room Impulse Response synthesis, where accuracy, cost-effectiveness, and ease of application coexist.</p>\n<p>Sebastian: Acoustic measurements using sine sweeps are prone to background noise and non-stationary disturbances.\nRepeated measurements can be averaged to improve the resulting signal-to-noise ratio. However, averaging leads to poor\nrejection of non-stationary high-energy disturbances and, in the case of a time-variant environment, causes attenuation at\nhigh frequencies. This paper proposes a robust method to combine repeated sweep measurements using across-measurement\nmedian filtering in the time-frequency domain. The method, called Mosaic, successfully rejects non-stationary noise, sup-\npresses background noise and is more robust toward time variation than averaging. The proposed method allows high-\nquality measurement of impulse responses in a noisy environment.</p>\n<p><b>Bio</b>:\nGloria Dal Santo received the B.Sc. degree in Electronic and Communications Engineering from Politecnico di Torino, Turin, Italy, and the M.Sc. degree in Electrical and Electronic Engineering from the Ecole Polytechnique Fédérale de Lausanne, Lausanne, Switzerland, in 2020 and 2022. She is currently working toward the Doctoral degree with the Acoustics Lab, Aalto University, Espoo, Finland. Her research interests include artificial reverberation and audio applications of machine learning.</p>\n<p>Sebastian J. Schlecht (Senior Member, IEEE) received the Diploma in applied mathematics from the University of Trier, Trier, Germany, in 2010, and the M.Sc. degree in digital music processing from the School of Electronic Engineering and Computer Science, Queen Mary University of London, London, U.K., in 2011, and the Doctoral degree with the International Audio Laboratories Erlangen, Erlangen, Germany, on artificial spatial reverberation and reverberation enhancement systems in 2017.\nHe is currently an associate Professor for Signal Processing at the Friedrich-Alexander-Universität Erlangen-Nürnberg, Germany. This position is part of the Chair of Multimedia Communications and Signal Processing. Until 2024, he was Professor of Practice at Acoustics Lab, Aalto University, Department of Information and Communications Engineering, and Media Lab, Department of Art and Media, Aalto University, Espoo, Finland. From 2012 to 2019, he was also an External Research and Development Consultant and Lead Developer of the 3D Reverb algorithm with Fraunhofer IIS, Erlangen.</p>","id":"890cb0ea-9268-5156-bb0f-88edad5fc2cb"},{"fields":{"slug":"/news/2024-09-09.C4DM-Seminar_Peter_Meier"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar:  Peter Meier","author":"Admin","date":"Mon 09 Sep 2024"},"html":"<h3>C4DM Seminar: Peter Meier: Real-Time Beat Tracking and Control Signal Generation for Interactive Music Applications</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nPeter Meier</p>\n<p><strong>Date/time:  Monday, 9th September 2024, 2pm</strong></p>\n<p>**Location: GC105, Graduate Centre Building, Mile End Campus, QMUL, E1 4NS **\nZoom: <a href=\"https://qmul-ac-uk.zoom.us/j/2387202947\">https://qmul-ac-uk.zoom.us/j/2387202947</a></p>\n<h2><b>Title</b>: Real-Time Beat Tracking and Control Signal Generation for Interactive Music Applications</h2>\n<p><b>Abstract</b>: In this seminar, we will explore the theory and application of a real-time beat tracking system based on the Predominant Local Pulse (PLP) method. We will start by detailing how the traditional PLP algorithm has been transformed into a real-time procedure capable of operating with zero latency. This real-time adaptation not only tracks beat positions but also provides dynamic insights such as beat context, stability, and lookahead predictions for each frame of real-time audio. Following the theoretical overview, we will demonstrate practical applications of the system's outputs. These include the generation of beat-synchronous Low Frequency Oscillators (LFOs) and confidence-based control signals for dynamic manipulation of audio effect parameters in real-time. We will showcase a prototype audio plugin integrated within a Digital Audio Workstation (DAW), highlighting its utility in mixing scenarios and live music performances. Additionally, we will present various audio examples that use our system for controlling different audio effects in sync with the beat of the music.</p>\n<p><b>Bio</b>: Peter Meier is a PhD student at the International Audio Laboratories Erlangen, a joint institution of Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU) and the Fraunhofer Institute for Integrated Circuits (IIS) in Germany. Under the supervision of Prof. Meinard Müller, his research focuses on interactive music analysis, aiming to transform offline algorithms into real-time applications for creative music making and educational music gaming. Peter holds a Master's degree in Media Technology with a specialization in Media Informatics from the Deggendorf Institute of Technology in Germany, where he also gained 15 years of experience working as an audio engineer in the university's audio labs. Outside his academic work, Peter is a passionate musician who enjoys singing, drumming, and playing guitar.</p>\n<p><b>Presentation</b>:\n<a href=\"https://drive.google.com/file/d/1Nmd08IZyW1BS-XTUYhDM6uwHRvgN4Hwz/view?usp=share_link\">[PDF Slides]</a></p>","id":"955d4337-37c9-5966-b315-50769f1018be"},{"fields":{"slug":"/news/2024-09-03.C4DM-Seminar_Kazunobu_KONDO"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar:  Kazunobu KONDO (Yamaha)","author":"Admin","date":"Tue 03 Sep 2024"},"html":"<h3>C4DM Seminar: Kazunobu KONDO: Introduction of Yamaha Research and Development division and global research internship program</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nKazunobu KONDO</p>\n<p><strong>Date/time:  Tuesday, 3rd September 2024, 1.30pm</strong></p>\n<p>**Location: GC601 Montagu LT, Graduate Centre Building, Mile End Campus, QMUL, E1 4NS **\nZoom: <a href=\"https://qmul-ac-uk.zoom.us/j/9798452959\">https://qmul-ac-uk.zoom.us/j/9798452959</a></p>\n<h2><b>Title</b>: Introduction of Yamaha Research and Development division and global research internship program</h2>\n<p><b>Abstract</b>: Introducing our research direction, selected research topics, facilities of Yamaha Research and Development division.\nKansei (in Japanese) for music, which is a perceptual perspective of music understanding, is one of our key research quetions.\nMusic performers listen their own sound when performing in order to describe emotional musical expression.\nTherefore, we are analyzing sounds, musical symbols and human behavior to be utilized for AI-powered products.\nFrom September, we will begin accepting for our global internship program for the summer of 2025, in this presentation, the overview of our internship program will be introduced.</p>\n<p><b>Bio</b>: Kazunobu Kondo received the B.E., M.E. and Ph.D. degrees from Nagoya University, Japan, in 1991, 1993, and 2014, respectively.\nHe joined the Electronics Development Center, Yamaha Co., Ltd. in 1993.\nHe is currently a principal engineer of Yamaha Research and Development Division.\nHis research interests include blind source separation, noise reduction, and dereverbetation.\nHe is a member of the IEICE, the Acoustical Society of Japan and the Audio Engineering Society, and an editorial board member of the Journal of the Audio Engineering Society.</p>","id":"8b30fb78-449a-5470-953b-d4ae01280258"},{"fields":{"slug":"/news/2024-08-31.C4DM-at_Interspeech_2024"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/8c174aab3a9d4d0c7f6f5b3f49e4192b/3175f/Interspeech-2024.jpg","srcSet":"/static/8c174aab3a9d4d0c7f6f5b3f49e4192b/16e4b/Interspeech-2024.jpg 102w,\n/static/8c174aab3a9d4d0c7f6f5b3f49e4192b/d4e31/Interspeech-2024.jpg 203w,\n/static/8c174aab3a9d4d0c7f6f5b3f49e4192b/3175f/Interspeech-2024.jpg 406w","sizes":"(min-width: 406px) 406px, 100vw"},"sources":[{"srcSet":"/static/8c174aab3a9d4d0c7f6f5b3f49e4192b/9b4d3/Interspeech-2024.webp 102w,\n/static/8c174aab3a9d4d0c7f6f5b3f49e4192b/6f337/Interspeech-2024.webp 203w,\n/static/8c174aab3a9d4d0c7f6f5b3f49e4192b/3a205/Interspeech-2024.webp 406w","type":"image/webp","sizes":"(min-width: 406px) 406px, 100vw"}]},"width":406,"height":406}}},"title":"C4DM at Interspeech 2024","author":"Christos Plachouras","date":"Sat 31 Aug 2024"},"html":"<p>On 31 August to 5 September, C4DM members will participate in <a href=\"https://interspeech2024.org/\">Interspeech 2024</a> and its <a href=\"https://interspeech2024.org/satellite/\">satellite events</a>. Interspeech is the premier international conference for research on the science and technology of spoken language processing.</p>\n<p>C4DM PhD student Chin-Yun Yu will present his paper with C4DM academic Dr George Fazekas  \"<a href=\"https://arxiv.org/abs/2406.05128\">Differentiable Time-Varying Linear Prediction in the Context of End-to-End Analysis-by-Synthesis</a>\". The paper introduces improvements to the GOLF voice synthesizer, by implementing a joint filtering approach for noise and harmonics using a single LP (Linear Prediction) filter, resembling a classic source-filter model, and replacing frame-wise approximation with sample-by-sample LP processing, implemented efficiently in C++ and CUDA. These modifications result in smoother spectral envelopes, reduced artefacts, and improved performance in listening tests compared to other baselines. More information can be found <a href=\"https://yoyololicon.github.io/golf2-demo/\">here</a>.</p>\n<p>C4DM PhD student Antonella Torrisi will present her paper with C4DM PhD student Inês Nolasco and C4DM academic Emmanouil Benetos entitled \"<a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/98700\">Exploratory analysis of early-life chick calls</a>\" at the <a href=\"https://vihar-2024.vihar.org/\">International Workshop on Vocal Interactivity in-and-between Humans, Animals and Robots (VIHAR)</a>. The paper describes the development of a computational framework for automatically detecting and analysing vocalizations of one-day-old chicks (Gallus gallus). Various clustering techniques are used to explore whether chick calls can be categorised into distinct types or if they exist on a continuous spectrum, aiming to provide a more systematic and unbiased approach to understanding chick vocal behaviour compared to previous studies.</p>\n<p>C4DM PhD student Farida Yusuf is part of the programme committee for the <a href=\"https://sites.google.com/view/yfrsw-2024\">Young Female Researchers in Speech Workshop (YFRSW)</a>. The workshop is designed for Bachelor’s and Master’s students currently engaged in speech science and technology research, aiming to promote interest in the field among those who haven’t yet committed to pursuing a PhD. It features panel discussions, student poster presentations, and mentoring sessions, providing participants with opportunities to showcase their research and engage with PhD students and senior researchers in the field.</p>\n<p>See you at Interspeech!</p>","id":"6bbbc6f5-d989-5582-99fd-a096953860ad"}]}}}
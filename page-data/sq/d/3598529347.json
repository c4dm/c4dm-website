{"data":{"text":{"html":"<p>The Communication Acoustics Lab at the Centre for Digital Music conducts cutting edge research into the ways we perceive sound and technologies for improving communication. We work across disciplines of music, engineering, psychology, and cognitive science to study communication between humans and between humans and machines.</p>\n<p>Topics of interest include:</p>\n<ul>\n<li>Perception, cognition &#x26; aesthetics of sound</li>\n<li>Cross-sensory perception involving sound</li>\n<li>Enabling digital audio systems with an understanding of how we listen</li>\n<li>Modelling human values (&#x26; biases) with multimodal music data</li>\n<li>Digital tools for education, outreach, &#x26; research</li>\n</ul>\n<p>For more information, please visit the lab webpage (under construction)</p>\n<h2>Members</h2>","frontmatter":{"title":""}},"people":{"nodes":[{"id":"5dd23ff3-3112-5b88-8b4e-f6493bdcdbac","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#8898c8","images":{"fallback":{"src":"/static/bf8d7356ac6d442350ba1cb9bcc35445/baaed/andrewmcpherson.jpg","srcSet":"/static/bf8d7356ac6d442350ba1cb9bcc35445/dd515/andrewmcpherson.jpg 200w,\n/static/bf8d7356ac6d442350ba1cb9bcc35445/47930/andrewmcpherson.jpg 400w,\n/static/bf8d7356ac6d442350ba1cb9bcc35445/baaed/andrewmcpherson.jpg 800w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[{"srcSet":"/static/bf8d7356ac6d442350ba1cb9bcc35445/2e34e/andrewmcpherson.webp 200w,\n/static/bf8d7356ac6d442350ba1cb9bcc35445/416c3/andrewmcpherson.webp 400w,\n/static/bf8d7356ac6d442350ba1cb9bcc35445/c1587/andrewmcpherson.webp 800w","type":"image/webp","sizes":"(min-width: 800px) 800px, 100vw"}]},"width":800,"height":800}}},"name":"Prof Andrew McPherson","role":"Academic","url":"http://www.eecs.qmul.ac.uk/~andrewm","acadposition":"Professor of Musical Interaction","blurb":"new interfaces for musical expression, augmented instruments, performance study, human-computer interaction, embedded hardware","themes":["augmi","soundsynthesis"]}},{"id":"7920f116-7143-57a4-b40b-9325b220e835","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Benjamin Hayes","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/hayesbenjaminjames.html","acadposition":"PhD Student","blurb":"Perceptually motivated deep learning approaches to creative sound synthesis","themes":["soundsynthesis","mcog"]}},{"id":"3e2bbfc8-0f9b-5aac-ba55-b0f8ffc899b9","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Berker Banar","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/banarberker.html","acadposition":"PhD Student","blurb":"Towards Composing Contemporary Classical Music using Generative Deep Learning","themes":["mir","soundsynthesis"]}},{"id":"66b87075-3456-5b0c-9dfe-f91b43204096","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Brendan O'Connor","role":"PhD","url":"https://trebolium.github.io/","acadposition":"PhD Student","blurb":"Singing Voice Attribute Transformation","themes":["soundsynthesis","audioeng","mir"]}},{"id":"34ef929b-de59-5c95-a554-597f98d80226","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Chin-Yun Yu","role":"PhD","url":"https://yoyololicon.github.io/","acadposition":"PhD Student","blurb":"Neural Audio Synthesis with Expressiveness Control","themes":["audioeng","mir","soundsynthesis"]}},{"id":"612a6483-8966-5db8-9744-f6fd1021a11a","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"David Südholt","role":"PhD","url":"https://dsuedholt.github.io/","acadposition":"PhD Student","blurb":"Machine Learning of Physical Models for Voice Synthesis","themes":["soundsynthesis","audioeng"]}},{"id":"2b006670-df63-567c-9c51-62d19c6b1372","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Eleanor Row","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/roweleanorroxannevictoria.html","acadposition":"PhD Student","blurb":"Automatic micro-composition for professional/novice composers using generative models as creativity support tools","themes":["soundsynthesis"]}},{"id":"ca2575a9-c402-50c9-a660-eea9648428a6","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8d8f8","images":{"fallback":{"src":"/static/e3f990a7d5022b64b25e5c46e4a6d5bf/47930/joshuadreiss.jpg","srcSet":"/static/e3f990a7d5022b64b25e5c46e4a6d5bf/e07e1/joshuadreiss.jpg 100w,\n/static/e3f990a7d5022b64b25e5c46e4a6d5bf/dd515/joshuadreiss.jpg 200w,\n/static/e3f990a7d5022b64b25e5c46e4a6d5bf/47930/joshuadreiss.jpg 400w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/e3f990a7d5022b64b25e5c46e4a6d5bf/d8057/joshuadreiss.webp 100w,\n/static/e3f990a7d5022b64b25e5c46e4a6d5bf/2e34e/joshuadreiss.webp 200w,\n/static/e3f990a7d5022b64b25e5c46e4a6d5bf/416c3/joshuadreiss.webp 400w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":400}}},"name":"Prof. Joshua D Reiss","role":"Academic","url":"http://www.eecs.qmul.ac.uk/~josh/","acadposition":"Professor of Audio Engineering","blurb":"sound engineering, intelligent audio production, sound synthesis, audio effects, automatic mixing","themes":["audioeng","soundsynthesis"]}},{"id":"96844f85-b589-5907-be07-6cc39213f206","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Marco Comunità","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/comunitamarco.html","acadposition":"PhD Student","blurb":"Machine learning applied to sound synthesis models","themes":["audioeng","soundsynthesis"]}},{"id":"c5ddd5bb-999c-511e-91f9-f17ae20473e7","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Max Graf","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/grafmax.html","acadposition":"PhD Student","blurb":"PERFORM-AI (Provide Extended Realities for Musical Performance using AI)","themes":["augmi","isam","soundsynthesis"]}},{"id":"de8a6a9d-d9db-5d6f-aa49-d6383eab8ec3","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Ningzhi Wang","role":"PhD","url":"","acadposition":"PhD Student","blurb":"Generative Models For Music Audio Representation And Understanding","themes":["mir","mcog","soundsynthesis"]}},{"id":"72d3c05f-c5ce-5fe9-9c95-733813b814f7","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Shubhr Singh","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/singhshubhr.html","acadposition":"PhD Student","blurb":"Audio Applications of Novel Mathematical Methods in Deep Learning","themes":["soundsynthesis","mlist"]}},{"id":"b699de08-bbee-5b10-bd93-9643604e2681","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Yin-Jyun Luo","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/luoyin-jyun.html","acadposition":"PhD Student","blurb":"Industry-scale Machine Listening for Music and Audio Data","themes":["soundsynthesis","mlist"]}}]}}}
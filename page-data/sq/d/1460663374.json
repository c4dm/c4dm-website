{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/Rcbs4NvMFHM","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/cdtdata"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080838","images":{"fallback":{"src":"/static/bd2eecf83bf5392a4ac7061919a52914/cd63e/cdtdata.png","srcSet":"/static/bd2eecf83bf5392a4ac7061919a52914/08744/cdtdata.png 145w,\n/static/bd2eecf83bf5392a4ac7061919a52914/3c29b/cdtdata.png 290w,\n/static/bd2eecf83bf5392a4ac7061919a52914/cd63e/cdtdata.png 579w","sizes":"(min-width: 579px) 579px, 100vw"},"sources":[{"srcSet":"/static/bd2eecf83bf5392a4ac7061919a52914/72079/cdtdata.webp 145w,\n/static/bd2eecf83bf5392a4ac7061919a52914/a7f7a/cdtdata.webp 290w,\n/static/bd2eecf83bf5392a4ac7061919a52914/ba6fb/cdtdata.webp 579w","type":"image/webp","sizes":"(min-width: 579px) 579px, 100vw"}]},"width":579,"height":579}}},"title":"Centre for Doctoral Training (CDT) in Data Centric Engineering","author":"Prof Eram Rizvi (PI), Prof Mark Sandler (CI), Prof Nick Bryan-Kinns (CI)","date":null},"html":"","id":"e68ade9f-702f-5695-9a7d-e2ab94af8c72"},{"fields":{"slug":"/projects/Fazekas-Sony"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"New Methodologies for Efficient and Controllable Music Generation","author":"Dr George Fazekas (PI)","date":null},"html":"","id":"a3e9f3a1-9edd-50fe-b941-283fe6f7de85"},{"fields":{"slug":"/projects/Fazekas-UMG2"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Beyond Supervised Learning for Musical Audio","author":"Dr George Fazekas (PI)","date":null},"html":"","id":"433744e4-ecdf-538d-903d-85748cab2ea6"},{"fields":{"slug":"/projects/aimcdt"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png","srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/4b686/aimcdt.png 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/d2213/aimcdt.png 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png 615w","sizes":"(min-width: 615px) 615px, 100vw"},"sources":[{"srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/b2942/aimcdt.webp 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/46581/aimcdt.webp 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/fa942/aimcdt.webp 615w","type":"image/webp","sizes":"(min-width: 615px) 615px, 100vw"}]},"width":615,"height":615}}},"title":"UKRI Centre for Doctoral Training in Artificial Intelligence and Music (AIM)","author":"Prof Simon Dixon (PI), Dr Mathieu Barthet (CI), Dr Nick Bryan-Kinns (CI), Dr Gyorgy Fazekas (CI), Prof Mark Sandler (CI), Dr Andrew McPherson (CI), Dr Emmanouil Benetos (CI)","date":null},"html":"","id":"237af1c8-14dc-5167-a5e1-fa55e6860dea"},{"fields":{"slug":"/projects/Dixon-DAACI"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Performance Rendering for Music Generation Systems","author":"Prof Simon Dixon (PI)","date":null},"html":"","id":"780d9bfc-fda8-5320-aa0c-0a93788ae549"},{"fields":{"slug":"/projects/ZBenetos-Spotify"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Style classification of podcasts using audio","author":"Dr Emmanouil Benetos (PI)","date":null},"html":"","id":"19d1254c-f423-52ab-ae78-51e201f32ab6"}]},"news":{"nodes":[{"fields":{"slug":"/news/2024-06-24.C4DM-academic_at_AI_chamber_music_symposium"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/0f7df906e23f5363c766cfa5e0df523a/f4df4/ccdd-event-poster.png","srcSet":"/static/0f7df906e23f5363c766cfa5e0df523a/b4532/ccdd-event-poster.png 281w,\n/static/0f7df906e23f5363c766cfa5e0df523a/31998/ccdd-event-poster.png 563w,\n/static/0f7df906e23f5363c766cfa5e0df523a/f4df4/ccdd-event-poster.png 1125w","sizes":"(min-width: 1125px) 1125px, 100vw"},"sources":[{"srcSet":"/static/0f7df906e23f5363c766cfa5e0df523a/a37a7/ccdd-event-poster.webp 281w,\n/static/0f7df906e23f5363c766cfa5e0df523a/0fbb3/ccdd-event-poster.webp 563w,\n/static/0f7df906e23f5363c766cfa5e0df523a/94e11/ccdd-event-poster.webp 1125w","type":"image/webp","sizes":"(min-width: 1125px) 1125px, 100vw"}]},"width":1125,"height":1125}}},"title":"C4DM academic at AI & Chamber Music Symposium","author":"Admin","date":"Mon 24 Jun 2024"},"html":"<p>On 24 June, C4DM academic Johan Pauwels will give a talk on \"Opportunities Unleashed by AI for Music\" at <a href=\"https://ilcs.sas.ac.uk/events/critical-creative-digital-dynamics-a-symposium-ai-digital-innovations-inter-art-chamber-0\">Critical &#x26; Creative Digital Dynamics: A Symposium on AI &#x26; Digital Innovations for Inter-art Chamber Music\nPractices</a>. The\nevent is hosted by the <a href=\"https://ilcs.sas.ac.uk/\">Institute of Languages, Cultures and Societies</a> at the <a href=\"https://ilcs.sas.ac.uk/\">School of Advanced Study, University of London</a>. It will take place between 10am and 7pm at <a href=\"https://www.openstreetmap.org/#map=16/51.5210/-0.1314\">Senate\nHouse Library, Malet Street, WC1E 7HU London</a>.</p>","id":"8c16cf0d-e7fa-5607-99b6-c04571ecbefb"},{"fields":{"slug":"/news/news.2024-06-21.C4DM- Ilyass_Moummad"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar:  Ilyass Moummad","author":"Admin","date":"Fri 21 Jun 2024"},"html":"<h3>C4DM Seminar: Ilyass Moummad: Self-Supervised Invariant Learning of Bird Sound Representations</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nIlyass Moummad</p>\n<p><strong>Date/time:  Friday, 21st June 2024, 2pm</strong></p>\n<p>**Location: G2, ENG, Mile End Campus, QMUL, E1 4NS **\nZoom: <a href=\"https://qmul-ac-uk.zoom.us/j/9798452959\">https://qmul-ac-uk.zoom.us/j/9798452959</a></p>\n<p><b>Title</b>: Self-Supervised Invariant Learning of Bird Sound Representations</p>\n<hr>\n<p><b>Abstract</b>: Abstract: Self-supervised learning (SSL) involves the learning of data representations without any manual annotation. It consists of solving a pretext task relevant for learning informative data representations, which can be used for transfer learning to solve downstream tasks. Among the different learning paradigms, the most successful in learning discriminative features for classification tasks are “Invariant Learning” methods. They train the model to be insensitive to pre-defined transformations (e.g. if pitch shift is used as a transformation, the model is shown two versions of the same signal with different pitch shifts and is trained to output the same representation for both versions). The choice of data transformations for learning invariance is crucial and depends on the data domain and its relevance to downstream tasks.\nIn bioacoustics, it is not yet clear which data transformations the model should be robust to. In this work, we show that simple and domain-agnostic data augmentations (which do not use any prior knowledge of the nature of bioacoustic sounds) can learn robust and informative features. We evaluate the learned representations through transfer learning to downstream tasks with different challenges such as novel classes (downstream datasets can have classes never seen during pretraining), few-shot (very few annotations given for a downstream), label shift (evaluation recordings come from different geographical regions), and covariate shift (difference in recording settings and environmental conditions of the same classes between pretraining and evaluation data).</p>\n<p><b>Bio</b>: Ilyass Moummad is a PhD student (December 2021 - November 2024) at IMT Atlantique, Brest, France. He works under the supervision of Nicolas Farrugia and is co-supervised by Romain Serizel. Currently, Ilyass is a Visiting Researcher at C4DM, QMUL, working under the supervision of Emmanouil Benetos (April - June 2024). Ilyass’s PhD topic is Deep Learning for Bioacoustics, with an interest in representation learning (both self-supervised and supervised) of animal sounds, as well as few-shot learning (species sound classification and detection from very few annotated examples).</p>","id":"eff5e87a-d691-5412-a7b5-cde72a465f19"},{"fields":{"slug":"/news/2024-06-20.C4DM-at_ACM_CC_2024"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/378b54314e1a61271fd1713f1ec7a3fc/cb5b3/cc24_logo_new.png","srcSet":"/static/378b54314e1a61271fd1713f1ec7a3fc/2655f/cc24_logo_new.png 318w,\n/static/378b54314e1a61271fd1713f1ec7a3fc/be4dc/cc24_logo_new.png 636w,\n/static/378b54314e1a61271fd1713f1ec7a3fc/cb5b3/cc24_logo_new.png 1271w","sizes":"(min-width: 1271px) 1271px, 100vw"},"sources":[{"srcSet":"/static/378b54314e1a61271fd1713f1ec7a3fc/4ce64/cc24_logo_new.webp 318w,\n/static/378b54314e1a61271fd1713f1ec7a3fc/1852a/cc24_logo_new.webp 636w,\n/static/378b54314e1a61271fd1713f1ec7a3fc/7e90c/cc24_logo_new.webp 1271w","type":"image/webp","sizes":"(min-width: 1271px) 1271px, 100vw"}]},"width":1271,"height":1271}}},"title":"C4DM at ACM Creativity & Cognition 2024","author":"Corey Ford","date":"Thu 20 Jun 2024"},"html":"<p>On 23rd-26th June, several C4DM researchers will participate in the <a href=\"https://cc.acm.org/2024/\">16th ACM Conference on Creativity and Cognition 2024 (C&#x26;C 2024)</a>. C&#x26;C is a leading international conference that brings together researchers and practitioners from various disciplines to explore technologies wide impacts on creativity, from designing and working with AI tools, to the social and cultural aspects of creativity.</p>\n<p>The following paper received an <b>honorable mention award</b> at the conference:</p>\n<ul>\n<li><a href=\"https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/97327/Ford%20Reflection%20Across%20AI-based%202024%20Accepted.pdf?sequence=2&#x26;isAllowed=y\">Reflection Across AI-based Music Composition</a>, by Corey Ford, Ashley Noel-Hirst, Sara Cardinale, Jackson Loth, Pedro Sarmento, Elizabeth Wilson, Lewis Wolstanholme, Kyle Worrall and Nick Bryan-Kinns.</li>\n</ul>\n<p>The paper collects first-person accounts, interview and questionnaire measures, on how several C4DM researchers used AI tools of their choice in their music making, contributing descriptions of how they reflected when using AI generated content.</p>\n<p>C4DM members <a href=\"https://ashleynoelhirst.co.uk/\">Ashley Noel-Hirst</a> and <a href=\"http://codetta.codes/\">Corey Ford</a> are also co-authors of the paper:</p>\n<ul>\n<li><a href=\"https://ualresearchonline.arts.ac.uk/id/eprint/22055/1/incongrous_unmarked.pdf\">Using Incongruous Genres to Explore Music Making with AI Generated Content</a>, by Nick Bryan-Kinns, Ashley Noel-Hirst, and Corey Ford.</li>\n</ul>\n<p>The paper explores how an AI tool trained on a Folk dataset is used and appropriated by musicians in the genres of both, as a playful way to explore Human-AI Interaction.</p>\n<p>The <a href=\"https://xaixarts.github.io/2024\">2nd international workshop on eXplainable AI for the Arts</a> is also run at the conference, co-organized by C4DM members Corey Ford and Shuoyang Zheng. This workshop examines the challenges and opportunities at the intersection of explainable AI and the Arts, offering a critical view on the explainable aspects of Responsible AI and Human-Centred AI. With an accepted paper from C4DM of:</p>\n<ul>\n<li>A Mapping Strategy for Interacting with Latent Audio Synthesis Using Artistic Materials by Shuoyang Zheng, Anna Xambó Sedó and Nick Bryan-Kinns.</li>\n</ul>\n<p>The paper describes how mapping sketches to the latent space of the audio synthesis RAVE model can be used to support temporal and cross-modal aspects of explainable AI. See this paper alongside other proceedings at <a href=\"https://xaixarts.github.io/2024\">https://xaixarts.github.io/2024</a></p>\n<p><a href=\"http://codetta.codes/\">Corey Ford</a> is also on the organising committee as a chair for Student Volunteers.</p>\n<p>We hope to see you all at ACM C&#x26;C!</p>","id":"eeac8eee-d488-5a11-b34d-f7a8a8dd2ad0"},{"fields":{"slug":"/news/news.2024-06-18.C4DM-Mikolaj_Kegler"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Mikolaj Kegler","author":"Admin","date":"Tue 18 Jun 2024"},"html":"<h3>C4DM Seminar: Mikolaj Kegler: Hear What You Want, towards seamless, immersive AI experiences for wearable audio devices</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nMikolaj Kegler</p>\n<p><strong>Date/time:  Tuesday, 18th June 2024, 3pm</strong></p>\n<p>**Location: GC205, Greduate Centre, QMUL, E1 4NS **\nZoom: <a href=\"https://qmul-ac-uk.zoom.us/j/9798452959\">https://qmul-ac-uk.zoom.us/j/9798452959</a></p>\n<p><b>Title</b>: Hear What You Want, towards seamless, immersive AI experiences for wearable audio devices</p>\n<hr>\n<p><b>Abstract</b>: Hear What You Want is both the mission statement and the title of the Bose Research franchise established to build AI-powered experiences for next-generation wearable audio devices. This interdisciplinary franchise involves a wide range of research tracks, such as source separation and sound event detection, to name a few. All these efforts share one common denominator, the resulting solutions must be suitable for low-latency execution on the embedded target devices to enable seamless, immersive experiences. In this talk, I will start by outlining the fundamental problem of limited computational resources on embedded platforms and present our latest methods for building compact yet performant ML models. Subsequently, I will discuss the problem of low-latency causal target source extraction and highlight our recent developments. I will conclude by presenting our latest work on improving the efficiency and fidelity of text-toaudio generative models for Foley sound synthesis.</p>\n<p><b>Bio</b>: Mikolaj “Miko” Kegler is an Audio Machine Learning Scientist at Bose Corporation. His research at the interface of ML and DSP is focused on developing methods for lightweight, low-latency, on-device speech and audio signal processing. Before joining Bose, Miko was awarded a PhD from the Department of Bioengineering &#x26; Centre for Neurotechnology, Imperial College London, where he has been investigating neural mechanisms underlying perception and comprehension of speech, especially in challenging listening conditions. During his PhD, he completed multiple audio ML research placements, including at Amazon Lab126 and Logitech, where he also served as a long-term scientific consultant.</p>\n<p><b>Presentation</b>:\n<a href=\"https://drive.google.com/file/d/1nR0uuj8AEb9YF7XbUqL3ixsAPgI6Eh-T/view?usp=sharing\">[PDF Slides]</a>\n<a href=\"https://www.youtube.com/watch?v=tUi4ER2dXjQ\">[Video Recording]</a></p>","id":"e286df2d-7792-5cdd-b6ac-2810abc3f1d0"},{"fields":{"slug":"/news/news.2024-06-17.C4DM-Yinghao_Ma"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar:  Yinghao Ma","author":"Admin","date":"Mon 17 Jun 2024"},"html":"<h3>C4DM Seminar: Yinghao Ma: The Impact of Cutting-edge AI Technologies to Music Industry</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nYinghao Ma</p>\n<p><strong>Date/time:  Monday, 17th June 2024, 2:15pm</strong></p>\n<p>**Location: G2, ENG, Mile End campus, QMUL, E1 4NS **\nZoom: <a href=\"https://qmul-ac-uk.zoom.us/j/9798452959\">https://qmul-ac-uk.zoom.us/j/9798452959</a></p>\n<p><b>Title</b>: The Impact of Cutting-edge AI Technologies to Music Industry</p>\n<hr>\n<p><b>Abstract</b>: Abstract: This is a rehearsal of the invitation presentation to the Whitgift School for its teachers and Independent Directors.  In this presentation, I will introduce cutting-edge AI technologies including Large Language Models (LLMs) and Latent Diffusion Models (LDMs) and their application to music understanding and music generation. We will briefly discuss the industrial applications and ethical concerns in the music industry and music education such as deepfake and copyright issues etc.</p>\n<p><b>Bio</b>: MA Yinghao (马英浩) is a Ph.D. student in Artificial Intelligence and Music (AIM) program at Centre for Digital Music (C4DM), School of EECS, Queen Mary University of London, supervised by Dr. Emmanouil Benetos, Dr. Chris Donahue (secondary), and Prof. Simon Dixon (independent assessor). He is one of the co-founders of the Multimodal Art Projection (MAP) community. Together with his colleague, he proposed an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), with more than 50k downloads on the Huggingfac page, and established a Music Audio Representation Benchmark for universaL Evaluation (MRABLE). He is also interested in music-related multimodality and developed MusiLingo, a music captioning and query response model based on the alignment of single-modality pre-trained models.</p>","id":"1a01c73f-cdc6-5ead-a847-0430b4e2b2aa"},{"fields":{"slug":"/news/2024-14-06.C4DM-at_SMC_2024"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#083858","images":{"fallback":{"src":"/static/0b07bafbd3ec51947ac07a1f22fe3e1d/f03da/SMC2024.png","srcSet":"/static/0b07bafbd3ec51947ac07a1f22fe3e1d/208b0/SMC2024.png 746w,\n/static/0b07bafbd3ec51947ac07a1f22fe3e1d/5ff0b/SMC2024.png 1491w,\n/static/0b07bafbd3ec51947ac07a1f22fe3e1d/f03da/SMC2024.png 2982w","sizes":"(min-width: 2982px) 2982px, 100vw"},"sources":[{"srcSet":"/static/0b07bafbd3ec51947ac07a1f22fe3e1d/47834/SMC2024.webp 746w,\n/static/0b07bafbd3ec51947ac07a1f22fe3e1d/7e32e/SMC2024.webp 1491w,\n/static/0b07bafbd3ec51947ac07a1f22fe3e1d/bb5d7/SMC2024.webp 2982w","type":"image/webp","sizes":"(min-width: 2982px) 2982px, 100vw"}]},"width":2982,"height":2982}}},"title":"C4DM at SMC 2024","author":"Ashley Noel-Hirst","date":"Sat 15 Jun 2024"},"html":"<p>On 4–6th July, several C4DM researchers will participate in the <b><a href=\"https://smcnetwork.org/smc2024/#programme\">2024 conference on Sound and Music computing (SMC 2024)</a></b> in Porto, Portugal. With this years theme being 'immersive', the conference brings together interdisciplinary work from composers, scientists and other researchers tackling engagement with digital sound and music.</p>\n<p>The Centre for Digital Music will be present with work ranging from piano transcription to latent audio models. We look forward to sharing the below papers, authored or co-authored by C4DM members:</p>\n<ul>\n<li>\n<p>A Generative Framework for Composition-aware Loop Recommendation In Music Production: Drum2Bass Use Case, by Xiaowan Yi and Mathieu Barthet</p>\n</li>\n<li>\n<p>Simulating Piano Performance Mistakes for Music Learning Context, by Alia Morsi (Universitat Pompeu Fabra), Huan Zhang, Akira Maezawa (Yamaha Corporation), Simon Dixon and Xavier Serra (Universitat Pompeu Fabra)</p>\n</li>\n<li>\n<p>Temporal Analysis of Emotion Perception in Film Music: Insights from the FME-24 Dataset, by Ruby O.N Crocker and George Fazekas</p>\n</li>\n<li>\n<p><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/97438#\">Reconstructing the Charlie Parker Omnibook using an audio-to-score automatic transcription pipeline</a>, by Xavier Riley and Simon Dixon</p>\n</li>\n<li>\n<p>A Sonification Method for Monitoring Chemical Sensor Data, by Yutian Hu, Tony Stockman, Aleksandar Radu, Ernesto Saiz, Nick Bryan-Kinns</p>\n</li>\n</ul>","id":"63fee073-5152-5714-8abf-c3f3ceef055a"}]}}}
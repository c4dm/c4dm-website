{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/Rcbs4NvMFHM","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/cdtdata"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080838","images":{"fallback":{"src":"/static/bd2eecf83bf5392a4ac7061919a52914/cd63e/cdtdata.png","srcSet":"/static/bd2eecf83bf5392a4ac7061919a52914/08744/cdtdata.png 145w,\n/static/bd2eecf83bf5392a4ac7061919a52914/3c29b/cdtdata.png 290w,\n/static/bd2eecf83bf5392a4ac7061919a52914/cd63e/cdtdata.png 579w","sizes":"(min-width: 579px) 579px, 100vw"},"sources":[{"srcSet":"/static/bd2eecf83bf5392a4ac7061919a52914/72079/cdtdata.webp 145w,\n/static/bd2eecf83bf5392a4ac7061919a52914/a7f7a/cdtdata.webp 290w,\n/static/bd2eecf83bf5392a4ac7061919a52914/ba6fb/cdtdata.webp 579w","type":"image/webp","sizes":"(min-width: 579px) 579px, 100vw"}]},"width":579,"height":579}}},"title":"Centre for Doctoral Training (CDT) in Data Centric Engineering","author":"Prof Eram Rizvi (PI), Prof Mark Sandler (CI), Prof Nick Bryan-Kinns (CI)","date":null},"html":"","id":"e68ade9f-702f-5695-9a7d-e2ab94af8c72"},{"fields":{"slug":"/projects/Fazekas-Sony"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"New Methodologies for Efficient and Controllable Music Generation","author":"Dr George Fazekas (PI)","date":null},"html":"","id":"a3e9f3a1-9edd-50fe-b941-283fe6f7de85"},{"fields":{"slug":"/projects/Fazekas-UMG2"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Beyond Supervised Learning for Musical Audio","author":"Dr George Fazekas (PI)","date":null},"html":"","id":"433744e4-ecdf-538d-903d-85748cab2ea6"},{"fields":{"slug":"/projects/aimcdt"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png","srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/4b686/aimcdt.png 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/d2213/aimcdt.png 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png 615w","sizes":"(min-width: 615px) 615px, 100vw"},"sources":[{"srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/b2942/aimcdt.webp 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/46581/aimcdt.webp 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/fa942/aimcdt.webp 615w","type":"image/webp","sizes":"(min-width: 615px) 615px, 100vw"}]},"width":615,"height":615}}},"title":"UKRI Centre for Doctoral Training in Artificial Intelligence and Music (AIM)","author":"Prof Simon Dixon (PI), Dr Mathieu Barthet (CI), Dr Nick Bryan-Kinns (CI), Dr Gyorgy Fazekas (CI), Prof Mark Sandler (CI), Dr Andrew McPherson (CI), Dr Emmanouil Benetos (CI)","date":null},"html":"","id":"237af1c8-14dc-5167-a5e1-fa55e6860dea"},{"fields":{"slug":"/projects/Dixon-DAACI"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Performance Rendering for Music Generation Systems","author":"Prof Simon Dixon (PI)","date":null},"html":"","id":"780d9bfc-fda8-5320-aa0c-0a93788ae549"},{"fields":{"slug":"/projects/ZBenetos-Spotify"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Style classification of podcasts using audio","author":"Dr Emmanouil Benetos (PI)","date":null},"html":"","id":"19d1254c-f423-52ab-ae78-51e201f32ab6"}]},"news":{"nodes":[{"fields":{"slug":"/news/news.2024-06-21.C4DM- Ilyass_Moummad"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar:  Ilyass Moummad","author":"Admin","date":"Fri 21 Jun 2024"},"html":"<h3>C4DM Seminar: Ilyass Moummad: Self-Supervised Invariant Learning of Bird Sound Representations</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nIlyass Moummad</p>\n<p><strong>Date/time:  Friday, 21st June 2024, 2pm</strong></p>\n<p>**Location: G2, ENG, Mile End Campus, QMUL, E1 4NS **\nZoom: <a href=\"https://qmul-ac-uk.zoom.us/j/9798452959\">https://qmul-ac-uk.zoom.us/j/9798452959</a></p>\n<p><b>Title</b>: Self-Supervised Invariant Learning of Bird Sound Representations</p>\n<hr>\n<p><b>Abstract</b>: Abstract: Self-supervised learning (SSL) involves the learning of data representations without any manual annotation. It consists of solving a pretext task relevant for learning informative data representations, which can be used for transfer learning to solve downstream tasks. Among the different learning paradigms, the most successful in learning discriminative features for classification tasks are “Invariant Learning” methods. They train the model to be insensitive to pre-defined transformations (e.g. if pitch shift is used as a transformation, the model is shown two versions of the same signal with different pitch shifts and is trained to output the same representation for both versions). The choice of data transformations for learning invariance is crucial and depends on the data domain and its relevance to downstream tasks.\nIn bioacoustics, it is not yet clear which data transformations the model should be robust to. In this work, we show that simple and domain-agnostic data augmentations (which do not use any prior knowledge of the nature of bioacoustic sounds) can learn robust and informative features. We evaluate the learned representations through transfer learning to downstream tasks with different challenges such as novel classes (downstream datasets can have classes never seen during pretraining), few-shot (very few annotations given for a downstream), label shift (evaluation recordings come from different geographical regions), and covariate shift (difference in recording settings and environmental conditions of the same classes between pretraining and evaluation data).</p>\n<p><b>Bio</b>: Ilyass Moummad is a PhD student (December 2021 - November 2024) at IMT Atlantique, Brest, France. He works under the supervision of Nicolas Farrugia and is co-supervised by Romain Serizel. Currently, Ilyass is a Visiting Researcher at C4DM, QMUL, working under the supervision of Emmanouil Benetos (April - June 2024). Ilyass’s PhD topic is Deep Learning for Bioacoustics, with an interest in representation learning (both self-supervised and supervised) of animal sounds, as well as few-shot learning (species sound classification and detection from very few annotated examples).</p>","id":"eff5e87a-d691-5412-a7b5-cde72a465f19"},{"fields":{"slug":"/news/news.2024-06-18.C4DM-Mikolaj_Kegler"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Mikolaj Kegler","author":"Admin","date":"Tue 18 Jun 2024"},"html":"<h3>C4DM Seminar: Mikolaj Kegler: Hear What You Want, towards seamless, immersive AI experiences for wearable audio devices</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nMikolaj Kegler</p>\n<p><strong>Date/time:  Tuesday, 18th June 2024, 3pm</strong></p>\n<p>**Location: GC205, Greduate Centre, QMUL, E1 4NS **\nZoom: <a href=\"https://qmul-ac-uk.zoom.us/j/9798452959\">https://qmul-ac-uk.zoom.us/j/9798452959</a></p>\n<p><b>Title</b>: Hear What You Want, towards seamless, immersive AI experiences for wearable audio devices</p>\n<hr>\n<p><b>Abstract</b>: Hear What You Want is both the mission statement and the title of the Bose Research franchise established to build AI-powered experiences for next-generation wearable audio devices. This interdisciplinary franchise involves a wide range of research tracks, such as source separation and sound event detection, to name a few. All these efforts share one common denominator, the resulting solutions must be suitable for low-latency execution on the embedded target devices to enable seamless, immersive experiences. In this talk, I will start by outlining the fundamental problem of limited computational resources on embedded platforms and present our latest methods for building compact yet performant ML models. Subsequently, I will discuss the problem of low-latency causal target source extraction and highlight our recent developments. I will conclude by presenting our latest work on improving the efficiency and fidelity of text-toaudio generative models for Foley sound synthesis.</p>\n<p><b>Bio</b>: Mikolaj “Miko” Kegler is an Audio Machine Learning Scientist at Bose Corporation. His research at the interface of ML and DSP is focused on developing methods for lightweight, low-latency, on-device speech and audio signal processing. Before joining Bose, Miko was awarded a PhD from the Department of Bioengineering &#x26; Centre for Neurotechnology, Imperial College London, where he has been investigating neural mechanisms underlying perception and comprehension of speech, especially in challenging listening conditions. During his PhD, he completed multiple audio ML research placements, including at Amazon Lab126 and Logitech, where he also served as a long-term scientific consultant.</p>","id":"e286df2d-7792-5cdd-b6ac-2810abc3f1d0"},{"fields":{"slug":"/news/2024-14-06.C4DM-at_SMC_2024"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#083858","images":{"fallback":{"src":"/static/0b07bafbd3ec51947ac07a1f22fe3e1d/f03da/SMC2024.png","srcSet":"/static/0b07bafbd3ec51947ac07a1f22fe3e1d/208b0/SMC2024.png 746w,\n/static/0b07bafbd3ec51947ac07a1f22fe3e1d/5ff0b/SMC2024.png 1491w,\n/static/0b07bafbd3ec51947ac07a1f22fe3e1d/f03da/SMC2024.png 2982w","sizes":"(min-width: 2982px) 2982px, 100vw"},"sources":[{"srcSet":"/static/0b07bafbd3ec51947ac07a1f22fe3e1d/47834/SMC2024.webp 746w,\n/static/0b07bafbd3ec51947ac07a1f22fe3e1d/7e32e/SMC2024.webp 1491w,\n/static/0b07bafbd3ec51947ac07a1f22fe3e1d/bb5d7/SMC2024.webp 2982w","type":"image/webp","sizes":"(min-width: 2982px) 2982px, 100vw"}]},"width":2982,"height":2982}}},"title":"C4DM at SMC 2024","author":"Ashley Noel-Hirst","date":"Sat 15 Jun 2024"},"html":"<p>On 4–6th July, several C4DM researchers will participate in the <b><a href=\"https://smcnetwork.org/smc2024/#programme\">2024 conference on Sound and Music computing (SMC 2024)</a></b> in Porto, Portugal. With this years theme being 'immersive', the conference brings together interdisciplinary work from composers, scientists and other researchers tackling engagement with digital sound and music.</p>\n<p>The Centre for Digital Music will be present with work ranging from piano transcription to latent audio models. We look forward to sharing the below papers, authored or co-authored by C4DM members:</p>\n<ul>\n<li>\n<p>A Generative Framework for Composition-aware Loop Recommendation In Music Production: Drum2Bass Use Case, by Xiaowan Yi and Mathieu Barthet</p>\n</li>\n<li>\n<p>Simulating Piano Performance Mistakes for Music Learning Context, by Alia Morsi (Universitat Pompeu Fabra), Huan Zhang, Akira Maezawa (Yamaha Corporation), Simon Dixon and Xavier Serra (Universitat Pompeu Fabra)</p>\n</li>\n<li>\n<p>Temporal Analysis of Emotion Perception in Film Music: Insights from the FME-24 Dataset, by Ruby O.N Crocker and George Fazekas</p>\n</li>\n<li>\n<p><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/97438#\">Reconstructing the Charlie Parker Omnibook using an audio-to-score automatic transcription pipeline</a>, by Xavier Riley and Simon Dixon</p>\n</li>\n<li>\n<p>A Sonification Method for Monitoring Chemical Sensor Data, by Yutian Hum, Tony Stockman, Aleksandar Radu, Ernesto Saiz, Nick Bryan-Kinns</p>\n</li>\n</ul>","id":"63fee073-5152-5714-8abf-c3f3ceef055a"},{"fields":{"slug":"/news/2024-06-13.C4DM-organises_and_participates_at_AES_symposium"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#183848","images":{"fallback":{"src":"/static/e985701b87726741bf3a416eeecfbebd/b9792/AES_AiMusician.jpg","srcSet":"/static/e985701b87726741bf3a416eeecfbebd/845e4/AES_AiMusician.jpg 192w,\n/static/e985701b87726741bf3a416eeecfbebd/c2c05/AES_AiMusician.jpg 384w,\n/static/e985701b87726741bf3a416eeecfbebd/b9792/AES_AiMusician.jpg 768w","sizes":"(min-width: 768px) 768px, 100vw"},"sources":[{"srcSet":"/static/e985701b87726741bf3a416eeecfbebd/694c3/AES_AiMusician.webp 192w,\n/static/e985701b87726741bf3a416eeecfbebd/6d535/AES_AiMusician.webp 384w,\n/static/e985701b87726741bf3a416eeecfbebd/482be/AES_AiMusician.webp 768w","type":"image/webp","sizes":"(min-width: 768px) 768px, 100vw"}]},"width":768,"height":768}}},"title":"C4DM students organise and participate at the AES International Symposium on AI and the Musician","author":"Emmanouil Benetos","date":"Thu 13 Jun 2024"},"html":"<p>Centre for Digital Music PhD students both organised and presented at the <a href=\"https://aes2.org/events-calendar/aes-international-symposium-on-ai-and-the-musician/\">AES International Symposium on AI and the Musician</a>, which took place on 6-8 June 2024 at the Berklee College of Music, Boston, MA, USA.</p>\n<p>C4DM PhD student <a href=\"https://www.christiansteinmetz.com/\">Christian Steinmetz</a> was both Papers Chair and Workshops Chair for the symposium, and was instrumental in organising a <a href=\"https://www.linkedin.com/posts/akoretzky_music-musicproduction-audio-activity-7206806041240494080-D8kh/\">successful event</a>.</p>\n<p>The following works were authored/coauthored by C4DM PhD students and academic staff:</p>\n<ul>\n<li>\n<p><a href=\"https://aes2024aisymposiumon.sched.com/event/1e14l/deep-learning-based-audio-representations-for-the-analysis-and-visualisation-of-electronic-dance-music-dj-mixes\">Deep learning-based audio representations for the analysis and visualisation of electronic dance music DJ mixes</a>, by Alexander Williams, Haokun Tian, Stefan Latter, Mathieu Barthet, and Charalampos Saitis</p>\n</li>\n<li>\n<p><a href=\"https://aes2024aisymposiumon.sched.com/event/1e14x/advancing-ai-in-music-composition-refining-the-generative-music-overpainting-task\">Advancing AI in music composition: refining the generative music overpainting task</a>, by Eleanor Row and George Fazekas</p>\n</li>\n<li>\n<p><a href=\"https://aes2024aisymposiumon.sched.com/event/1e150/when-xr-meets-ai-integrating-interactive-machine-learning-with-an-xr-musical-instrument\">When XR meets AI: Integrating interactive machine learning with an XR musical instrument</a>, by Max Graf and Mathieu Barthet</p>\n</li>\n</ul>","id":"6cb7c1cc-3a91-544d-9866-c0be95ea8fb8"},{"fields":{"slug":"/news/news.2024-06-13.C4DM-research_pitched_at_QMI_showcase"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/d9368b814c6575ee52e942a469298264/baaed/QMI-showcase.jpg","srcSet":"/static/d9368b814c6575ee52e942a469298264/dd515/QMI-showcase.jpg 200w,\n/static/d9368b814c6575ee52e942a469298264/47930/QMI-showcase.jpg 400w,\n/static/d9368b814c6575ee52e942a469298264/baaed/QMI-showcase.jpg 800w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[{"srcSet":"/static/d9368b814c6575ee52e942a469298264/2e34e/QMI-showcase.webp 200w,\n/static/d9368b814c6575ee52e942a469298264/416c3/QMI-showcase.webp 400w,\n/static/d9368b814c6575ee52e942a469298264/c1587/QMI-showcase.webp 800w","type":"image/webp","sizes":"(min-width: 800px) 800px, 100vw"}]},"width":800,"height":800}}},"title":"C4DM Spinouts pitched at QMI Showcase","author":"Emmanouil Benetos","date":"Thu 13 Jun 2024"},"html":"<p>C4DM spinouts were showcased at an event organised by <a href=\"https://qminnovation.co.uk/\">Queen Mary Innovation (QMI)</a> which took place on 7 June 2024 - all led by inspiring academic entrepreneurs developing incredible new uses for music, audio, and AI.</p>\n<p><a href=\"https://www.linkedin.com/company/roexaudio/\">RoEx</a>, founded by David Ronan, lets musicians produce professional-level mixes in minutes. By removing barriers, this technology can democratise professional hashtag#music creation, making it accessible and intuitive for all.</p>\n<p><a href=\"https://www.linkedin.com/company/nemisindo/\">Nemisindo</a>, founded by Josh Reiss, generates original sound effects in real time, creating sounds which are truly believable - rather than relying on pre-recorded effects which are never quite right.</p>\n<p>HITar, founded by <a href=\"https://www.linkedin.com/in/andrea-martelloni-7ab10a60/\">Andrea Martelloni</a>, uses real-time embedded AI to provide a rich control surface on an acoustic guitar's body for making drum and synthesiser sounds - giving the artist true freedom of expression.</p>\n<p><a href=\"https://qminnovation.co.uk/contact-us/\">Get in touch with QMI</a> and spinout leads to learn more about any of these companies and what else is in the pipeline!</p>","id":"1a8003fe-724d-5e85-a20b-3f5b8aaf8388"},{"fields":{"slug":"/news/2024-06-11.C4DM-at_KCL_Interdisciplinar_Perspectives"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#c8e828","images":{"fallback":{"src":"/static/a02775ecb3d31f0a35264ba537e3d9c9/baaed/kcl_workshop_programme.jpg","srcSet":"/static/a02775ecb3d31f0a35264ba537e3d9c9/dd515/kcl_workshop_programme.jpg 200w,\n/static/a02775ecb3d31f0a35264ba537e3d9c9/47930/kcl_workshop_programme.jpg 400w,\n/static/a02775ecb3d31f0a35264ba537e3d9c9/baaed/kcl_workshop_programme.jpg 800w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[{"srcSet":"/static/a02775ecb3d31f0a35264ba537e3d9c9/2e34e/kcl_workshop_programme.webp 200w,\n/static/a02775ecb3d31f0a35264ba537e3d9c9/416c3/kcl_workshop_programme.webp 400w,\n/static/a02775ecb3d31f0a35264ba537e3d9c9/c1587/kcl_workshop_programme.webp 800w","type":"image/webp","sizes":"(min-width: 800px) 800px, 100vw"}]},"width":800,"height":800}}},"title":"C4DM at KCL Interdisciplinary Perspectives","author":"Admin","date":"Tue 11 Jun 2024"},"html":"<p>On the 11th of June, two C4DM researchers will participate at the <b><a href=\"https://www.kcl.ac.uk/events/interdisciplinary-perspectives-bridging-sociological-studies-in-the-digital-age\">King's College Workshop on Interdisciplinary Perspectives: Bridging Sociological Studies in the Digital Age</a></b>. Organised by DDH PhD students, this event aims to foster discussions and collaborations across various disciplines, specifically including Natural Language Processing (NLP), Social Sciences, and Social Media and Digital Methods in broader research in Digital Humanities.</p>\n<p>The below presentations at the event are authored or co-authored by C4DM members:</p>\n<ul>\n<li>\n<p><b>Beyond the note: automatic detection of moral foundations in song lyrics using large language models</b> by Preniqi V., Ghinassi I., Kalimeri K., Saitis C.</p>\n</li>\n<li>\n<p><b>Towards Musically-informed Automated Discourse Analysis: A Case Study in Gender and Media</b> by Marinelli L., Saitis C.</p>\n</li>\n</ul>","id":"41f83af5-c9e6-55f5-9705-23f02adaa01d"}]}}}
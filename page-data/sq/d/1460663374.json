{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/Rcbs4NvMFHM","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/cdtdata"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080838","images":{"fallback":{"src":"/static/bd2eecf83bf5392a4ac7061919a52914/cd63e/cdtdata.png","srcSet":"/static/bd2eecf83bf5392a4ac7061919a52914/08744/cdtdata.png 145w,\n/static/bd2eecf83bf5392a4ac7061919a52914/3c29b/cdtdata.png 290w,\n/static/bd2eecf83bf5392a4ac7061919a52914/cd63e/cdtdata.png 579w","sizes":"(min-width: 579px) 579px, 100vw"},"sources":[{"srcSet":"/static/bd2eecf83bf5392a4ac7061919a52914/72079/cdtdata.webp 145w,\n/static/bd2eecf83bf5392a4ac7061919a52914/a7f7a/cdtdata.webp 290w,\n/static/bd2eecf83bf5392a4ac7061919a52914/ba6fb/cdtdata.webp 579w","type":"image/webp","sizes":"(min-width: 579px) 579px, 100vw"}]},"width":579,"height":579}}},"title":"Centre for Doctoral Training (CDT) in Data Centric Engineering","author":"Prof Eram Rizvi (PI), Prof Mark Sandler (CI), Prof Nick Bryan-Kinns (CI)","date":null},"html":"","id":"e68ade9f-702f-5695-9a7d-e2ab94af8c72"},{"fields":{"slug":"/projects/Fazekas-Sony"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"New Methodologies for Efficient and Controllable Music Generation","author":"Dr George Fazekas (PI)","date":null},"html":"","id":"a3e9f3a1-9edd-50fe-b941-283fe6f7de85"},{"fields":{"slug":"/projects/Fazekas-UMG2"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Beyond Supervised Learning for Musical Audio","author":"Dr George Fazekas (PI)","date":null},"html":"","id":"433744e4-ecdf-538d-903d-85748cab2ea6"},{"fields":{"slug":"/projects/aimcdt"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png","srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/4b686/aimcdt.png 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/d2213/aimcdt.png 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png 615w","sizes":"(min-width: 615px) 615px, 100vw"},"sources":[{"srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/b2942/aimcdt.webp 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/46581/aimcdt.webp 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/fa942/aimcdt.webp 615w","type":"image/webp","sizes":"(min-width: 615px) 615px, 100vw"}]},"width":615,"height":615}}},"title":"UKRI Centre for Doctoral Training in Artificial Intelligence and Music (AIM)","author":"Prof Simon Dixon (PI), Dr Mathieu Barthet (CI), Dr Nick Bryan-Kinns (CI), Dr Gyorgy Fazekas (CI), Prof Mark Sandler (CI), Dr Andrew McPherson (CI), Dr Emmanouil Benetos (CI)","date":null},"html":"","id":"237af1c8-14dc-5167-a5e1-fa55e6860dea"},{"fields":{"slug":"/projects/Dixon-DAACI"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Performance Rendering for Music Generation Systems","author":"Prof Simon Dixon (PI)","date":null},"html":"","id":"780d9bfc-fda8-5320-aa0c-0a93788ae549"},{"fields":{"slug":"/projects/ZBenetos-Spotify"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Style classification of podcasts using audio","author":"Dr Emmanouil Benetos (PI)","date":null},"html":"","id":"19d1254c-f423-52ab-ae78-51e201f32ab6"}]},"news":{"nodes":[{"fields":{"slug":"/news/2024-06-24.C4DM-academic_at_AI_chamber_music_symposium"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/0f7df906e23f5363c766cfa5e0df523a/f4df4/ccdd-event-poster.png","srcSet":"/static/0f7df906e23f5363c766cfa5e0df523a/b4532/ccdd-event-poster.png 281w,\n/static/0f7df906e23f5363c766cfa5e0df523a/31998/ccdd-event-poster.png 563w,\n/static/0f7df906e23f5363c766cfa5e0df523a/f4df4/ccdd-event-poster.png 1125w","sizes":"(min-width: 1125px) 1125px, 100vw"},"sources":[{"srcSet":"/static/0f7df906e23f5363c766cfa5e0df523a/a37a7/ccdd-event-poster.webp 281w,\n/static/0f7df906e23f5363c766cfa5e0df523a/0fbb3/ccdd-event-poster.webp 563w,\n/static/0f7df906e23f5363c766cfa5e0df523a/94e11/ccdd-event-poster.webp 1125w","type":"image/webp","sizes":"(min-width: 1125px) 1125px, 100vw"}]},"width":1125,"height":1125}}},"title":"C4DM academic at AI & Chamber Music Symposium","author":"Admin","date":"Mon 24 Jun 2024"},"html":"<p>On 24 June, C4DM academic Johan Pauwels will give a talk on \"Opportunities Unleashed by AI for Music\" at <a href=\"https://ilcs.sas.ac.uk/events/critical-creative-digital-dynamics-a-symposium-ai-digital-innovations-inter-art-chamber-0\">Critical &#x26; Creative Digital Dynamics: A Symposium on AI &#x26; Digital Innovations for Inter-art Chamber Music\nPractices</a>. The\nevent is hosted by the <a href=\"https://ilcs.sas.ac.uk/\">Institute of Languages, Cultures and Societies</a> at the <a href=\"https://ilcs.sas.ac.uk/\">School of Advanced Study, University of London</a>. It will take place between 10am and 7pm at <a href=\"https://www.openstreetmap.org/#map=16/51.5210/-0.1314\">Senate\nHouse Library, Malet Street, WC1E 7HU London</a>.</p>","id":"8c16cf0d-e7fa-5607-99b6-c04571ecbefb"},{"fields":{"slug":"/news/news.2024-06-21.C4DM- Ilyass_Moummad"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar:  Ilyass Moummad","author":"Admin","date":"Fri 21 Jun 2024"},"html":"<h3>C4DM Seminar: Ilyass Moummad: Self-Supervised Invariant Learning of Bird Sound Representations</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nIlyass Moummad</p>\n<p><strong>Date/time:  Friday, 21st June 2024, 2pm</strong></p>\n<p>**Location: G2, ENG, Mile End Campus, QMUL, E1 4NS **\nZoom: <a href=\"https://qmul-ac-uk.zoom.us/j/9798452959\">https://qmul-ac-uk.zoom.us/j/9798452959</a></p>\n<p><b>Title</b>: Self-Supervised Invariant Learning of Bird Sound Representations</p>\n<hr>\n<p><b>Abstract</b>: Abstract: Self-supervised learning (SSL) involves the learning of data representations without any manual annotation. It consists of solving a pretext task relevant for learning informative data representations, which can be used for transfer learning to solve downstream tasks. Among the different learning paradigms, the most successful in learning discriminative features for classification tasks are “Invariant Learning” methods. They train the model to be insensitive to pre-defined transformations (e.g. if pitch shift is used as a transformation, the model is shown two versions of the same signal with different pitch shifts and is trained to output the same representation for both versions). The choice of data transformations for learning invariance is crucial and depends on the data domain and its relevance to downstream tasks.\nIn bioacoustics, it is not yet clear which data transformations the model should be robust to. In this work, we show that simple and domain-agnostic data augmentations (which do not use any prior knowledge of the nature of bioacoustic sounds) can learn robust and informative features. We evaluate the learned representations through transfer learning to downstream tasks with different challenges such as novel classes (downstream datasets can have classes never seen during pretraining), few-shot (very few annotations given for a downstream), label shift (evaluation recordings come from different geographical regions), and covariate shift (difference in recording settings and environmental conditions of the same classes between pretraining and evaluation data).</p>\n<p><b>Bio</b>: Ilyass Moummad is a PhD student (December 2021 - November 2024) at IMT Atlantique, Brest, France. He works under the supervision of Nicolas Farrugia and is co-supervised by Romain Serizel. Currently, Ilyass is a Visiting Researcher at C4DM, QMUL, working under the supervision of Emmanouil Benetos (April - June 2024). Ilyass’s PhD topic is Deep Learning for Bioacoustics, with an interest in representation learning (both self-supervised and supervised) of animal sounds, as well as few-shot learning (species sound classification and detection from very few annotated examples).</p>","id":"eff5e87a-d691-5412-a7b5-cde72a465f19"},{"fields":{"slug":"/news/news.2024-06-18.C4DM-Mikolaj_Kegler"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Mikolaj Kegler","author":"Admin","date":"Tue 18 Jun 2024"},"html":"<h3>C4DM Seminar: Mikolaj Kegler: Hear What You Want, towards seamless, immersive AI experiences for wearable audio devices</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nMikolaj Kegler</p>\n<p><strong>Date/time:  Tuesday, 18th June 2024, 3pm</strong></p>\n<p>**Location: GC205, Greduate Centre, QMUL, E1 4NS **\nZoom: <a href=\"https://qmul-ac-uk.zoom.us/j/9798452959\">https://qmul-ac-uk.zoom.us/j/9798452959</a></p>\n<p><b>Title</b>: Hear What You Want, towards seamless, immersive AI experiences for wearable audio devices</p>\n<hr>\n<p><b>Abstract</b>: Hear What You Want is both the mission statement and the title of the Bose Research franchise established to build AI-powered experiences for next-generation wearable audio devices. This interdisciplinary franchise involves a wide range of research tracks, such as source separation and sound event detection, to name a few. All these efforts share one common denominator, the resulting solutions must be suitable for low-latency execution on the embedded target devices to enable seamless, immersive experiences. In this talk, I will start by outlining the fundamental problem of limited computational resources on embedded platforms and present our latest methods for building compact yet performant ML models. Subsequently, I will discuss the problem of low-latency causal target source extraction and highlight our recent developments. I will conclude by presenting our latest work on improving the efficiency and fidelity of text-toaudio generative models for Foley sound synthesis.</p>\n<p><b>Bio</b>: Mikolaj “Miko” Kegler is an Audio Machine Learning Scientist at Bose Corporation. His research at the interface of ML and DSP is focused on developing methods for lightweight, low-latency, on-device speech and audio signal processing. Before joining Bose, Miko was awarded a PhD from the Department of Bioengineering &#x26; Centre for Neurotechnology, Imperial College London, where he has been investigating neural mechanisms underlying perception and comprehension of speech, especially in challenging listening conditions. During his PhD, he completed multiple audio ML research placements, including at Amazon Lab126 and Logitech, where he also served as a long-term scientific consultant.</p>","id":"e286df2d-7792-5cdd-b6ac-2810abc3f1d0"},{"fields":{"slug":"/news/news.2024-06-17.C4DM-Yinghao_Ma"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar:  Yinghao Ma","author":"Admin","date":"Mon 17 Jun 2024"},"html":"<h3>C4DM Seminar: Yinghao Ma: The Impact of Cutting-edge AI Technologies to Music Industry</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nYinghao Ma</p>\n<p><strong>Date/time:  Monday, 17th June 2024, 2:15pm</strong></p>\n<p>**Location: G2, ENG, Mile End campus, QMUL, E1 4NS **\nZoom: <a href=\"https://qmul-ac-uk.zoom.us/j/9798452959\">https://qmul-ac-uk.zoom.us/j/9798452959</a></p>\n<p><b>Title</b>: The Impact of Cutting-edge AI Technologies to Music Industry</p>\n<hr>\n<p><b>Abstract</b>: Abstract: This is a rehearsal of the invitation presentation to the Whitgift School for its teachers and Independent Directors.  In this presentation, I will introduce cutting-edge AI technologies including Large Language Models (LLMs) and Latent Diffusion Models (LDMs) and their application to music understanding and music generation. We will briefly discuss the industrial applications and ethical concerns in the music industry and music education such as deepfake and copyright issues etc.</p>\n<p><b>Bio</b>: MA Yinghao (马英浩) is a Ph.D. student in Artificial Intelligence and Music (AIM) program at Centre for Digital Music (C4DM), School of EECS, Queen Mary University of London, supervised by Dr. Emmanouil Benetos, Dr. Chris Donahue (secondary), and Prof. Simon Dixon (independent assessor). He is one of the co-founders of the Multimodal Art Projection (MAP) community. Together with his colleague, he proposed an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), with more than 50k downloads on the Huggingfac page, and established a Music Audio Representation Benchmark for universaL Evaluation (MRABLE). He is also interested in music-related multimodality and developed MusiLingo, a music captioning and query response model based on the alignment of single-modality pre-trained models.</p>","id":"1a01c73f-cdc6-5ead-a847-0430b4e2b2aa"},{"fields":{"slug":"/news/2024-14-06.C4DM-at_SMC_2024"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#083858","images":{"fallback":{"src":"/static/0b07bafbd3ec51947ac07a1f22fe3e1d/f03da/SMC2024.png","srcSet":"/static/0b07bafbd3ec51947ac07a1f22fe3e1d/208b0/SMC2024.png 746w,\n/static/0b07bafbd3ec51947ac07a1f22fe3e1d/5ff0b/SMC2024.png 1491w,\n/static/0b07bafbd3ec51947ac07a1f22fe3e1d/f03da/SMC2024.png 2982w","sizes":"(min-width: 2982px) 2982px, 100vw"},"sources":[{"srcSet":"/static/0b07bafbd3ec51947ac07a1f22fe3e1d/47834/SMC2024.webp 746w,\n/static/0b07bafbd3ec51947ac07a1f22fe3e1d/7e32e/SMC2024.webp 1491w,\n/static/0b07bafbd3ec51947ac07a1f22fe3e1d/bb5d7/SMC2024.webp 2982w","type":"image/webp","sizes":"(min-width: 2982px) 2982px, 100vw"}]},"width":2982,"height":2982}}},"title":"C4DM at SMC 2024","author":"Ashley Noel-Hirst","date":"Sat 15 Jun 2024"},"html":"<p>On 4–6th July, several C4DM researchers will participate in the <b><a href=\"https://smcnetwork.org/smc2024/#programme\">2024 conference on Sound and Music computing (SMC 2024)</a></b> in Porto, Portugal. With this years theme being 'immersive', the conference brings together interdisciplinary work from composers, scientists and other researchers tackling engagement with digital sound and music.</p>\n<p>The Centre for Digital Music will be present with work ranging from piano transcription to latent audio models. We look forward to sharing the below papers, authored or co-authored by C4DM members:</p>\n<ul>\n<li>\n<p>A Generative Framework for Composition-aware Loop Recommendation In Music Production: Drum2Bass Use Case, by Xiaowan Yi and Mathieu Barthet</p>\n</li>\n<li>\n<p>Simulating Piano Performance Mistakes for Music Learning Context, by Alia Morsi (Universitat Pompeu Fabra), Huan Zhang, Akira Maezawa (Yamaha Corporation), Simon Dixon and Xavier Serra (Universitat Pompeu Fabra)</p>\n</li>\n<li>\n<p>Temporal Analysis of Emotion Perception in Film Music: Insights from the FME-24 Dataset, by Ruby O.N Crocker and George Fazekas</p>\n</li>\n<li>\n<p><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/97438#\">Reconstructing the Charlie Parker Omnibook using an audio-to-score automatic transcription pipeline</a>, by Xavier Riley and Simon Dixon</p>\n</li>\n<li>\n<p>A Sonification Method for Monitoring Chemical Sensor Data, by Yutian Hum, Tony Stockman, Aleksandar Radu, Ernesto Saiz, Nick Bryan-Kinns</p>\n</li>\n</ul>","id":"63fee073-5152-5714-8abf-c3f3ceef055a"},{"fields":{"slug":"/news/2024-06-13.C4DM-organises_and_participates_at_AES_symposium"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#183848","images":{"fallback":{"src":"/static/e985701b87726741bf3a416eeecfbebd/b9792/AES_AiMusician.jpg","srcSet":"/static/e985701b87726741bf3a416eeecfbebd/845e4/AES_AiMusician.jpg 192w,\n/static/e985701b87726741bf3a416eeecfbebd/c2c05/AES_AiMusician.jpg 384w,\n/static/e985701b87726741bf3a416eeecfbebd/b9792/AES_AiMusician.jpg 768w","sizes":"(min-width: 768px) 768px, 100vw"},"sources":[{"srcSet":"/static/e985701b87726741bf3a416eeecfbebd/694c3/AES_AiMusician.webp 192w,\n/static/e985701b87726741bf3a416eeecfbebd/6d535/AES_AiMusician.webp 384w,\n/static/e985701b87726741bf3a416eeecfbebd/482be/AES_AiMusician.webp 768w","type":"image/webp","sizes":"(min-width: 768px) 768px, 100vw"}]},"width":768,"height":768}}},"title":"C4DM students organise and participate at the AES International Symposium on AI and the Musician","author":"Emmanouil Benetos","date":"Thu 13 Jun 2024"},"html":"<p>Centre for Digital Music PhD students both organised and presented at the <a href=\"https://aes2.org/events-calendar/aes-international-symposium-on-ai-and-the-musician/\">AES International Symposium on AI and the Musician</a>, which took place on 6-8 June 2024 at the Berklee College of Music, Boston, MA, USA.</p>\n<p>C4DM PhD student <a href=\"https://www.christiansteinmetz.com/\">Christian Steinmetz</a> was both Papers Chair and Workshops Chair for the symposium, and was instrumental in organising a <a href=\"https://www.linkedin.com/posts/akoretzky_music-musicproduction-audio-activity-7206806041240494080-D8kh/\">successful event</a>.</p>\n<p>The following works were authored/coauthored by C4DM PhD students and academic staff:</p>\n<ul>\n<li>\n<p><a href=\"https://aes2024aisymposiumon.sched.com/event/1e14l/deep-learning-based-audio-representations-for-the-analysis-and-visualisation-of-electronic-dance-music-dj-mixes\">Deep learning-based audio representations for the analysis and visualisation of electronic dance music DJ mixes</a>, by Alexander Williams, Haokun Tian, Stefan Latter, Mathieu Barthet, and Charalampos Saitis</p>\n</li>\n<li>\n<p><a href=\"https://aes2024aisymposiumon.sched.com/event/1e14x/advancing-ai-in-music-composition-refining-the-generative-music-overpainting-task\">Advancing AI in music composition: refining the generative music overpainting task</a>, by Eleanor Row and George Fazekas</p>\n</li>\n<li>\n<p><a href=\"https://aes2024aisymposiumon.sched.com/event/1e150/when-xr-meets-ai-integrating-interactive-machine-learning-with-an-xr-musical-instrument\">When XR meets AI: Integrating interactive machine learning with an XR musical instrument</a>, by Max Graf and Mathieu Barthet</p>\n</li>\n</ul>","id":"6cb7c1cc-3a91-544d-9866-c0be95ea8fb8"}]}}}
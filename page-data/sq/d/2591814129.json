{"data":{"text":{"html":"<p>The Communication Acoustics Lab at the Centre for Digital Music conducts cutting edge research into the ways we perceive sound and technologies for improving communication. We work across disciplines of music, engineering, psychology, and cognitive science to study communication between humans and between humans and machines.</p>\n<p>Topics of interest include:</p>\n<ul>\n<li>Perception, cognition &#x26; aesthetics of sound</li>\n<li>Cross-sensory perception involving sound</li>\n<li>Enabling digital audio systems with an understanding of how we listen</li>\n<li>Modelling human values (&#x26; biases) with multimodal music data</li>\n<li>Digital tools for education, outreach, &#x26; research</li>\n</ul>\n<p>For more information, please visit the lab webpage (under construction)</p>\n<h2>Members</h2>","frontmatter":{"title":""}},"people":{"nodes":[{"id":"226ac21f-bb0b-56df-9975-41dc5d56d6ca","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Aditya Bhattacharjee","role":"PhD","url":"https://www.linkedin.com/in/adibh/","acadposition":"PhD Student","blurb":"Self-supervision in Audio Fingerprinting","themes":["mir","mlist"]}},{"id":"cc381a30-c5d2-5ff7-b60d-1f10d81dbf9b","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Alexander Williams","role":"PhD","url":"","acadposition":"PhD Student","blurb":"User-driven deep music generation in digital audio workstations","themes":["mir","audioeng"]}},{"id":"5bc5dee9-7eda-5f71-aed9-1b5319da59d7","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Andrea Guidi","role":"PhD","url":"https://mat.qmul.ac.uk/students/andrea-guidi","acadposition":"PhD Student","blurb":"Design for auditory imagery","themes":["mir"]}},{"id":"d35868ef-7cda-5e98-aa09-bb356da9e5d3","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Andrea Martelloni","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/martelloniandrea-1.html","acadposition":"PhD Student","blurb":"Real-Time Gesture Classification on an Augmented Acoustic Guitar using Deep Learning to Improve Extended-Range and Percussive Solo Playing","themes":["mir"]}},{"id":"738dd58b-17cc-53bb-b319-0186faf690cc","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Andrew (Drew) Edwards","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/edwardsandrewcharles.html","acadposition":"PhD Student","blurb":"Deep Learning for Jazz Piano: Transcription + Generative Modeling","themes":["mir","mlist","mcog"]}},{"id":"9a6273ce-dc95-5d23-b5f9-7bcebfdc1772","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#685848","images":{"fallback":{"src":"/c4dm-website/static/c0f574ea604c6ff9084a488c4bc7e4e5/0be83/ashleynoelhirst.png","srcSet":"/c4dm-website/static/c0f574ea604c6ff9084a488c4bc7e4e5/5f035/ashleynoelhirst.png 320w,\n/c4dm-website/static/c0f574ea604c6ff9084a488c4bc7e4e5/eadd3/ashleynoelhirst.png 640w,\n/c4dm-website/static/c0f574ea604c6ff9084a488c4bc7e4e5/0be83/ashleynoelhirst.png 1280w","sizes":"(min-width: 1280px) 1280px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/c0f574ea604c6ff9084a488c4bc7e4e5/5e011/ashleynoelhirst.webp 320w,\n/c4dm-website/static/c0f574ea604c6ff9084a488c4bc7e4e5/90d07/ashleynoelhirst.webp 640w,\n/c4dm-website/static/c0f574ea604c6ff9084a488c4bc7e4e5/9e21f/ashleynoelhirst.webp 1280w","type":"image/webp","sizes":"(min-width: 1280px) 1280px, 100vw"}]},"width":1280,"height":1280}}},"name":"Ashley Noel-Hirst","role":"PhD","url":"https://ashleynoelhirst.co.uk/","acadposition":"PhD Student","blurb":"Latent Spaces for Human-AI music generation","themes":["mir","isam"]}},{"id":"3e2bbfc8-0f9b-5aac-ba55-b0f8ffc899b9","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Berker Banar","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/banarberker.html","acadposition":"PhD Student","blurb":"Towards Composing Contemporary Classical Music using Generative Deep Learning","themes":["mir","soundsynthesis"]}},{"id":"66b87075-3456-5b0c-9dfe-f91b43204096","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Brendan O'Connor","role":"PhD","url":"https://trebolium.github.io/","acadposition":"PhD Student","blurb":"Singing Voice Attribute Transformation","themes":["soundsynthesis","audioeng","mir"]}},{"id":"4f0a9d6f-a76c-5668-832e-de53c816bfd5","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Carey Bunks","role":"PhD","url":"","acadposition":"PhD Student","blurb":"Cover Song Identification","themes":["mir"]}},{"id":"fbc098ab-2b5e-5325-ae02-ad67d316619d","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Carlos Lordelo","role":"PhD","url":"https://cpvlordelo.github.io/","acadposition":"PhD Student","blurb":"Instrument modelling to aid polyphonic transcription","themes":["mir","mlist"]}},{"id":"34ef929b-de59-5c95-a554-597f98d80226","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Chin-Yun Yu","role":"PhD","url":"https://yoyololicon.github.io/","acadposition":"PhD Student","blurb":"Neural Audio Synthesis with Expressiveness Control","themes":["audioeng","mir","soundsynthesis"]}},{"id":"c210c41b-f202-57d7-ab0c-14538a1c59bc","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Christopher Mitcheltree","role":"PhD","url":"https://christhetr.ee","acadposition":"PhD Student","blurb":"Representation Learning for Audio Production Style and Modulations","themes":["mlist","audioeng","mir"]}},{"id":"f855554f-b1f4-56be-8d59-801f8b9664fe","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"David Foster","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/fosterdavid.html","acadposition":"PhD Student","blurb":"Modelling the Creative Process of Jazz Improvisation","themes":["mir"]}},{"id":"50ed75b3-142a-592e-9edf-31ddba652941","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Domenico Stefani","role":"Visitor","url":"https://domenicostefani.com/","acadposition":"University of Trento, Italy","blurb":"Embedded machine learning for smart musical instruments","themes":["mir"]}},{"id":"70aeaa3d-8fcd-545e-913c-259db12737c3","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Edward Hall","role":"PhD","url":"https://mat.qmul.ac.uk/students/edward-hall","acadposition":"PhD Student","blurb":"Probabilistic modelling of thematic development and structural coherence in music","themes":["mir"]}},{"id":"b50fe044-034c-561a-9c0c-70e7b9811648","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Elizabeth Wilson","role":"PhD","url":"https://lwlsn.github.io","acadposition":"PhD Student","blurb":"Co-creative Algorithmic Composition Based on Models of Affective Response","themes":["mir","isam"]}},{"id":"7b043f0b-de2f-5155-9877-1dfff2b8c117","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Elona Shatri","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/shatrielona-1.html","acadposition":"PhD Student","blurb":"Optical music recognition using deep learning","themes":["mir"]}},{"id":"ef8d4761-ec9b-58f0-bfa5-26159c60c38e","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/8106f707dd10b39434bd1c08f1f22f6d/30f07/emmanouilbenetos.jpg","srcSet":"/c4dm-website/static/8106f707dd10b39434bd1c08f1f22f6d/41624/emmanouilbenetos.jpg 160w,\n/c4dm-website/static/8106f707dd10b39434bd1c08f1f22f6d/1b894/emmanouilbenetos.jpg 320w,\n/c4dm-website/static/8106f707dd10b39434bd1c08f1f22f6d/30f07/emmanouilbenetos.jpg 640w","sizes":"(min-width: 640px) 640px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/8106f707dd10b39434bd1c08f1f22f6d/60b4d/emmanouilbenetos.webp 160w,\n/c4dm-website/static/8106f707dd10b39434bd1c08f1f22f6d/5e011/emmanouilbenetos.webp 320w,\n/c4dm-website/static/8106f707dd10b39434bd1c08f1f22f6d/90d07/emmanouilbenetos.webp 640w","type":"image/webp","sizes":"(min-width: 640px) 640px, 100vw"}]},"width":640,"height":640}}},"name":"Dr Emmanouil Benetos","role":"Academic","url":"http://www.eecs.qmul.ac.uk/people/view/4741/dr-emmanouil-benetos","acadposition":"Reader in Machine Listening, Turing Fellow","blurb":"Machine listening, music information retrieval, computational sound scene analysis, machine learning for audio analysis, language models for music and audio, computational musicology","themes":["mir","mlist"]}},{"id":"1b6798fe-1d19-52a2-add1-70afa99577f6","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Gary Bromham","role":"PhD","url":"","acadposition":"PhD Student","blurb":"The role of nostalga in music production","themes":["mir","audioeng"]}},{"id":"9aa31664-07ac-5194-a7fe-146cab80fddb","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#a8b8e8","images":{"fallback":{"src":"/c4dm-website/static/eaf04fbbdffdb3aca0caed16cfdf1102/7900e/gyorgyfazekas.jpg","srcSet":"/c4dm-website/static/eaf04fbbdffdb3aca0caed16cfdf1102/f713e/gyorgyfazekas.jpg 55w,\n/c4dm-website/static/eaf04fbbdffdb3aca0caed16cfdf1102/e2afd/gyorgyfazekas.jpg 110w,\n/c4dm-website/static/eaf04fbbdffdb3aca0caed16cfdf1102/7900e/gyorgyfazekas.jpg 219w","sizes":"(min-width: 219px) 219px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/eaf04fbbdffdb3aca0caed16cfdf1102/938d3/gyorgyfazekas.webp 55w,\n/c4dm-website/static/eaf04fbbdffdb3aca0caed16cfdf1102/8c6ff/gyorgyfazekas.webp 110w,\n/c4dm-website/static/eaf04fbbdffdb3aca0caed16cfdf1102/3f57d/gyorgyfazekas.webp 219w","type":"image/webp","sizes":"(min-width: 219px) 219px, 100vw"}]},"width":219,"height":219}}},"name":"Dr George Fazekas","role":"Academic","url":"http://eecs.qmul.ac.uk/~gyorgyf","acadposition":"Senior Lecturer","blurb":"Semantic Audio, Music Information Retrieval, Semantic Web for Music, Machine Learning and Data Science, Music Emotion Recognition, Interactive music sytems (e.g. intellignet editing, audio production and performance systems)","themes":["mir"]}},{"id":"b71f6653-2c4e-5f2b-b283-2eb0ae2679b6","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Harnick Khera","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/kheraharnicksingh.html","acadposition":"PhD Student","blurb":"Informed source separation for multi-mic production","themes":["mir","mlist"]}},{"id":"1918386c-730c-5e3e-9d42-0369c7ed79b1","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Huan Zhang","role":"PhD","url":"http://eecs.qmul.ac.uk/people/profiles/zhanghuan.html","acadposition":"PhD Student","blurb":"Computational Modelling of Expressive Piano Performance","themes":["mir"]}},{"id":"27c0231d-35a2-57d5-8273-8562fba3319f","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Iacopo Ghinassi","role":"PhD","url":"https://github.com/Ighina","acadposition":"PhD Student","blurb":"Semantic understanding of TV programme content and structure to enable automatic enhancement and adjustment","themes":["mir"]}},{"id":"8f618d2e-72c3-5ff1-a65f-f14410f7ea3e","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Ilaria Manco","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/mancoilaria.html","acadposition":"PhD Student","blurb":"Multimodal Deep Learning for Music Information Retrieval","themes":["mir","mlist"]}},{"id":"b3e3ac4a-cd06-5d69-8ac9-f47d73ef5c0b","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8a898","images":{"fallback":{"src":"/c4dm-website/static/5c4ebc68a7723fc56115ed3fe749ff7a/96deb/johanpauwels.jpg","srcSet":"/c4dm-website/static/5c4ebc68a7723fc56115ed3fe749ff7a/c81d1/johanpauwels.jpg 38w,\n/c4dm-website/static/5c4ebc68a7723fc56115ed3fe749ff7a/91a6d/johanpauwels.jpg 75w,\n/c4dm-website/static/5c4ebc68a7723fc56115ed3fe749ff7a/96deb/johanpauwels.jpg 150w","sizes":"(min-width: 150px) 150px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/5c4ebc68a7723fc56115ed3fe749ff7a/0852d/johanpauwels.webp 38w,\n/c4dm-website/static/5c4ebc68a7723fc56115ed3fe749ff7a/18188/johanpauwels.webp 75w,\n/c4dm-website/static/5c4ebc68a7723fc56115ed3fe749ff7a/c65bc/johanpauwels.webp 150w","type":"image/webp","sizes":"(min-width: 150px) 150px, 100vw"}]},"width":150,"height":150}}},"name":"Dr Johan Pauwels","role":"Academic","url":"http://www.eecs.qmul.ac.uk/people/view/50775/johan-pauwels","acadposition":"Lecturer in Audio Signal Processing","blurb":"automatic music labelling, music information retrieval, music signal processing, machine learning for audio, chord/key/structure (joint) estimation, instrument identification, multi-track/channel audio, music transcription, graphical models, big data science","themes":["mir","mlist"]}},{"id":"12a808a3-c5af-5b8f-927d-9078a2f03d06","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Lele Liu","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/liulele.html","acadposition":"PhD Student","blurb":"Automatic music transcription with end-to-end deep neural networks","themes":["mir","mlist"]}},{"id":"f9561550-c8f0-5400-b97b-10f940bd70bd","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Louise Thorpe","role":"PhD","url":"","acadposition":"PhD Student","blurb":"Using Signal-informed Source Separation (SISS) principles to improve instrument separation from legacy recordings","themes":["mir","audioeng"]}},{"id":"3fffc866-8036-535d-8b12-673653a1e811","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#d8c8c8","images":{"fallback":{"src":"/c4dm-website/static/22312dadef02a15635234aa9c85e6a6b/6f467/marksandler.jpg","srcSet":"/c4dm-website/static/22312dadef02a15635234aa9c85e6a6b/99d04/marksandler.jpg 247w,\n/c4dm-website/static/22312dadef02a15635234aa9c85e6a6b/72fdf/marksandler.jpg 494w,\n/c4dm-website/static/22312dadef02a15635234aa9c85e6a6b/6f467/marksandler.jpg 988w","sizes":"(min-width: 988px) 988px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/22312dadef02a15635234aa9c85e6a6b/f19fb/marksandler.webp 247w,\n/c4dm-website/static/22312dadef02a15635234aa9c85e6a6b/dab72/marksandler.webp 494w,\n/c4dm-website/static/22312dadef02a15635234aa9c85e6a6b/3c8a3/marksandler.webp 988w","type":"image/webp","sizes":"(min-width: 988px) 988px, 100vw"}]},"width":988,"height":988}}},"name":"Prof Mark Sandler ","role":"Academic","url":"http://www.eecs.qmul.ac.uk/people/view/3114/prof-mark-sandler","acadposition":"C4DM Director, Turing Fellow, Royal Society Wolfson Research Merit award holder","blurb":"Digital Signal Processing, Digital Audio, Music Informatics, Audio Features, Semantic Audio, Immersive Audio, Studio Science, Music Data Science, Music Linked Data.","themes":["mir"]}},{"id":"4743033f-175f-5477-9ea2-bb9f890dcb85","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Maryam Torshizi","role":"PhD","url":"","acadposition":"PhD Student","blurb":"Music emotion modelling using graph analysis","themes":["comma","mir"]}},{"id":"9b6c0163-cfdc-560b-82fe-8b8853e02502","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Mary Pilataki","role":"PhD","url":"https://github.com/marypilataki","acadposition":"PhD Student","blurb":"Deep Learning methods for Multi-Instrument Music Transcription","themes":["mir"]}},{"id":"d74e8c2b-56f9-50cf-a43b-8b0191803bff","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/661d84eba8a4676c5210510bd4b2c4b6/93106/mathieubarthet.webp","srcSet":"/c4dm-website/static/661d84eba8a4676c5210510bd4b2c4b6/c23ec/mathieubarthet.webp 123w,\n/c4dm-website/static/661d84eba8a4676c5210510bd4b2c4b6/02ea5/mathieubarthet.webp 246w,\n/c4dm-website/static/661d84eba8a4676c5210510bd4b2c4b6/93106/mathieubarthet.webp 492w","sizes":"(min-width: 492px) 492px, 100vw"},"sources":[]},"width":492,"height":492}}},"name":"Dr Mathieu Barthet","role":"Academic","url":"http://www.eecs.qmul.ac.uk/people/view/4808/dr-mathieu-barthet","acadposition":"Senior Lecturer in Digital Media","blurb":"Music information research, Internet of musical things, Extended reality, New interfaces for musical expression, Semantic audio, Music perception (timbre, emotions), Audience-Performer interaction, Participatory art","themes":["mir","augmi","isam","mupae"]}},{"id":"680dbd18-ee36-5c29-8ae3-3c894e883317","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Dr Matthias Mauch","role":"Visitor","url":"http://www.matthiasmauch.net/","acadposition":"Visiting Academic","blurb":"music transcription (chords, beats, drums, melody, ...), interactive music annotation, singing research, research in the evolution of musical styles","themes":["mir"]}},{"id":"de8a6a9d-d9db-5d6f-aa49-d6383eab8ec3","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Ningzhi Wang","role":"PhD","url":"","acadposition":"PhD Student","blurb":"Generative Models For Music Audio Representation And Understanding","themes":["mir","mcog","soundsynthesis"]}},{"id":"68891a8b-ed44-55dc-a3f7-f02f74bda436","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Pedro Sarmento","role":"PhD","url":"https://otnemrasordep.github.io/","acadposition":"PhD Student","blurb":"Guitar-Oriented Neural Music Generation in Symbolic Format","themes":["mir"]}},{"id":"9be482ab-e543-5ed7-a0ec-2d37af553450","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Ruby Crocker","role":"PhD","url":"","acadposition":"PhD Student","blurb":"Continuous mood recognition in film music","themes":["mir","mcog"]}},{"id":"fa3ed1ea-d5f0-59f2-92de-1a1aaa953db9","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Saurjya Sarkar","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/sarkarsaurjya-1.html","acadposition":"PhD Student","blurb":"New perspectives in instrument-based audio source separation","themes":["mir"]}},{"id":"06196503-f058-5081-ab1e-21e9b406e714","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/91f7a353a15c6e68c71f84ff92066743/ffdfb/simondixon.png","srcSet":"/c4dm-website/static/91f7a353a15c6e68c71f84ff92066743/5bd84/simondixon.png 92w,\n/c4dm-website/static/91f7a353a15c6e68c71f84ff92066743/f9b49/simondixon.png 184w,\n/c4dm-website/static/91f7a353a15c6e68c71f84ff92066743/ffdfb/simondixon.png 367w","sizes":"(min-width: 367px) 367px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/91f7a353a15c6e68c71f84ff92066743/483b8/simondixon.webp 92w,\n/c4dm-website/static/91f7a353a15c6e68c71f84ff92066743/42e3f/simondixon.webp 184w,\n/c4dm-website/static/91f7a353a15c6e68c71f84ff92066743/46fdb/simondixon.webp 367w","type":"image/webp","sizes":"(min-width: 367px) 367px, 100vw"}]},"width":367,"height":367}}},"name":"Prof. Simon Dixon","role":"Academic","url":"http://www.eecs.qmul.ac.uk/~simond/","acadposition":"Professor of Computer Science, Deputy Director of C4DM, Director of the AIM CDT, Turing Fellow","blurb":"Music informatics, music signal processing, artificial intelligence, music cognition; extraction of musical content (e.g. rhythm, harmony, intonation) from audio signals: beat tracking, audio alignment, chord and note transcription, singing intonation; using signal processing approaches, probabilistic models, and deep learning.","themes":["mir"]}},{"id":"64e68bf1-afa9-5242-ba95-3bbb3ff80584","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Soumya Sai Vanka","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/vankasaisoumya.html","acadposition":"PhD Student","blurb":"Music Production Style Transfer and Mix Similarity","themes":["audioeng","mir"]}},{"id":"c4baa5bd-09a1-5c86-9e66-b8a4a269c168","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Sungkyun Chang","role":"Research Assistant","url":"http://eecs.qmul.ac.uk/profiles/sungkyun-chang.html","acadposition":"Research Assistant","blurb":"Deep learning technologies for multi-instrument automatic music transcription","themes":["mir","mlist"]}},{"id":"4fa7138d-03bd-56bc-8a7a-4c5cc66811f2","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Thomas Kaplan","role":"PhD","url":"https://kappers.github.io/","acadposition":"PhD Student","blurb":"Probabilistic modelling of rhythm perception and production","themes":["mir"]}},{"id":"aac95703-2ad6-5435-ba42-e404dc3a150e","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Tyler Howard McIntosh","role":"PhD","url":"","acadposition":"PhD Student","blurb":"Expressive Performance Rendering for Music Generation Systems","themes":["mcog","mir"]}},{"id":"18bb4e4a-ca3d-51a0-9beb-5aea409131ab","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Vjosa Preniqi","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/preniqivjosa.html","acadposition":"PhD Student","blurb":"Predicting demographics, personalities, and global values from digital media behaviours","themes":["mir","mcog"]}},{"id":"d964f102-7a03-5e39-8ca5-d62171217446","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Xavier Riley","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/rileyjohnxavier.html","acadposition":"PhD Student","blurb":"Pitch tracking for music applications - beyond 99% accuracy","themes":["mir","audioeng"]}},{"id":"04fb1f99-0166-5a9c-8f32-bbda54b39c18","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Yannis (John) Vasilakis","role":"PhD","url":"https://www.linkedin.com/in/yannis-vasilakis-6bb9b11b1/","acadposition":"PhD Student","blurb":"Active Learning for Interactive Music Transcription","themes":["mlist","mir"]}},{"id":"e0104201-8b94-5c6c-862b-a813f7cdc558","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Yinghao Ma","role":"PhD","url":"https://nicolaus625.github.io/","acadposition":"PhD Student","blurb":"Self-supervision in machine listening","themes":["mir","mlist"]}},{"id":"8b81527a-163a-5125-9fc9-247a73a54a05","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Yixiao Zhang","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/zhangyixiao.html","acadposition":"PhD Student","blurb":"Machine Learning Methods for Artificial Musicality","themes":["mir"]}},{"id":"8d93ecc6-ed02-5662-a390-01b9e68ef5f5","frontmatter":{"image":null,"name":"Yukun Li","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/liyukun.html","acadposition":"PhD Student","blurb":"Computational Comparison Between Different Genres of Music in Terms of the Singing Voice","themes":["mir"]}},{"id":"1e774e43-fee0-57f5-b2b5-632fe8762c2a","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Cyrus Vahidi","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/vahidicyrus.html","acadposition":"PhD Student","blurb":"Perceptual end to end learning for music understanding","themes":["mir"]}}]}}}
{"data":{"text":{"html":"<ul>\n<li><a href=\"http://c4dm.eecs.qmul.ac.uk/audioengineering/news/index.html\"><strong>News and reporting</strong></a> on our research</li>\n<li><strong>Demonstration videos</strong> on <a href=\"http://www.youtube.com/user/IntelligentSoundEng\">YouTube Channel: Intelligent Sound Engineering</a></li>\n<li>Check out our <a href=\"https://intelligentsoundengineering.wordpress.com/\"><strong>blog, Intelligent Sound Engineering</strong></a></li>\n<li><a href=\"http://www.eecs.qmul.ac.uk/~josh/publications.htm\"><strong>Academic publications</strong></a></li>\n</ul>\n<p>Some of our current and recent research projects include;</p>\n<ul>\n<li><a href=\"https://github.com/BrechtDeMan/WebAudioEvaluationTool\">Web Audio Evaluation Tool</a>- a browser-based listening test environment. See the <a href=\"http://www.eecs.qmul.ac.uk/~josh/documents/2016/Jillings%20-%20WAET%20-%202016.pdf\">Web Audio Conference</a> and <a href=\"http://www.eecs.qmul.ac.uk/~josh/documents/2015/Jillings%20-%20SMC10%202015.pdf\">Sound and Music Computing</a> publications and <a href=\"https://youtu.be/aHmgSSVaPRY\">this demo video</a></li>\n<li><a href=\"http://multitrack.eecs.qmul.ac.uk/\">The Open Multitrack Testbed</a></li>\n<li><a href=\"http://c4dm.eecs.qmul.ac.uk/audioengineering/automaticmixing/index.html\">Automatic mixing</a> - Methods for automatic multitrack audio production in real-time</li>\n<li><a href=\"soundsynthesis.html\">Sound synthesis</a></li>\n<li><a href=\"http://c4dm.eecs.qmul.ac.uk/audioengineering/spatialaudio/index.html\">Spatial audio production and reproduction</a></li>\n<li><a href=\"http://c4dm.eecs.qmul.ac.uk/audioengineering/reverseengineering/index.html\">Reverse engineering the mix - a targeted mixing and mastering tool</a></li>\n<li><a href=\"http://c4dm.eecs.qmul.ac.uk/audioengineering/microphoneartifacts/index.html\">Reducing Artifacts Between Source and Microphone in Live Sound</a></li>\n<li><a href=\"http://c4dm.eecs.qmul.ac.uk/audioengineering/transientmodification/index.html\">Transient modification</a></li>\n<li><a href=\"http://c4dm.eecs.qmul.ac.uk/audioengineering/sourceseparation/index.html\">Source separation for live sound</a> - Crosstalk, bleed and interference reduction techniques</li>\n<li><a href=\"http://c4dm.eecs.qmul.ac.uk/audioengineering/gesturecontrol/index.html\">Gesture-controlled mixing of multichannel audio</a></li>\n<li><a href=\"http://www.eecs.qmul.ac.uk/~stevenh/multi_seg.html\">Multitrack segmentation</a></li>\n<li><a href=\"http://c4dm.eecs.qmul.ac.uk/audioengineering/compressors/index.html\">Intelligent dynamic range compression</a></li>\n<li><a href=\"http://www.brechtdeman.com/blog/microphone-shootout.html\">Microphone shoot-out</a> (pairwise and multi-stimuli evaluation)</li>\n</ul>\n<p><a href=\"study.html\">PhD Study</a> - interested in joining the team? We are currently accepting PhD applications.</p>\n<h2>Aims</h2>\n<p>We develop <strong>intelligent recording techniques</strong>, for use by audio editors, mixers and sound engineers, which speed up the recording process, minimise preparation for live performance, and enable easy preparation and transmission of high resolution audio.</p>\n<p>Advances in signal processing, machine learning and adaptive systems, have rarely been applied to the professional audio market. This is partly because most digital signal processing applications in these areas have remained focused on replicating or improving those techniques which could be applied in the analogue domain. And until recently, mixing consoles and audio workstations did not have the computing power to allow the introduction of multi-input, multi-output processing tools. Thus audio effects have traditionally been limited to those which operate only on single or stereo channels. There is now an opportunity to develop <strong>advanced audio effects</strong> which analyse all input channels in order to produce the ideal mix.</p>\n<p>Audio engineering for <strong>live sound production</strong> represents a field with strong potential for improvement and automation. Much of the effort of a sound engineer in preparation for a live performance is consumed by tedious, repetitive tasks. Levels must be set to avoid feedback, input channels must be panned to stereo or surround sound, equalisation, normalisation and compression must be applied to each channel, and all equipment must be tested along with establishing an optimal choice of microphone placement. Only after these tasks have been performed, if time and resources permit, may the sound engineer refine these choices to produce an aesthetically pleasing mix which best captures the intended sound. There is a need for tools which minimise sound-checks by automating complex but non-artistic tasks, establish recommended settings based on the input signals and acoustics, and identifying and avoid issues such as acoustic feedback and microphone crossover.</p>\n<p>We develop and test techniques to <strong>convert audio mixes</strong> between formats. We are working to devise methods to automatically create a surround sound mix, which minimise the masking of sources and places sources in positions that are most subjectively pleasing to the listener.</p>\n<p>We investigate <strong>methodologies of audio editing</strong> used by professional sound engineers, in order to better establish best practices and specify the metadata which will be used to enable automation of audio editing.</p>\n<p>The benefits of these techniques are demonstrated by developing, evaluating and deploying prototype systems for intelligent recording and sound reproduction.\nPersonnel</p>\n<h2>Members</h2>","frontmatter":{"title":""}},"people":{"nodes":[{"id":"cc381a30-c5d2-5ff7-b60d-1f10d81dbf9b","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Alexander Williams","role":"PhD","url":"","acadposition":"PhD Student","blurb":"User-driven deep music generation in digital audio workstations","themes":["mir","audioeng"]}},{"id":"66b87075-3456-5b0c-9dfe-f91b43204096","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Brendan O'Connor","role":"PhD","url":"https://trebolium.github.io/","acadposition":"PhD Student","blurb":"Singing Voice Attribute Transformation","themes":["soundsynthesis","audioeng","mir"]}},{"id":"34ef929b-de59-5c95-a554-597f98d80226","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Chin-Yun Yu","role":"PhD","url":"https://yoyololicon.github.io/","acadposition":"PhD Student","blurb":"Neural Audio Synthesis with Expressiveness Control","themes":["audioeng","mir","soundsynthesis"]}},{"id":"4dc93e11-7859-5d54-80fd-7f2c652e4c3a","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Christian Steinmetz","role":"PhD","url":"https://www.christiansteinmetz.com/","acadposition":"PhD Student","blurb":"End-to-end generative modeling of multitrack mixing with non-parallel data and adversarial networks","themes":["audioeng"]}},{"id":"c210c41b-f202-57d7-ab0c-14538a1c59bc","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Christopher Mitcheltree","role":"PhD","url":"https://christhetr.ee","acadposition":"PhD Student","blurb":"Representation Learning for Audio Production Style and Modulations","themes":["mlist","audioeng","mir"]}},{"id":"612a6483-8966-5db8-9744-f6fd1021a11a","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"David Südholt","role":"PhD","url":"https://dsuedholt.github.io/","acadposition":"PhD Student","blurb":"Machine Learning of Physical Models for Voice Synthesis","themes":["soundsynthesis","audioeng"]}},{"id":"1b6798fe-1d19-52a2-add1-70afa99577f6","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Gary Bromham","role":"PhD","url":"","acadposition":"PhD Student","blurb":"The role of nostalga in music production","themes":["mir","audioeng"]}},{"id":"ca2575a9-c402-50c9-a660-eea9648428a6","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8d8f8","images":{"fallback":{"src":"/c4dm-website/static/e3f990a7d5022b64b25e5c46e4a6d5bf/47930/joshuadreiss.jpg","srcSet":"/c4dm-website/static/e3f990a7d5022b64b25e5c46e4a6d5bf/e07e1/joshuadreiss.jpg 100w,\n/c4dm-website/static/e3f990a7d5022b64b25e5c46e4a6d5bf/dd515/joshuadreiss.jpg 200w,\n/c4dm-website/static/e3f990a7d5022b64b25e5c46e4a6d5bf/47930/joshuadreiss.jpg 400w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e3f990a7d5022b64b25e5c46e4a6d5bf/d8057/joshuadreiss.webp 100w,\n/c4dm-website/static/e3f990a7d5022b64b25e5c46e4a6d5bf/2e34e/joshuadreiss.webp 200w,\n/c4dm-website/static/e3f990a7d5022b64b25e5c46e4a6d5bf/416c3/joshuadreiss.webp 400w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":400}}},"name":"Prof. Joshua D Reiss","role":"Academic","url":"http://www.eecs.qmul.ac.uk/~josh/","acadposition":"Professor of Audio Engineering","blurb":"sound engineering, intelligent audio production, sound synthesis, audio effects, automatic mixing","themes":["audioeng","soundsynthesis"]}},{"id":"f9561550-c8f0-5400-b97b-10f940bd70bd","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Louise Thorpe","role":"PhD","url":"","acadposition":"PhD Student","blurb":"Using Signal-informed Source Separation (SISS) principles to improve instrument separation from legacy recordings","themes":["mir","audioeng"]}},{"id":"96844f85-b589-5907-be07-6cc39213f206","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Marco Comunità","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/comunitamarco.html","acadposition":"PhD Student","blurb":"Machine learning applied to sound synthesis models","themes":["audioeng","soundsynthesis"]}},{"id":"ade5f066-9369-5605-9c73-a41b0112d3cc","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Nelly Garcia","role":"PhD","url":"","acadposition":"PhD Student","blurb":"An investigation evaluating realism in sound design","themes":["comma","audioeng"]}},{"id":"64e68bf1-afa9-5242-ba95-3bbb3ff80584","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Soumya Sai Vanka","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/vankasaisoumya.html","acadposition":"PhD Student","blurb":"Music Production Style Transfer and Mix Similarity","themes":["audioeng","mir"]}},{"id":"d964f102-7a03-5e39-8ca5-d62171217446","frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png","srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/acb7c/defaultprofile.png 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/ccc41/defaultprofile.png 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/b5658/defaultprofile.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/22bfc/defaultprofile.webp 256w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/d689f/defaultprofile.webp 512w,\n/c4dm-website/static/e5b025c2bb9de3e2f93805061b3f4561/67ded/defaultprofile.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"name":"Xavier Riley","role":"PhD","url":"http://eecs.qmul.ac.uk/profiles/rileyjohnxavier.html","acadposition":"PhD Student","blurb":"Pitch tracking for music applications - beyond 99% accuracy","themes":["mir","audioeng"]}}]}}}